{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roman Microlensing Data Challenge 2026: Workflow\n",
    "***\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you, the participant, will be able to:\n",
    "\n",
    "- Load official Data Challenge light curve data from cloud storage.\n",
    "- Install required software in the Roman Research Nexus (RRN).\n",
    "- Initialize a submission project using the `microlens-submit` tool.\n",
    "- Perform a microlensing model fit using `MulensModel`.\n",
    "- Package your model results, plots, and notes into a standardized solution file.\n",
    "- Validate and export your final submission for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook provides a complete, end-to-end workflow for submitting an entry to the Roman Microlensing Data Challenge 2026 (RMDC26). Following these steps is mandatory for a valid submission.\n",
    "\n",
    "The process involves accessing data directly from the cloud, performing your analysis within the RRN, and using the `microlens-submit` package to standardize your results. This helps ensure a level playing field and allows the evaluation committee to process all submissions efficiently.\n",
    "\n",
    "### Important Links\n",
    "- **microlens-submit documentation:** [Read the Docs](https://microlens-submit.readthedocs.io/en/latest/)\n",
    "- **RRN teams & servers:** For information on setting up a collaborative team server, please see the teams page\n",
    "  <img src=\"../../../../images/icons/team.svg\" style=\"vertical-align: bottom; width:1.5em; margin-right:0.5em;\"/> [Working on a Team](../../../../markdown/teams.md)\n",
    "- **RRN software guide:** [Installing Extra Software](../../../../markdown/software.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Installing Required Libraries\n",
    "\n",
    "First, we install the necessary Python packages into our environment. However, we also need to install the Jupyter kernel for that environment, and we need to make sure we keep our package data in a persistent location. This is all handled by the `kernel-create` command or by using a preinstalled kernel such as `roman-cal` or `rges-pit-dc` (not yet installed for all users, but can be created using the `env.yml` in the `data-challenge-notebooks` repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAS workshop kernel workaround\n",
    "\n",
    "# This line assumes you have the entire data-challenge-notebooks repo locally available\n",
    "!kernel-create rges-pit-dc ../../env.yml \"rges-pit-dc\"\n",
    "\n",
    "# Workaround\n",
    "!conda init bash\n",
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are still missing packages, try running this:\n",
    "!source kernel-activate rges-pit-dc\n",
    "!conda install ../../env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command activates the kernel with all relevant data challenge packages preinstalled, including commonly used modeling tools and the submission management tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "This line activates the environment in a terminal session:\n",
    "\n",
    "```python\n",
    "!source kernel-activate rges-pit-dc  # in a Jupyter notebook cell\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```bash\n",
    "source kernel-activate rges-pit-dc  # in a terminal\n",
    "```\n",
    "\n",
    "However, you still need to select it in the top right corner of this notebook for it to be active here.\n",
    "\n",
    "If your kernel or conda environment is not showing up in the kernel selection dropdown, ensure the Jupyter `ipykernel` has been installed by running a command like this one in a terminal tab:\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\n",
    "```\n",
    "\n",
    "Replace `myenv` with your environment name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "colab-only"
    ]
   },
   "source": [
    "<!-- COLAB-ONLY / LOCAL ENVIRONMENT -->\n",
    "> ### Running outside the Roman Research Nexus\n",
    "> If you are running this notebook on Google Colab, locally, or in any environment where the `rges-pit-dc` kernel is not pre-configured, run the cell below to install the required packages.\n",
    ">\n",
    "> **Note:** In the future, on the Nexus, you will skip this cell because the preinstalled `rges-pit-dc` kernel will already include all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "colab-only"
    ]
   },
   "outputs": [],
   "source": [
    "# COLAB-ONLY / LOCAL ENVIRONMENT / KERNEL ISSUES\n",
    "# Uncomment and run if you are NOT on the Roman Research Nexus\n",
    "%pip install --quiet MulensModel microlens_submit emcee corner matplotlib numpy huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Now we import all the libraries we'll need for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system imports\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# data access imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "# display imports\n",
    "from IPython.display import HTML\n",
    "\n",
    "# multiprocessing imports\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# data challenge imports\n",
    "import microlens_submit\n",
    "import MulensModel\n",
    "import emcee\n",
    "\n",
    "# data analysis/visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Access\n",
    "\n",
    "We will now load a light curve directly from the challenge's public directory. You do not need to download anything for processing on the Nexus; the data is local.\n",
    "\n",
    "> Future data releases from the Roman Space Telescope will likely be streamed from S3 buckets. You can use `s3fs` to stream the data directly into memory, as shown in the `data_discovery_and_access.md` guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIER = \"test\"  # \"test\" \"Experienced\" \"Beginner\"\n",
    "EVENT_ID = \"data_challenge_0_129_335\"  # \"we don't have a canonical naming system yet\"\n",
    "FILENAME = f\"{EVENT_ID}.det.lc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXUS-ONLY\n",
    "# This is the official, secure path to the data challenge files on the Nexus.\n",
    "DATA_DIR = \"/data/data-challenge/rges\"\n",
    "#DATA_URI = f\"{DATA_DIR}/{TIER}/{FILENAME}\"\n",
    "DATA_URI = f\"{DATA_DIR}/{FILENAME}\"  # currently includes only the AAS test set\n",
    "\n",
    "# Load the data with header and band column\n",
    "lc_data = pd.read_csv(DATA_URI, sep=r'\\s+', comment='#', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate data store\n",
    "REPO_ID = f\"RGES-PIT/{TIER}\"\n",
    "\n",
    "lc_data = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\"), sep=r'\\s+', comment='#', header=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a plot now, to check that whichever data retrieval method you are using is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique bands (`observatory_code`)\n",
    "bands = np.sort(np.unique(lc_data['observatory_code']))\n",
    "print(f\"Bands found: {bands}\")\n",
    "\n",
    "# calculate magnitude\n",
    "def gulls_flux_to_mag(df, fs, Obssrcmag, bands):\n",
    "    \"\"\"Calculate mag and mag_err for Gulls lightcurves (baseline-relative flux).\"\"\"\n",
    "    # gulls uses a relative flux such that baseline is always 1 and the flux-system zeropoint changes for each event\n",
    "    f    = df[\"measured_relative_flux\"].to_numpy()\n",
    "    ferr = df[\"measured_relative_flux_error\"].to_numpy()\n",
    "    obs  = df[\"observatory_code\"].to_numpy()\n",
    "\n",
    "    # Per-band zero-point (fs and Obssrcmag must be aligned with `bands` order)\n",
    "    m0_per_band = 2.5 * np.log10(fs) + Obssrcmag\n",
    "    m0_map = dict(zip(bands, m0_per_band))\n",
    "\n",
    "    # Map each row's band -> m0\n",
    "    m0 = np.vectorize(m0_map.get)(obs)\n",
    "\n",
    "    # Magnitude + error propagation\n",
    "    mag = m0 - 2.5 * np.log10(f)\n",
    "    mag_err = (2.5 / np.log(10.0)) * (ferr / f)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"mag\"] = mag\n",
    "    df[\"mag_err\"] = mag_err\n",
    "    return df\n",
    "\n",
    "# from the data file comments we know that: #fs:\n",
    "fs = np.array([0.0858942, 0.991581, 0.000266992])\n",
    "Obssrcmag = np.array([23.6466, 24.2485, 23.7078])\n",
    "\n",
    "lc_data = gulls_flux_to_mag(lc_data, fs, Obssrcmag, bands)\n",
    "\n",
    "# Plot each band separately\n",
    "plt.figure(figsize=(8, 5))\n",
    "for band in bands:\n",
    "    mask = lc_data[\"observatory_code\"] == band\n",
    "    plt.errorbar(\n",
    "        lc_data[\"BJD\"][mask], lc_data[\"mag\"][mask], yerr=lc_data[\"mag_err\"][mask],\n",
    "        fmt=\".\", label=f\"Band {band}\", alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"{EVENT_ID} Light Curve by Band\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend(title=\"Band\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Your Submission Project\n",
    "\n",
    "To initialize a submission object in Python, use `microlens_submit.load(project_path)`.\n",
    "If the directory at `project_path` does not exist, it will be created with the correct structure and a blank submission.\n",
    "You can then set the required metadata (like `team_name`, `tier`, etc.) on the returned `Submission` object, and call `.save()` to persist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your project directory in your persistent home folder\n",
    "#TEAM_NAME = \"The Transiting Poachers\"  # Nexus team name\n",
    "TEAM_NAME = \"rges-pit\"\n",
    "project_path = Path(f\"/teams/{TEAM_NAME}/\")\n",
    "\n",
    "# To locate your submission project in the current working directory, use:\n",
    "# project_path = Path.cwd() / TEAM_NAME\n",
    "# To locate your submission project in your home directory, use:\n",
    "# project_path = Path.home() / TEAM_NAME\n",
    "\n",
    "# Now, load the project into our session\n",
    "submission = microlens_submit.load(project_path)  # creates the project directory if it doesn't exist\n",
    "submission.team_name = TEAM_NAME  # assuming your Nexus team name is the same as your data-challenge team name\n",
    "submission.tier = TIER\n",
    "print(f\"\\nProject for '{submission.team_name}' loaded successfully.\")\n",
    "\n",
    "# You can expect saving at this point to result in warnings about missing info\n",
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing or loading your microlens-submit project, and initializing your linked git repository, simply set the repo_url attribute on your Submission. If the directory already exists, load() will just load the existing project, not overwrite it. If the directory exists but no submission content, the content will be added when you run `submission.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize your git repo\n",
    "#!git init\n",
    "#!git add .\n",
    "#!git commit -m \"Initial commit\"\n",
    "#!git branch -m main\n",
    "#!git remote add origin https://github.com/yourusername/your-repo.git\n",
    "#!git push -u origin main\n",
    "\n",
    "# set the repo url in the submission object\n",
    "submission.repo_url = \"https://github.com/yourusername/your-repo.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may leave this repository private during the data challenge, but we ask that you make it public once submissions close, to ensure it is accessible to evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to auto-populate your hardware info and are using the Nexus (like the CLI `nexus-init` command does), you can call the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your hardware info\n",
    "submission.autofill_nexus_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will attempt to detect and fill in hardware details if running in the Roman Science Platform environment, but you can always set or override any values manually.\n",
    "\n",
    "```python\n",
    "# add your hardware info\n",
    "submission.hardware_info = {\n",
    "    \"cpu_model\": \"Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\",\n",
    "    \"num_cores\": 16,\n",
    "    \"memory_gb\": 64,\n",
    "    \"platform\": \"Linux\",\n",
    "    # ...any other relevant info\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A valid submission will include the attributes:\n",
    "* `team_name` to know who the submission belongs to\n",
    "* `tier` to validate event IDs\n",
    "* `repo_url` for reproducibility and evaluation\n",
    "* `hardware_info` for benchmarking purposes\n",
    "\n",
    "You can continue to edit your project without all of this information to make it \"valid\", but you will not be able to submit. You can check the submission validity of your entire project at any time using `submission.run_validation()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Command Line Interface (CLI)\n",
    "\n",
    "If you prefer using the command line or want to automate parts of your workflow, you can accomplish the same tasks using the `microlens-submit` CLI. The CLI is particularly useful for batch processing and automation.\n",
    "\n",
    "### Nexus-Specific CLI Commands\n",
    "\n",
    "The following commands are specifically designed for the Roman Research Nexus environment:\n",
    "\n",
    "#### 1. Initialize Project with Nexus Hardware Info\n",
    "\n",
    "The `nexus-init` command automatically detects and records your Nexus environment details:\n",
    "\n",
    "```bash\n",
    "# Initialize project with automatic Nexus hardware detection\n",
    "microlens-submit nexus-init --team-name \"Your Team\" --tier \"2018-test\" ./my_dc2_submission\n",
    "\n",
    "# This command will automatically detect:\n",
    "# - CPU model from /proc/cpuinfo\n",
    "# - Memory from /proc/meminfo\n",
    "# - Nexus image from JUPYTER_IMAGE_SPEC\n",
    "# - Platform information\n",
    "```\n",
    "\n",
    "#### 2. Set Hardware Information Manually\n",
    "\n",
    "You can also set hardware information manually using the `set-hardware-info` command:\n",
    "\n",
    "```bash\n",
    "# Set CPU information\n",
    "microlens-submit set-hardware-info --cpu \"Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\" \\\n",
    "    --memory-gb 64 --platform \"Roman Research Nexus\" \\\n",
    "    --nexus-image \"roman-dc2:latest\"\n",
    "\n",
    "# Or set individual components\n",
    "microlens-submit set-hardware-info --cpu-details \"16-core Intel Xeon\" \\\n",
    "    --ram-gb 64 --platform \"Roman Science Platform\"\n",
    "```\n",
    "\n",
    "### Complete CLI Workflow Example\n",
    "\n",
    "Here's how to accomplish the same tasks using the command-line interface:\n",
    "\n",
    "```bash\n",
    "# 1. Initialize project with Nexus hardware info\n",
    "microlens-submit nexus-init --team-name \"The Transiting Poachers\" --tier \"2018-test\" ./my_dc2_submission\n",
    "\n",
    "# 2. Add a solution with parameters\n",
    "microlens-submit add-solution ulwdc1_001 1S1L \\\n",
    "    --param t0=2459123.5 --param u0=0.15 --param tE=20.5 \\\n",
    "    --log-likelihood -1234.56 --n-data-points 1250 \\\n",
    "    --cpu-hours 15.2 --wall-time-hours 3.8 \\\n",
    "    --lightcurve-plot-path plots/ulwdc1_001_lc.png \\\n",
    "    --lens-plane-plot-path plots/eulwdc1_001_lens.png \\\n",
    "    --notes \"Initial PSPL fit using MulensModel and emcee\"\n",
    "\n",
    "# 3. Add a more complex solution\n",
    "microlens-submit add-solution ulwdc1_001 1S2L \\\n",
    "    --param t0=2459123.5 --param u0=0.12 --param tE=22.1 \\\n",
    "    --param q=0.001 --param s=1.15 --param alpha=45.2 \\\n",
    "    --log-likelihood -1189.34 --n-data-points 1250 \\\n",
    "    --cpu-hours 28.5 --wall-time-hours 7.2 \\\n",
    "    --alias \"planetary_fit\" \\\n",
    "    --notes \"Binary lens fit with planetary companion\"\n",
    "\n",
    "# 4. Compare solutions\n",
    "microlens-submit compare-solutions ulwdc1_001\n",
    "\n",
    "# 5. Validate your submission\n",
    "microlens-submit validate-submission\n",
    "\n",
    "# 6. Generate dossier for review\n",
    "microlens-submit generate-dossier\n",
    "\n",
    "# 7. Export final submission\n",
    "microlens-submit export final_submission.zip\n",
    "```\n",
    "\n",
    "### CLI vs Python API\n",
    "\n",
    "**When to use CLI:**\n",
    "- Batch processing multiple events\n",
    "- Automation and scripting\n",
    "- Quick parameter updates\n",
    "- Command-line workflows\n",
    "- Integration with other tools\n",
    "\n",
    "**When to use Python API:**\n",
    "- Interactive analysis in Jupyter notebooks\n",
    "- Complex data processing\n",
    "- Custom validation logic\n",
    "- Integration with scientific Python ecosystem\n",
    "\n",
    "### Additional CLI Resources\n",
    "\n",
    "For a comprehensive CLI tutorial, see the [Command Line Tutorial](https://microlens-submit.readthedocs.io/en/latest/tutorial.html) in the documentation.\n",
    "\n",
    "For detailed API reference, see the [API Documentation](https://microlens-submit.readthedocs.io/en/latest/api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the project\n",
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in a project directory of the following structure:\n",
    "\n",
    "```\n",
    "   <project_dir>/\n",
    "   \u251c\u2500\u2500 submission.json\n",
    "   \u2514\u2500\u2500 events/\n",
    "```\n",
    "\n",
    "As you add events, solutions, notes, and figures they will be populated inside this project as follows:\n",
    "\n",
    "```\n",
    "   <submission_dir>/\n",
    "   \u251c\u2500\u2500 dossier/\n",
    "   \u251c       \u251c\u2500\u2500 index.html\n",
    "   \u251c       \u251c\u2500\u2500 <event_id>.html\n",
    "   \u251c       \u251c\u2500\u2500 <solution_id>.html \n",
    "   \u251c       \u251c\u2500\u2500 full_report.html \n",
    "   \u251c       \u2514\u2500\u2500 assets/\n",
    "   \u251c\u2500\u2500 submission.json\n",
    "   \u2514\u2500\u2500 events/\n",
    "       \u2514\u2500\u2500 <event_id>/\n",
    "           \u251c\u2500\u2500 event.json\n",
    "           \u2514\u2500\u2500 solutions/\n",
    "               \u251c\u2500\u2500 <lightcurve image>  # your generated lightcurve plots (optional)\n",
    "               \u251c\u2500\u2500 <lens plane image>  # your generated lens-plane diagram (optional)\n",
    "               \u251c\u2500\u2500 <posteriors>.csv    # your generated posteriors (optional)\n",
    "               \u251c\u2500\u2500 <solution_id>.md    # your notes (optional)\n",
    "               \u2514\u2500\u2500 <solution_id>.json\n",
    "```\n",
    "\n",
    "> A note for later: We recommend you save generated lightcurve plots, lens-plane diagrams, and notes according to this format, but the submission tool is flexible to all save locations, so long as you include the relative path to your <project_dir> in the <solution_id>.json (accessed through the microlens_submit tool or manually). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Microlensing Model Fitting\n",
    "\n",
    "This is where the science happens. Define your `MulensModel` and use your preferred fitting algorithm (e.g., MCMC) to find the best-fit parameters for the event data. \n",
    "\n",
    "### 5.1. Define the Model\n",
    "As an example, we are going to do a preliminary fit using only one band and a point-source-point-lens model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Define the model (e.g., Point Source Point Lens)\n",
    "# Use only the first band for this preliminary fit\n",
    "first_band = bands[0]\n",
    "band_mask = lc_data[\"observatory_code\"] == first_band\n",
    "band_data = lc_data[band_mask]\n",
    "\n",
    "# Create MulensModel data object for the first band\n",
    "my_data = MulensModel.MulensData(\n",
    "    data_list=[band_data['BJD'], \n",
    "               band_data['mag'], \n",
    "               band_data['mag_err']],\n",
    "    phot_fmt='mag',\n",
    "    bandpass='H'  # randomly chosen - not indicative of the true band color\n",
    ")\n",
    "\n",
    "# Initial model parameters (no parallax for preliminary fit)\n",
    "initial_params = {\n",
    "    't_0': band_data['BJD'][np.argmin(band_data['mag'])],\n",
    "    'u_0': 0.1,\n",
    "    't_E': 25.,\n",
    "}\n",
    "\n",
    "pspl_model = MulensModel.Model(initial_params)\n",
    "\n",
    "# Create the event object\n",
    "event_object = MulensModel.Event(datasets=my_data, model=pspl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Setting up the Fitting Procedure\n",
    "\n",
    "We will use `emcee` to drive this fit, so we can demonstrate more attributes of the submission object, like the `posterior_path` and `cpu_time` vs `wall_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Define the likelihood function for emcee\n",
    "def log_likelihood(theta, event_object, parameters_to_fit):\n",
    "    \"\"\"Log likelihood function for emcee\"\"\"\n",
    "    try:\n",
    "        # Update model parameters\n",
    "        for i, param_name in enumerate(parameters_to_fit):\n",
    "            if param_name == 'log_rho':\n",
    "                # Convert log_rho back to rho\n",
    "                event_object.model.parameters.rho = 10**theta[i]\n",
    "            else:\n",
    "                setattr(event_object.model.parameters, param_name, theta[i])\n",
    "        \n",
    "        # Fit the fluxes given the current model parameters\n",
    "        event_object.fit_fluxes()\n",
    "        \n",
    "        # Get the source and blend fluxes\n",
    "        ([F_S], F_B) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
    "        \n",
    "        # Calculate chi-squared\n",
    "        chi2 = event_object.get_chi2()\n",
    "        \n",
    "        # Add flux priors if needed (optional)\n",
    "        penalty = 0.0\n",
    "        if F_B <= 0:\n",
    "            penalty = ((F_B / 100)**2)  # Penalize negative blend flux\n",
    "        if F_S <= 0 or (F_S + F_B) <= 0:\n",
    "            return -np.inf  # Return inf if fluxes are non-physical\n",
    "        \n",
    "        return -0.5 * chi2 - penalty\n",
    "    except:\n",
    "        return -np.inf\n",
    "\n",
    "# Set up emcee\n",
    "nwalkers = 32\n",
    "ndim = 3  # t_0, u_0, t_E\n",
    "nsteps = 1000\n",
    "burnin = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Performing the Fit\n",
    "\n",
    "We will use multithreading with `ThreadPool` from the `multiprocessing` library for this fit, because it is more compatible with notebooks. However, it caches the state of your notebook and can cause unexpected behaviors when you edit code related to your fit and re-run. It is a better idea to use a script and multiprocessing using the `Pool` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3. Run MCMC fitting\n",
    "print(\"Starting MCMC fit...\")\n",
    "start_time = time.time()\n",
    "start_cpu = time.process_time()\n",
    "\n",
    "# Set up parameters to fit\n",
    "parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\"]\n",
    "ndim = len(parameters_to_fit)\n",
    "\n",
    "# Initial positions (slightly perturbed from initial guess)\n",
    "pos = np.array([\n",
    "    initial_params['t_0'], initial_params['u_0'], initial_params['t_E']\n",
    "]) + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "# Use ThreadPool instead of Pool for notebooks\n",
    "n_cores = mp.cpu_count()\n",
    "print(f\"Using {n_cores} threads for MCMC\")\n",
    "\n",
    "# Create the thread pool\n",
    "with ThreadPool(processes=n_cores) as pool:\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nwalkers, ndim, log_likelihood, \n",
    "        args=(event_object, parameters_to_fit),\n",
    "        pool=pool\n",
    "    )\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "# Calculate timing\n",
    "wall_time_hours = (time.time() - start_time) / 3600\n",
    "cpu_time_hours = (time.process_time() - start_cpu) / 3600\n",
    "\n",
    "# Get \"best fit\" parameters (median of posterior)\n",
    "samples = sampler.chain[:, burnin:, :].reshape((-1, ndim))\n",
    "best_fit_params = {\n",
    "    \"t0\": np.median(samples[:, 0]),\n",
    "    \"u0\": np.median(samples[:, 1]),\n",
    "    \"tE\": np.median(samples[:, 2]),\n",
    "}\n",
    "\n",
    "# Update the model with best fit parameters and calculate fluxes\n",
    "for i, param_name in enumerate(parameters_to_fit):\n",
    "    setattr(event_object.model.parameters, param_name, best_fit_params[f\"t{param_name[2:]}\" if param_name.startswith('t_') else param_name.replace('_', '')])\n",
    "\n",
    "# Fit fluxes using MulensModel\n",
    "event_object.fit_fluxes()\n",
    "([F_S], F_B) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
    "\n",
    "best_fit_params[f\"F{first_band}_S\"] = F_S\n",
    "best_fit_params[f\"F{first_band}_B\"] = F_B\n",
    "\n",
    "# Calculate log likelihood at best fit\n",
    "best_log_likelihood = log_likelihood([\n",
    "    best_fit_params[\"t0\"], \n",
    "    best_fit_params[\"u0\"], \n",
    "    best_fit_params[\"tE\"]\n",
    "], event_object, parameters_to_fit)\n",
    "\n",
    "n_data_points = len(my_data.time)\n",
    "\n",
    "print(f\"Best-fit parameters obtained: {best_fit_params}\")\n",
    "print(f\"Wall time: {wall_time_hours:.3f} hours\")\n",
    "print(f\"CPU time: {cpu_time_hours:.3f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that's running, consider consulting the [`microlens-submit` documentation](https://microlens-submit.readthedocs.io/en/latest/submission_manual.html) for details on the submission structure. You can add solutions as you progress with your analysis or collect like-models in csv files and use the bulk-csv-import function. The documentation includes more detailed instructions for building your own submission [manually](https://microlens-submit.readthedocs.io/en/latest/submission_manual.html#manual-submission-format), using the [python API](https://microlens-submit.readthedocs.io/en/latest/api.html), using the [CLI tool](https://microlens-submit.readthedocs.io/en/latest/tutorial.html), and using the [csv bulk import procedure](https://microlens-submit.readthedocs.io/en/latest/submission_manual.html#csv-import-format). \n",
    "\n",
    "The sumission tool will check that you meet all sumission requirements and that your inputs for a model, solution, event, or tier are as expected. It also creates html dossiers that allows you to inspect your progress.\n",
    "\n",
    "You can choose for yourself how integrated you want the tool to be in your workflow, but submissions intended for evalutation should pass validation. This ensure that we can partially automate the evaluation process, quantitatively assesing your submission accuracy compared with select simulation truths. \n",
    "\n",
    "If you run in to issues with the tool, contact the developer through the [RMDC26 Slack Workspace](rmdc26.slack.com), [GitHub Issues](https://github.com/rges-pit/microlens-submit/issues), or on the [RGES-PIT website help form](https://rges-pit.org/data-challenge/help/). \n",
    "\n",
    "> This tool is under active development and feedback or feature requests are welcome. Updates such as dossier plots can be expected during the challenge so we recommend you periodically update the package; `pip install -U microlens_submit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Results\n",
    "Let's save and plot all the fit results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4. Create the initial solution and event objects\n",
    "event = submission.get_event(EVENT_ID)\n",
    "solution = event.add_solution(\n",
    "    model_type=\"1S1L\",\n",
    "    parameters=best_fit_params,\n",
    "    alias=f\"Preliminary PSPL - Band {first_band}\"\n",
    ")\n",
    "solution.bands = [\"0\"]\n",
    "\n",
    "# We are doing this now because we want generated solution id for our file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save posteriors\n",
    "posterior_dir = Path(project_path) / \"events\" / EVENT_ID / \"solutions\" / solution.solution_id\n",
    "posterior_dir.mkdir(parents=True, exist_ok=True)\n",
    "posterior_path = posterior_dir / \"posteriors.csv\"\n",
    "\n",
    "# Save posterior samples with column headers (no parallax parameters)\n",
    "posterior_data = np.column_stack([\n",
    "    samples[:, 0], samples[:, 1], samples[:, 2]\n",
    "])\n",
    "np.savetxt(posterior_path, posterior_data, \n",
    "           delimiter=',', \n",
    "           header='t0,u0,tE',\n",
    "           comments='')\n",
    "\n",
    "# Save lightcurve plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(my_data.time, my_data.mag, yerr=my_data.err_mag, fmt='.', \n",
    "             color='k', alpha=0.5, label='Data')\n",
    "\n",
    "# Plot best fit model using the event object\n",
    "event_object.plot_model(color='r', label='Best Fit')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"{EVENT_ID} Best Fit - Band {first_band}\")\n",
    "plt.xlabel(\"BJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "\n",
    "lightcurve_path = posterior_dir / \"lightcurve.png\"\n",
    "plt.savefig(lightcurve_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save caustic diagram (for PSPL, this will be empty since no caustics)\n",
    "plt.figure(figsize=(8, 8))\n",
    "event_object.model.plot_caustics()\n",
    "plt.title(f\"{EVENT_ID} Caustic Diagram - Band {first_band}\")\n",
    "caustic_path = posterior_dir / \"caustic.png\"\n",
    "plt.savefig(caustic_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Solution Management\n",
    "\n",
    "Now, we package these results into the submission object.\n",
    "\n",
    "A solution entry has the following attributes:\n",
    "* `model_type`\n",
    "* `parameters` matching the indicated `model_type` and `higher_order_effects`\n",
    "* `higher_order_effects` (if any are present in the model)\n",
    "* `log_likelihood` (the fit's log-likelihood value; $-\\chi^2/2$)\n",
    "* `relative_probability` (optional, probability of this solution being the true model)\n",
    "* `n_data_points` (number of data points used in the fit)\n",
    "* `compute_info` containing:\n",
    "  - `cpu_hours`\n",
    "  - `wall_time_hours`\n",
    "  - `dependencies` (list of installed Python packages, auto-captured)\n",
    "  - `git_info` (commit, branch, is_dirty; auto-captured if in a git repo)\n",
    "* `t_ref` (reference time, if required for indicated higher-order effects)\n",
    "* `bands` (if the fit is band-dependent)\n",
    "* `alias` (optional, for human-readable, editable solution names)\n",
    "* `notes_path` (path to Markdown notes file, optional but recommended)\n",
    "* `parameter_uncertainties` (uncertainties for parameters)\n",
    "* `physical_parameters` (optional, derived physical parameters)\n",
    "* `posterior_path` (optional, path to posterior samples)\n",
    "* `lightcurve_plot_path` (optional, path to lightcurve plot image)\n",
    "* `lens_plane_plot_path` (optional, path to lens plane plot image)\n",
    "* `used_astrometry` (optional, whether astrometric data was used)\n",
    "* `used_postage_stamps` (optional, whether postage stamp data was used)\n",
    "* `limb_darkening_model` and limb_darkening_coeffs (if relevant)\n",
    "\n",
    "A solution requires only these fields for validity:\n",
    "* `model_type`\n",
    "* `parameters` matching the indicated `model_type` with no `higher_order_effects`\n",
    "* `log_likelihood` (the fit's log-likelihood value; $-\\chi^2/2$)\n",
    "* `n_data_points` (number of data points used in the fit)\n",
    "* `compute_info` containing at least:\n",
    "  - `cpu_hours`\n",
    "  - `wall_time_hours`\n",
    "\n",
    "You can set these `compute_info` values using the method:\n",
    "`solution.set_compute_info(cpu_hours=..., wall_time_hours=...)`\n",
    "This method also captures:\n",
    "* The current Python environment (pip freeze output)\n",
    "* Git repository info (commit, branch, dirty status)\n",
    "* You can call `set_compute_info()` with either or both values, or leave them blank if you want only the environment info.\n",
    "\n",
    "We calculated these compute times in the previous section using `time.time()` and `time.process_time()`. `time.time()` records the literal time, whereas `time.process_time()` returns the total CPU time (in seconds) that the current process has used since it started. So calculating the difference between two `time.time()` calls will tell you the time elapsed (in seconds) between each call, while the difference between two `time.process_time()` calls will tell you the CPU time elapsed, not including time spent sleeping or waiting for I/O. For example, for a process with 4 cores, running for 1 hour, this would mean a CPU time of 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set compute info\n",
    "# automatically detects the current python environment and git repository state\n",
    "solution.set_compute_info(cpu_hours=cpu_time_hours, wall_time_hours=wall_time_hours)\n",
    "\n",
    "# Set other metadata\n",
    "solution.log_likelihood = best_log_likelihood\n",
    "solution.n_data_points = n_data_points\n",
    "solution.bands = [str(first_band)]\n",
    "\n",
    "# Set file references\n",
    "solution.posterior_path = str(posterior_path.relative_to(project_path))\n",
    "solution.lightcurve_plot_path = str(lightcurve_path.relative_to(project_path))\n",
    "solution.lens_plane_plot_path = str(caustic_path.relative_to(project_path))\n",
    "\n",
    "# Set notes\n",
    "solution.set_notes(f\"\"\"\n",
    "# Preliminary Fit Analysis\n",
    "\n",
    "This is a preliminary PSPL fit to determine approximate parameters.\n",
    "\n",
    "## Fit Details\n",
    "- **Model Type**: 1S1L\n",
    "- **Band Used**: {first_band}\n",
    "- **Data Points**: {n_data_points}\n",
    "- **MCMC Walkers**: {nwalkers}\n",
    "- **MCMC Steps**: {nsteps}\n",
    "- **Burn-in**: {burnin} steps\n",
    "\n",
    "This solution is marked as inactive as it represents a preliminary analysis.\n",
    "\"\"\")\n",
    "\n",
    "# Save everything\n",
    "submission.save()\n",
    "\n",
    "print(f\"\u2705 Solution '{solution.alias}' ({solution.solution_id}) created and saved!\")\n",
    "print(f\"   - Posteriors saved to: {posterior_path}\")\n",
    "print(f\"   - Lightcurve plot saved to: {lightcurve_path}\")\n",
    "print(f\"   - Caustic diagram saved to: {caustic_path}\")\n",
    "print(f\"   - Solution marked as inactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Solution Comparison and Relative Probability\n",
    "\n",
    "When you have multiple solutions for the same event, you can specify their relative probabilities to indicate which model you think is most likely to be correct.\n",
    "\n",
    "**Key Points:**\n",
    "* **Relative probabilities must sum to 1.0** across all active solutions for an event\n",
    "* **If you don't specify relative probabilities**, they are automatically calculated during export using the Bayesian Information Criterion (BIC)\n",
    "* **BIC calculation** uses: `BIC = k*ln(n) - 2*log_likelihood` where k = number of parameters, n = number of data points\n",
    "* **Equal probabilities** are assigned if BIC calculation is not possible (missing data)\n",
    "\n",
    "**Example of manual relative probability assignment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If you have multiple solutions for the same event\n",
    "# solution1.relative_probability = 0.7  # 70% confidence\n",
    "# solution2.relative_probability = 0.3  # 30% confidence\n",
    "# Total must equal 1.0\n",
    "\n",
    "# For this example, we'll just note that relative probability is optional\n",
    "# and will be calculated automatically if not provided\n",
    "print(\"Relative probability will be calculated automatically during export if not specified.\")\n",
    "print(\"The calculation uses BIC: BIC = k*ln(n) - 2*log_likelihood\")\n",
    "print(\"where k = number of parameters, n = number of data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How automatic calculation works during export:**\n",
    "1. **If you provide relative probabilities** that sum to 1.0, those are used\n",
    "2. **If some solutions have relative probabilities** but others don't, the remaining probability is distributed among the unspecified solutions using BIC\n",
    "3. **If no solutions have relative probabilities**, all active solutions get equal probability\n",
    "4. **If BIC calculation fails** (missing log_likelihood, n_data_points, or parameters), equal probabilities are assigned\n",
    "\n",
    "This ensures that evaluators always have a complete probability distribution for your solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dossier and Solution Management\n",
    "\n",
    "This is the final step. We will generate a human-readable HTML report (a 'dossier'), validate the submission for completeness, and export the final `.zip` file to the official submission location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the HTML Dossier for your own records\n",
    "from microlens_submit.dossier import generate_dashboard_html\n",
    "\n",
    "dossier_path = project_path / \"dossier\"\n",
    "generate_dashboard_html(submission, dossier_path, open=False)\n",
    "print(f\"HTML Dossier report generated at: {dossier_path}/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Display the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dashboard inline (requires IPython)\n",
    "\n",
    "# Read the generated HTML file\n",
    "with open(dossier_path / \"index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook \n",
    "HTML(html)\n",
    "\n",
    "# Note: images paths not resolving correctly for submission not in the same folder as the notebook is a known issue currently being addressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Display the Event Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(dossier_path / f\"{event.event_id}.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Display the Solution Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(dossier_path / f\"{solution.solution_id}.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Display Full Report\n",
    "\n",
    "This is the report that will be generated for the evaluators, with placeholders for sections that are not meant for participants. You can generate this report to view the full state of your project as you go, in the format in which it will be evaluated.\n",
    "\n",
    "```python\n",
    "# Read the generated HTML file\n",
    "with open(dossier_path / \"full_dossier_report.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if you don't like a fit?\n",
    "\n",
    "You can soft delete a fit without removing it from your local project by deactivating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate the solution\n",
    "# we are doing this because this is a preliminary fit and we don't want it to \n",
    "# be used in the final report\n",
    "solution.is_active = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also hard delete a solution by calling `event.remove_solution(solution_id, force=True)` or navigating to the solution in the project directory and delete it manually.\n",
    "\n",
    "For these changes to persist outside of the active python objects, you must again save the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation and Submitting the Project\n",
    "\n",
    "This is the final step. We will validate the submission for completeness, and export the final `.zip` file to the official submission location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Validate the submission\n",
    "print(\"\\nRunning validation...\")\n",
    "warnings = submission.run_validation()\n",
    "if warnings:\n",
    "    print(\"\u26a0\ufe0f Validation Warnings Found:\")\n",
    "    for w in warnings:\n",
    "        print(f\"  - {w}\")\n",
    "else:\n",
    "    print(\"\u2705 Submission is valid and ready for export!\")\n",
    "\n",
    "# 2. Export the final submission file\n",
    "\n",
    "full_output_path = f\"~/\"\n",
    "\n",
    "print(f\"\\nExporting submission to: {full_output_path}\")\n",
    "try:\n",
    "    local_export_path = project_path / \"final_submission\"\n",
    "    submission.export(str(local_export_path))\n",
    "    print(f\"\\n\ud83c\udf89 Successfully exported to {local_export_path}\")\n",
    "    print(\"You would now upload this file to the secure submission location.\")\n",
    "except ValueError as e:\n",
    "    print(f\"\u274c Export failed: {e}\")\n",
    "\n",
    "# That bit's a trick. It doesn't actually go anywhere.\n",
    "# We will collect the most recent submission zip in your team storage on the RRN\n",
    "# for you. No need to do anything. \ud83e\udd73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the validation fails because the tool is not expecting `data_challenge_0_129_335` in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Roman Research Nexus: Data Discovery & Access](https://spacetelescope.github.io/roman-notebooks/notebooks/data_discovery_and_access/data_discovery_and_access.html)\n",
    "- [Session A \u2014 Nexus Setup & Help (AAS Workshop site)](https://rges-pit.org/data-challenge/aas-workshop/1-nexus/)\n",
    "- [Roman Microlensing Data Challenge landing page](https://rges-pit.org/data-challenge/)\n",
    "- [microlens-submit documentation](https://microlens-submit.readthedocs.io/en/latest/)\n",
    "- [Next notebook: Session B \u2014 Single Lens & Pipelines](https://github.com/rges-pit/data-challenge-notebooks/blob/main/AAS%20Workshop/Session%20B%3A%20Single%20Lens%20%26%20Pipelines/Single_Lens_Pipeline.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Working with Parquet Files\n",
    "\n",
    "The Roman Microlensing Data Challenge may provide data in **Parquet** format, a columnar storage format that is highly efficient for large datasets. This appendix covers the basics of reading, writing, and working with Parquet files in Python.\n",
    "\n",
    "### Why Parquet?\n",
    "\n",
    "- **Efficient storage**: Parquet uses columnar compression, significantly reducing file sizes compared to CSV.\n",
    "- **Fast read performance**: Only the columns you need are loaded into memory.\n",
    "- **Schema preservation**: Data types are preserved, avoiding issues with type inference.\n",
    "- **Compatibility**: Works seamlessly with pandas, PyArrow, and cloud storage (S3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Packages\n",
    "\n",
    "If you don't already have `pyarrow` or `fastparquet` installed, you can install them with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyarrow (recommended) or fastparquet\n",
    "# Uncomment the line below if needed\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Parquet Files\n",
    "\n",
    "#### Basic Reading with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a local Parquet file\n",
    "df = pd.read_parquet(\"path/to/file.parquet\")\n",
    "\n",
    "# Read only specific columns (more memory efficient)\n",
    "df = pd.read_parquet(\"path/to/file.parquet\", columns=[\"HJD\", \"mag\", \"mag_err\", \"band\"])\n",
    "\n",
    "# Example: Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Parquet from S3 (Cloud Storage)\n",
    "\n",
    "For data stored in S3 buckets (like the Data Challenge data), you can read Parquet files directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to S3 (anonymous access for public buckets)\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Read a Parquet file directly from S3\n",
    "PARQUET_URI = \"s3://rmdc26-data-public/example/lightcurve.parquet\"\n",
    "with fs.open(PARQUET_URI, 'rb') as f:\n",
    "    df = pd.read_parquet(f)\n",
    "\n",
    "# Alternative: Use pandas with storage_options\n",
    "df = pd.read_parquet(\n",
    "    \"s3://rmdc26-data-public/example/lightcurve.parquet\",\n",
    "    storage_options={\"anon\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Parquet Files\n",
    "\n",
    "Saving your processed data or results as Parquet is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "sample_data = pd.DataFrame({\n",
    "    \"event_id\": [\"event_001\", \"event_002\", \"event_003\"],\n",
    "    \"t0\": [2459000.5, 2459100.2, 2459200.8],\n",
    "    \"tE\": [25.3, 42.1, 18.7],\n",
    "    \"u0\": [0.1, 0.05, 0.2],\n",
    "    \"classification\": [\"PSPL\", \"Binary\", \"PSPL\"]\n",
    "})\n",
    "\n",
    "# Save to Parquet with default compression (snappy)\n",
    "sample_data.to_parquet(\"results.parquet\", index=False)\n",
    "\n",
    "# Save with gzip compression (smaller file, slower read/write)\n",
    "sample_data.to_parquet(\"results.parquet\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Parquet File Metadata\n",
    "\n",
    "You can inspect a Parquet file's schema and metadata without loading the entire file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read metadata without loading data\n",
    "parquet_file = pq.ParquetFile(\"path/to/file.parquet\")\n",
    "\n",
    "# Get schema (column names and types)\n",
    "print(\"Schema:\")\n",
    "print(parquet_file.schema_arrow)\n",
    "\n",
    "# Get number of rows without reading the data\n",
    "print(f\"\\nNumber of rows: {parquet_file.metadata.num_rows}\")\n",
    "\n",
    "# Get number of row groups\n",
    "print(f\"Number of row groups: {parquet_file.metadata.num_row_groups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Large Parquet Datasets\n",
    "\n",
    "For datasets too large to fit in memory, you can process them in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file in batches using PyArrow\n",
    "parquet_file = pq.ParquetFile(\"large_dataset.parquet\")\n",
    "\n",
    "# Process one row group at a time\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    df_chunk = batch.to_pandas()\n",
    "    # Process each chunk\n",
    "    print(f\"Processing {len(df_chunk)} rows...\")\n",
    "\n",
    "# Alternative: Read row groups individually\n",
    "for i in range(parquet_file.metadata.num_row_groups):\n",
    "    table = parquet_file.read_row_group(i)\n",
    "    df_chunk = table.to_pandas()\n",
    "    # Process each row group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Between Formats\n",
    "\n",
    "#### CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV to Parquet\n",
    "df = pd.read_csv(\"lightcurve.csv\")\n",
    "df.to_parquet(\"lightcurve.parquet\", index=False)\n",
    "\n",
    "# Convert Parquet to CSV\n",
    "df = pd.read_parquet(\"lightcurve.parquet\")\n",
    "df.to_csv(\"lightcurve.csv\", index=False)\n",
    "\n",
    "print(\"Conversion examples shown above (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Working with Parquet in the Data Challenge\n",
    "\n",
    "1. **Use column selection**: When reading large files, only load the columns you need:\n",
    "   ```python\n",
    "   df = pd.read_parquet(\"file.parquet\", columns=[\"HJD\", \"mag\", \"mag_err\"])\n",
    "   ```\n",
    "\n",
    "2. **Leverage filtering**: PyArrow supports predicate pushdown for efficient filtering:\n",
    "   ```python\n",
    "   df = pd.read_parquet(\"file.parquet\", filters=[(\"band\", \"==\", \"W149\")])\n",
    "   ```\n",
    "\n",
    "3. **Choose appropriate compression**:\n",
    "   - `snappy` (default): Fast compression/decompression, moderate size\n",
    "   - `gzip`: Smaller files, slower I/O\n",
    "   - `zstd`: Good balance of speed and compression\n",
    "\n",
    "4. **Preserve data types**: Parquet maintains column types, avoiding CSV parsing issues with floats/integers.\n",
    "\n",
    "5. **Use partitioned datasets**: For very large datasets, partition by a key column (e.g., `event_id`) for faster access to specific events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "**Author(s):** Amber Malpas, Meet Vyas <br>\n",
    "**Keyword(s):** Roman, Microlensing, Data Challenge, Workflow <br>\n",
    "**Last Updated:** December 2025\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `MulensModel`, `microlens-submit`, or this notebook for published research, please cite the\n",
    "authors. Follow these links for more information about citing:\n",
    "\n",
    "* [Citing `MulensModel`](https://github.com/rpoleski/MulensModel/blob/master/CITATION.cff)\n",
    "* [Citing `microlens-submit`](https://github.com/rges-pit/microlens-submit/blob/main/CITATION.cff)\n",
    "* [Citing **Roman Microlensing Data Challenge 2026 Notebooks**](https://github.com/rges-pit/data-challenge-notebooks/blob/main/zenodo.txt)\n",
    "\n",
    "[Top of Page](#top)\n",
    "<!-- Footer Start -->\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/roman_notebooks/refs/heads/main/stsci_logo2.png\" alt=\"Space Telescope Logo\" width=\"200px\"/>\n",
    "\n",
    "<!-- Footer End -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rges-pit-dc",
   "language": "python",
   "name": "rges-pit-dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "rges_sync": {
   "source_id": "session-a-roman-workflow",
   "session": "RRN workflow",
   "audiences": [
    "Roman Research Nexus participants",
    "Data challenge teams",
    "AAS workshop attendees"
   ],
   "website_render": "full_embed",
   "nexus_support": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
