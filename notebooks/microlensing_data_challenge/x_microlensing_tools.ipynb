{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVT-IDqXWVdf"
   },
   "source": [
    "# Microlensing Tools\n",
    "***\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/rges-pit/data-challenge-notebooks/blob/main/Extras/Microlensing_Tools.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "If you would like an introduction to python notebooks, please read this tutorial: https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Set-Up\n",
    "\n",
    "**Dependencies**\n",
    "This notebook has the following base dependencies:\n",
    "\n",
    "  - `python` (v>=3.11 recommended)\n",
    "  - `numpy`\n",
    "  - `matplotlib`\n",
    "  - `pandas`\n",
    "  - `scipy`\n",
    "  - `jupyter`\n",
    "  - `ipython`\n",
    "  - `astropy`\n",
    "  - `emcee` \n",
    "  - `corner`\n",
    "  - `popclass`\n",
    "  - `tqdm` (remaining packages are not currently available on conda)\n",
    "  - `pathos`\n",
    "  - `VBMicrolensing`\n",
    "  - `MulensModel`\n",
    "  - `pyLIMA`\n",
    "  - `RTModel`\n",
    "  from pathlib import Path\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "\n",
    "You can install these packages by running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nexus-only"
    ]
   },
   "outputs": [],
   "source": [
    "# NEXUS-ONLY\n",
    "%source activate-kernel rges-pit-dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PDvU-MTkyi0K",
    "outputId": "35a9d16c-0473-43ce-987b-312f468fdb54"
   },
   "outputs": [],
   "source": [
    "#@title General Imports\n",
    "\n",
    "# system tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "# data analysis tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from scipy.optimize import minimize\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "try:\n",
    "    from google.colab import sheets  # will only work if you are running on Colab\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# web scrapping tools\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# parallel processing tools\n",
    "from pathos.multiprocessing import ProcessingPool as Pool  # for multiprocessing inside jupyter\n",
    "import multiprocessing as mp  # Ensure this is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the **Roman Microlensing Data Challenge 2026 (RMDC26)** notebook on **Microlensing Tools**. If you have not already, you can click the link below to access the first notebook in this series: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**RMDC26/Nexus Workflow**](roman-workflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TA6gvhQWmaV"
   },
   "source": [
    "The data challenge is intended to be a semi-realistic representation of the data volume and type expected from the Roman Galactic Bulge Time Domain Survey. Specifically for microlensing events and microlensing false positives.\n",
    "\n",
    "This notebook is brought to you by the RGES-PIT and is inteneded to be an introductory workbook for users new to microlensing event fitting and physical parameter estimation, users who would like a refresher, or users who would like a comprehensive introduction to tools they have not used before. It is not recommended that you complete this notebook in its entirety. You should make use of the tool look-up table and index to navigate to the section relevant to your interest or issue.\n",
    "\n",
    "Our aim is to provide you with a centralized resource covering open-source microlensing code applications. It is not designed to provide you with a realistic view of what working with bulk microlensing data involves or the general procedures undertaken to demistify microlensings event data. We will link to resources to understand procedures that are currently implemented in microlensing detection and analysis.  \n",
    "\n",
    "### What data are used in this notebook?\n",
    "\n",
    "This notebook primarily uses lightcurves from the [2018 WFIRST Data Challenge](https://www.microlensing-source.org/data-challenge/). The data will be either fetched from an s3 bucket or [cloned](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) and sorted in later sections, depending on your environment. Because the 2018 WFIRST Data Challenge concluded in 2018, we can make full use of the [parameter \"truths\"](https://en.wikipedia.org/wiki/Statistical_parameter#:~:text=parameter%20describes%20the-,true%20value,-calculated%20from%20the) for each event to verify our fitting processes.\n",
    "\n",
    "The dataset consists of two lightcurve files for each event or star, representing the data from Roman's `W149` and `Z087` filters. The files are in ASCII format with the columns BJD, Aperture_Magnitude and Error, and follow the file-naming convention: `ulwdc1_nnn_[W149/Z087].txt`\n",
    "\n",
    "Supplementary files were also provided including `wfirst_ephemeris.txt`, which contains the `BJD` and 3D spacecraft location within the solar system. Information was provided on the surface-brightness color relation for `Z087-W149` to enable lens masses to be determined where applicable.\n",
    "\n",
    "It should be noted that in the simulated data, the inertial frame of reference was defined with the $x$-axis increasing from the binary primary lens object towards the secondary (less massive) lens object at `t0`, the time of closest approach to the primary lens. If viewed from the solar system barycenter, the inertial frame moves at the relative velocity `vlens_CoM - vobserver(t0)`. The inclination of the orbit is a counter-clockwise rotation about the $x$-axis. $\\alpha$ is the angle that the source trajectory made with the $x$-axis (if parallax was 0). Where finite source effects were significant, a linear limb darkening law was applied.\n",
    "\n",
    "<!--\n",
    "### <font face=\"Helvetica\" size=\"5\"> What lightcurve models are included in this notebook? </font>\n",
    "\n",
    "The first data challenge contained the following kinds of lightcurves:\n",
    "\n",
    "* Cataclysmic Variable Star (CVS) false positive\n",
    "* Single lens microlensing events\n",
    "* Binary lens microlensing events\n",
    "\n",
    "You can expect a much broader set in the current data challenge with the addition of binary source microlensing events and more higher-order effects, including parallax, lens orbital motion, . Higher-order effects included in the first data challenge, which are still included in this data challenge, are finite source effects and ... .\n",
    "\n",
    "Additional data types included in Data challenge 2 include:\n",
    "* Astrometric timeseries\n",
    "* Lens flux measurement\n",
    "-->\n",
    "\n",
    "### <font face=\"Helvetica\" size=\"5\"> What codes are used in this notebook? </font>\n",
    "\n",
    "Some open source microlensing codes:\n",
    "<!-- add a column for pip and conda -->\n",
    "| Name | Notes | Maintained | Link | Covered here |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| MuLensModel | User friendly, single- and binary-lens fitting code. | Yes ([Poleski](https://github.com/rpoleski)) | [GitHub](https://github.com/rpoleski/MulensModel) | [Yes](#mulensmodel) |\n",
    "| BAGEL | Incorporates photometric and astrometric microlensing. | Yes ([Moving Universe Lab](https://github.com/MovingUniverseLab)) | [GitHub](https://github.com/MovingUniverseLab/BAGLE_Microlensing) | [No](#bagel) |\n",
    "| VBMicroelensing | A more general version of the binary-lens code [VBBL](https://github.com/valboz/VBBinaryLensing). VBMicrolensing <br>is a tool for efficient computation in gravitational microlensing events <br>using the advanced contour integration method, supporting single, binary <br>and multiple lenses. | Yes ([Bozza](https://github.com/valboz)) | [GitHub](https://github.com/valboz/VBMicrolensing) | [No](#vbmicrolensing)\n",
    "| pyLIMA | pyLIMA is the first open source software for modeling microlensing <br>events. It should be flexible enough to handle your data and fit it. You can <br>also practice by simulating events. Useful for space-based observations. | Yes ([Bachelet](https://github.com/ebachelet)) | [GitHub](https://github.com/ebachelet/pyLIMA) | [Yes](#pylima) |\n",
    "| RTModel | Hands-off model fitting with built in model <i>\"interpretation\"</i> <br>(e.g. determing single-lens vs binary-lens arrangement) | Yes ([Bozza](https://github.com/valboz)) | [Github](https://github.com/valboz/RTModel) | [Yes](RTModel) |\n",
    "| eesunhong | No general description found. <br>See [Bennett and Rhie (1996)](https://ui.adsabs.harvard.edu/abs/1996ApJ...472..660B/abstract) and [Bennett (2010)](https://ui.adsabs.harvard.edu/abs/2010ApJ...716.1408B/abstract) | Yes ([Bennett]()) | [GitHub](https://github.com/golmschenk/eesunhong) | No |\n",
    "| pyLIMASS | Addition to pyLIMA for estimating physical properties of the lens <br>system. See [Bachelet, Hundertmark, and Calchi Novati (2024)](https://ui.adsabs.harvard.edu/abs/2024AJ....168...24B/abstract) | Yes ([Bachelet](https://github.com/ebachelet)) | [GitHub](https://github.com/ebachelet/pyLIMA/tree/master/pyLIMA/pyLIMASS) | [Yes](#pylima) |\n",
    "| popclass | Provides a flexible, probabilistic framework for classifying the lens <br>of a gravitational microlensing event. | Yes ([LLNL](https://github.com/LLNL)) | [GitHub](https://github.com/LLNL/popclass) | [Yes](#popclass) |\n",
    "| muLAn | Designed for fitting Roman microlensing lightcurve data. | No ([Cassan](https://github.com/ArnaudCassan)/[Ranc](https://github.com/clementranc)) | [GitHub](https://github.com/muLAn-project/muLAn) | [No](#muLAN) |\n",
    "| triplelens | Calculates light curves and image positions for triple microlensing <br>systems. (When the mass ratio is small (below ~ 1e-5), the solutions <br>from the lens equation solver are more accurate when the origin <br>of the coordinate system is set to be close to the smallest mass.) | No ([Kuang](https://github.com/rkkuang)) | [GitHub](https://github.com/rkkuang/triplelens) | No |\n",
    "| SingleLensFitter | Fits single lens events with finite source effects | No ([Albrow](https://github.com/MichaelDAlbrow)) | [GitHub](https://github.com/MichaelDAlbrow/SingleLensFitter) | No |\n",
    "<br>\n",
    "\n",
    "> This is a replica of the tools list on the [RGES-PIT website](https://rges-pit.org/tools). We would like this table to be a comprehensive list, so if you have an open source code you would like to contribute, please refer to the [contributing documentation](https://github.com/rges-pit/data-challenge-notebooks/blob/main/CONTRIBUTING.md), open a discussion on the [GitHub repo](https://github.com/rges-pit/data-challenge-notebooks/discussion), or submit to or website [portal](https://rges-pit.org/submit/).\n",
    "\n",
    "### Microlensing learning resources\n",
    "\n",
    "* [RGES-PIT Minicourse](https://rges-pit.org/outreach_mini_landing/)\n",
    "\n",
    "  The RGES PIT has developed a microlensing mini course for select students to participate in various Roman-related lectures and activities during the Summer of 2025. The virtual lectures were held in mid May 2025; you can find lecture materials, recordings, and assignments.\n",
    "\n",
    "* [Microlensing Source](https://www.microlensing-source.org/)\n",
    "\n",
    "  Microlensing Source is a resource center for all aspects of gravitational microlensing. It aims to make microlensing more accessible for anyone with an interest in the subject - including students considering a career in the field, citizen scientists and those looking for a ready reference.\n",
    "\n",
    "* [The Microlenser's Guide to the Galaxy](https://github.com/rges-pit/TheMicrolensersGuideToTheGalaxy)\n",
    "\n",
    "  The goal of this project is to create an all-encompassing collection of Jupyter notebooks\u2014your trusty companions for engaging exercises related to microlensing. Through these notebooks, the insights and experiences of microlensing veterins can light your path as you embark on your journey of discovery and exploration through scientific research.\n",
    "\n",
    "* [2017 Sagan Workshop](http://nexsci.caltech.edu/workshop/2017/)\n",
    "\n",
    "  The 2017 Sagan Summer Workshop focus on searching for planets with Roman (previously known as WFIRST) microlensing. Leaders in the field will discussed the importance of microlensing to understanding planetary populations and demographics, especially beyond the snow line. They reviewed the microlensing method, both in the context of current capabilities and the future Roman microlensing survey. In addition, speakers addressed the broad potential of the Romans's Wide Field Imaging microlensing survey for (non-microlensing) science in the galactic bulge. Attendees participated in hands-on group projects related to the Roman microlensing planet survey and had the opportunity to present their own work through short presentations (research POPs) and posters.\n",
    "  The recordings from this workshop can be found [here](https://www.youtube.com/watch?v=QPfKucBb9B8&list=PLIbTYGsIVYthWRS14eCEK8SK9IOTcaYsf)\n",
    "\n",
    "* [Glossary of Terms](https://www.microlensing-source.org/glossary/)\n",
    "\n",
    "  This glossary, from Microlensing Source, is intended as a quick reference, particularly to disambiguate the different symbol sets used by different authors over time. Interested readers are referred to the references at the bottom for a full discussion, especially Skowron et al. (2011), and to the Learning Resources menu.\n",
    "\n",
    "> This is also a replica of the background material listed on the [resources](https://rges-pit.org/resources/#:~:text=Background%20material,Jupyter%20Notebook%20Course%20%C2%A0) page of the RGES-PIT website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5OeZIbHv2eB"
   },
   "source": [
    "## Collecting the Data\n",
    "\n",
    "In this section and we will either stream our data from an s3 bucket or clone it from GitHub. We will also need to organize it in to sensible dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nexus-only"
    ]
   },
   "outputs": [],
   "source": [
    "# NEXUS-ONLY\n",
    "# Replace with s3 bucket streaming, when available.\n",
    "# For now we will clone the repo.\n",
    "\n",
    "# clone the microlensing data challenge repo\n",
    "!git clone https://github.com/microlensing-data-challenge/data-challenge-1.git\n",
    "\n",
    "# Extract the lightcurve files\n",
    "!tar -xzvf data-challenge-1/lc.tar.gz -C data-challenge-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "7Zvjvl6nw3hx",
    "outputId": "3c3368c0-52bd-41b8-9e13-76b03839b044"
   },
   "outputs": [],
   "source": [
    "#@title Putting everything in a tidy data frame\n",
    "\n",
    "master_file = '/content/data-challenge-1/Answers/master_file.txt'\n",
    "header_file = '/content/data-challenge-1/Answers/wfirstColumnNumbers.txt'\n",
    "\n",
    "rows = []\n",
    "with open(master_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines or comment lines\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        tokens = line.split()  # split on whitespace\n",
    "        # Keep only single-lens events\n",
    "        if \"dcnormffp\" not in tokens:\n",
    "            continue\n",
    "\n",
    "        # Single-lens lines should have exactly 96 columns\n",
    "        if len(tokens) != 96:\n",
    "            continue\n",
    "\n",
    "        rows.append(tokens)\n",
    "\n",
    "df_sl = pd.DataFrame(rows)\n",
    "\n",
    "# make an array of zeros with 97 elements\n",
    "colnames_96 = np.zeros(96, dtype=object)\n",
    "\n",
    "# Read the header file\n",
    "with open(header_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines or comments\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        # The second token is the 'name'\n",
    "        parts = line.split()\n",
    "        colnames_96[int(parts[0])] = parts[1]\n",
    "\n",
    "#For single lenses they are (***Note for these, the mass of the lens is given by the planet mass column, not the host mass column):\n",
    "#72 - unimportant\n",
    "#73 - N, number of consecutive W149 data points deviating by >=3 sigma from a flat line\n",
    "#74 - unimportant\n",
    "#75 - Delta chi^2 (relative to a flat line)\n",
    "#76-91 - unimportant\n",
    "#92 - simulated event type (dcnormffp = single lens or free-floating planet)\n",
    "#93 - unimportant (I think)\n",
    "#94 - lightcurve filename root\n",
    "#95 - Data challenge lightcurve number\n",
    "\n",
    "# Replace the column names in colnames_96\n",
    "colnames_96[73] = 'N'\n",
    "colnames_96[75] = 'Delta chi2'\n",
    "colnames_96[92] = 'sim type'\n",
    "colnames_96[94] = 'filename'\n",
    "colnames_96[95] = 'lc_number'\n",
    "\n",
    "# Make sure the column names are unique\n",
    "for i in range(94):\n",
    "    if colnames_96[i] == '|' or colnames_96[i] == 0:\n",
    "        colnames_96[i] = 'col_' + str(i)\n",
    "\n",
    "# Replace the column names in the data_frame\n",
    "df_sl.columns = colnames_96\n",
    "\n",
    "# Remove the dummy columns 'col_*'\n",
    "df_sl = df_sl.loc[:, ~df_sl.columns.str.startswith('col_')]\n",
    "\n",
    "df_sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejq8QsYFxI-t"
   },
   "source": [
    "The last column in this data frame has the lightcurve number, which we can use to pick out only the lightcurves matching our single-lens event list, for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "W5OmYak-xGVK",
    "outputId": "3dc25d6c-4170-49c6-ab16-7d7d0a25c8ee"
   },
   "outputs": [],
   "source": [
    "#@title Figuring out which files we want\n",
    "\n",
    "lc_number = df_sl['lc_number'].to_numpy()\n",
    "\n",
    "lc_file_path_format = 'data-challenge-1/lc/ulwdc1_XXX_filter.txt'\n",
    "\n",
    "lc_file_paths_W149 = [lc_file_path_format.replace('filter', 'W149')] * len(lc_number)\n",
    "lc_file_paths_Z087 = [lc_file_path_format.replace('filter', 'Z087')] * len(lc_number)\n",
    "\n",
    "# replace XXX, from the right, with the lc_number which is not necessarily of length 3\n",
    "lc_file_paths_W149 = [path.replace('XXX', str(num).zfill(3)) for path, num in zip(lc_file_paths_W149, lc_number)]\n",
    "lc_file_paths_Z087 = [path.replace('XXX', str(num).zfill(3)) for path, num in zip(lc_file_paths_Z087, lc_number)]\n",
    "\n",
    "df_sl['lc_file_path_W149'] = lc_file_paths_W149\n",
    "df_sl['lc_file_path_Z087'] = lc_file_paths_Z087\n",
    "\n",
    "df_sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1nERAlwxIk6"
   },
   "source": [
    "There are a few pieces of information that may need to be known for each event that are not in the lightcurve files. These are stored in event_info.txt\n",
    "\n",
    "Columns: `\"Event_name\"` `\"Event_number\"` `\"RA_(deg)\"` `\"Dec_(deg)\"` `\"Distance\"` `\"A_W149\"` `\"sigma_A_W149\"` `\"A_Z087\"` `\"sigma_A_Z087\"`\n",
    "\n",
    "Distance, A_W149/Z087 are an estimate of the distance and extinction in each band of the red clump stars. sigma_A_W149/Z087 are dispersions in the extinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "kX035n49xXov",
    "outputId": "4f29b8f0-c338-4c79-a679-dcfb8798ebd8"
   },
   "outputs": [],
   "source": [
    "#@title Event information data frame\n",
    "\n",
    "header = [\"Event_name\",\n",
    "          \"Event_number\",\n",
    "          \"RA_(deg)\",\n",
    "          \"Dec_(deg)\",\n",
    "          \"Distance\",\n",
    "          \"A_W149\",\n",
    "          \"sigma_A_W149\",\n",
    "          \"A_Z087\",\n",
    "          \"sigma_A_Z087\"\n",
    "]\n",
    "\n",
    "event_info = pd.read_csv('./data-challenge-1/event_info.txt', names=header, delim_whitespace=True)\n",
    "event_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "kYO43WegxcoQ",
    "outputId": "532b27ad-3bbd-4884-8c1b-5245394e7258"
   },
   "outputs": [],
   "source": [
    "#@title Combining the two data frames\n",
    "\n",
    "# Convert 'lc_number' to numeric type before merging\n",
    "merged_sl_df = pd.merge(event_info, df_sl.astype({'lc_number': 'int64'}), left_on='Event_number', right_on='lc_number', how='inner')\n",
    "merged_sl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XagtOAQx3Uh"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> Binary Lens Events </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n_RahyAx9YA"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> Triple Lens Events </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "BbIYV9J3xh6h",
    "outputId": "6b63215e-677b-4022-cc07-f51c2ff6c9a2"
   },
   "outputs": [],
   "source": [
    "try:  # this will only work on Colab.\n",
    "  sl_sheet = sheets.InteractiveSheet(df=merged_sl_df)\n",
    "  sl_sheet.show()\n",
    "  bl_sheet = sheets.InteractiveSheet(df=df_bl)\n",
    "  bl_sheet.show()\n",
    "except:\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h96szZeOxYyb"
   },
   "source": [
    "Great - data successfully wrangled. Let's forget we ever had to live through that and move right along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJmKF1cMyzho"
   },
   "source": [
    "## <font face=\"Helvetica\" size=\"6\"> Packages Covered in This Notebook </font>\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "<summary><font face=\"Helvetica\" size=\"5\">1. MulensModel</font></summary>\n",
    "\n",
    "[Go to the `MulensModel` section](#mulensmodel)\n",
    "  * [1.1 Data objects](#11-data-objects)\n",
    "  * [1.2 Model objects](#12-model-objects)\n",
    "  * [1.3 Event objects](#13-event-objetcs)\n",
    "  * [1.4 Fitting](#fitting-framework)\n",
    "  * [1.5 Higher-order effects](#model-examples)\n",
    "    - [1S1L](#1s1l-example)\n",
    "    - [1S2L](#1s2l-example)\n",
    "    - [1S3L](#1s3l-example)\n",
    "    - [2S1L](#2s1l-example)\n",
    "    - [Parallax](#parallax-example)\n",
    "</details>\n",
    "<details>\n",
    "<summary><font face=\"Helvetica\" size=\"5\">2. pyLIMA</font></summary>\n",
    "\n",
    "[Go to the `pyLIMA` section](#pylima)\n",
    "  * [2.1 Installation and Setup](#pylima-installation)\n",
    "  * [2.2 Basic Usage](#pylima-basic)\n",
    "  * [2.3 Model Examples](#pylima-models)\n",
    "    - [2.3.1 Single Lens (PSPL)](#pylima-pspl)\n",
    "    - [2.3.2 Finite Source (FSPL)](#pylima-fspl)\n",
    "    - [2.3.3 Simulation Capabilities](#pylima-simulation)\n",
    "  * [2.4 Advanced Features](#pylima-advanced)\n",
    "    - [2.4.1 Custom Parameter Definitions](#pylima-custom-params)\n",
    "    - [2.4.2 Multiple Telescopes and Filters](#pylima-multiple-telescopes)\n",
    "  * [2.5 Performance and Best Practices](#pylima-performance)\n",
    "</details>\n",
    "<details>\n",
    "<summary><font face=\"Helvetica\" size=\"5\">4. RTModel</font></summary>\n",
    "\n",
    "[Go to the `RTModel` section](#rtmodel)\n",
    "  * [3.1 Installation and Setup](#rtmodel-installation)\n",
    "  * [3.2 Data Preparation](#rtmodel-data-prep)\n",
    "  * [3.3 Basic Usage](#rtmodel-basic-usage)\n",
    "  * [3.4 Understanding RTModel Output](#rtmodel-output)\n",
    "  * [3.5 Model Categories](#rtmodel-model-categories)\n",
    "  * [3.6 Visualization and Results](#rtmodel-visualization)\n",
    "  * [3.7 Advanced Features](#rtmodel-advanced)\n",
    "  * [3.8 RTModel vs Other Tools](#rtmodel-comparison)\n",
    "  * [3.9 Astrophotometric Fitting (RTModel v3.0)](#rtmodel-astrometric)\n",
    "    - [3.9.1 Astrophotometric Data Format](#rtmodel-astrometric-data)\n",
    "    - [3.9.2 Astrophotometric Parameters](#rtmodel-astrometric-parameters)\n",
    "    - [3.9.3 Astrophotometric Fitting](#rtmodel-astrometric-fitting)\n",
    "    - [3.9.4 Astrometric Visualization](#rtmodel-astrometric-visualization)\n",
    "    - [3.9.5 Applications and Use Cases](#rtmodel-astrometric-applications)\n",
    "</details>\n",
    "<details>\n",
    "<summary><font face=\"Helvetica\" size=\"5\">4. popclass</font></summary>\n",
    "\n",
    "[Go to the `popclass` section](#popclass)\n",
    "  * [4.1 Installation](#popclass-installation)\n",
    "  * [4.2 Basic Usage Example](#popclass-basic-usage)\n",
    "  * [4.3 How It Works](#popclass-theory)\n",
    "  * [4.4 Population Models](#popclass-models)\n",
    "  * [4.5 Advanced Features](#popclass-advanced)\n",
    "  * [4.6 Best Practices & Caveats](#popclass-best-practices)\n",
    "  * [4.7 Further Reading](#popclass-references)\n",
    "</details>\n",
    "<!--\n",
    "<details>\n",
    "<summary><font face=\"Helvetica\" size=\"5\">5. BAGEL</font></summary> -->\n",
    "<!--\n",
    "[Go to the `BAGEL` section](#bagel)\n",
    "</details>\n",
    "<details>\n",
    "<summary><font face=\"Helvetica\" size=\"5\">6. MuLAN</font></summary> -->\n",
    "<!--\n",
    "[Go to the `MuLAN` section](#mulan)\n",
    "</details> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svCZcYocGgmW"
   },
   "source": [
    "## 1. MulensModel\n",
    "\n",
    "***\n",
    "\n",
    "`MulensModel` is arguably the simplest and most well-known microlensing modelling tool. It is an extremely capable tool, having a vast array of models and higher-order effects, but the developers do not attempt to automate the model selection process or infer physical parameter, as of yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4r2waMHJOfy"
   },
   "outputs": [],
   "source": [
    "#@title Importing the package\n",
    "import MulensModel as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qzz9eSGVIyTB"
   },
   "outputs": [],
   "source": [
    "#@title Clearing the old `data` directory/file\n",
    "# check this box if you would like to replace the data folder\n",
    "replace_data_folder = False #@param {type:\"boolean\"}\n",
    "skip_next_cell = False\n",
    "\n",
    "mulensmodel_dir = os.path.dirname(mm.__file__)\n",
    "data_file_path = os.path.join(mulensmodel_dir, 'data')\n",
    "\n",
    "if os.path.exists(data_file_path):\n",
    "  # make sure we have permissions to delete the data file\n",
    "  os.chmod(os.path.join(mulensmodel_dir, \"data\"), 0o777)\n",
    "\n",
    "  if os.path.isfile(data_file_path):\n",
    "    os.remove(data_file_path)\n",
    "    print(f\"Removed 'data' file from {mulensmodel_dir}\")\n",
    "  elif replace_data_folder:\n",
    "    shutil.rmtree(data_file_path)\n",
    "    print(f\"Removed 'data' directory from {mulensmodel_dir}\")\n",
    "  else:\n",
    "    skip_next_cell = True\n",
    "    print(\"\"\"MulensModel build looks correct. If it is not working, try selecting\n",
    "     `replace_data_folder` and running the subsection again.\"\"\")\n",
    "else:\n",
    "  print(f\"No 'data' file or directory found in {mulensmodel_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTi5AaPPLTov"
   },
   "outputs": [],
   "source": [
    "#@title Getting the data tree and cleaning up\n",
    "if not skip_next_cell:\n",
    "    # change the working directory to the mulensmodel_dir directory\n",
    "    os.chdir(mulensmodel_dir)\n",
    "\n",
    "    # starting from a fresh clone\n",
    "    if os.path.exists(os.path.join(mulensmodel_dir, \"MulensModel\")):\n",
    "        # remove the MulensModel repository\n",
    "        !chmod -R u+w MulensModel\n",
    "        !rm -r MulensModel\n",
    "\n",
    "    # clone the MulensModel repository\n",
    "    !git clone https://github.com/rpoleski/MulensModel.git\n",
    "\n",
    "    # move the data directory to the mulensmodel_dir directory\n",
    "    !mv MulensModel/data ./\n",
    "\n",
    "    # remove the MulensModel repository\n",
    "    !chmod -R u+w MulensModel\n",
    "    !rm -r MulensModel\n",
    "\n",
    "    # change the working directory back to the original directory\n",
    "    reset_cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuT7Llu-FtfS"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 1.1 Data Objects </font>\n",
    "\n",
    "When fitting Roman data with `MulensModel`, we need to make a minor adjustment to our model for data that is not ground based. In `MulensModel` this simply means adding the following keyword to the data object initialization: `ephemerides_file=PATH_TO_THE_FILE`.\n",
    "\n",
    "> Instructions specific to this data set for `MulensModel` are given [here](https://github.com/rpoleski/MulensModel/blob/master/documents/data_challenge.md).\n",
    "\n",
    "Most of the data for these events is in the W147 band, so we make the very reasonable decision to just fit those data and not have to deal with mutliple data sets with different $F_\\textrm{S}$ and $F_\\textrm{B}$ values. That should speen up our fits too. If we wanted to find the color of the source star at a later date we could fit just the flux parameters and leave the microlensing-model parameters fixed (as described in [this notebook](https://github.com/rges-pit/TheMicrolensersGuideToTheGalaxy/blob/main/Notebooks/SingleLens.ipynb)) using a linear regression, which would take a fraction of a second per event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYLdgDpqTgZ8",
    "outputId": "b96a1c4e-3b47-4905-f0e2-0561a46a5ca6"
   },
   "outputs": [],
   "source": [
    "#@title Collecting the relevent meta data\n",
    "data_file = merged_sl_df['lc_file_path_W149'][0]\n",
    "ra = merged_sl_df['RA_(deg)'][0]\n",
    "dec = merged_sl_df['Dec_(deg)'][0]\n",
    "print(ra, dec)\n",
    "\n",
    "# convert decimal ra and dec in degrees to \"17h57m16.56s -29d05m20.04s\"\n",
    "coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame='icrs')\n",
    "hms_dms_string = coord.to_string('hmsdms')\n",
    "print(f\"SkyCoord default: {hms_dms_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q1arR-nGW4u"
   },
   "outputs": [],
   "source": [
    "#@title Example L2 `Data` object\n",
    "\n",
    "# Here is the main difference for space data - we provide the ephemeris for Roman:\n",
    "EPHEM_FILE = 'data-challenge-1/wfirst_ephemeris_W149.txt'\n",
    "data_Roman_W149 = mm.MulensData(file_name=data_file,\n",
    "                                phot_fmt='mag',\n",
    "                                ephemerides_file=EPHEM_FILE,\n",
    "                                plot_properties={'color': '#a859e4',\n",
    "                                                 'label': 'Roman W149'\n",
    "                                                 },\n",
    "                                bandpass='H'\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5fC2Ks5Ts0N"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 1.2 Model Objects </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b3IdJjQT3Qw"
   },
   "outputs": [],
   "source": [
    "#@title Collecting the meta data\n",
    "# split the parallax equally out of a lack of better ideas\n",
    "pi_E_E = np.sqrt(float(merged_sl_df['piE'][0])**2 / 2.0)\n",
    "pi_E_N = pi_E_E * 1.0\n",
    "\n",
    "t_0 = float(merged_sl_df['t0'][0])\n",
    "# Annoyingly, t_0 is in simulation time not HJD so we need to do a conversion\n",
    "# (https://github.com/microlensing-data-challenge/evaluation_code/blob/master/parse_table1.py)\n",
    "# line 402\n",
    "t_0 = t_0 + 2458234.0  # simulation 0 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR9_hxDaUAuN"
   },
   "outputs": [],
   "source": [
    "#@title Making a dictionary of the guess parameters\n",
    "# Let's just tidy the \"guess\" parameters up into a dictionary, for easy accesss\n",
    "params = dict()\n",
    "parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\", \"rho\", \"pi_E_N\", \"pi_E_E\"]\n",
    "params['t_0'] = t_0 * 1.0\n",
    "params['t_0_par'] = t_0 * 1.0\n",
    "params['u_0'] = float(merged_sl_df['u0'][0]) * 1.1\n",
    "params['t_E'] = float(merged_sl_df['tE'][0]) * 1.1\n",
    "params['rho'] = float(merged_sl_df['rhos'][0]) * 1.1\n",
    "params['pi_E_N'] = pi_E_E\n",
    "params['pi_E_E'] = pi_E_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm3syBy5Sjc6"
   },
   "outputs": [],
   "source": [
    "#@title Example `Model` object\n",
    "\n",
    "# If we are using parallax, it is also important that we provide the event\n",
    "# coordinates, or MulensModel can't do necessary calculations\n",
    "Roman_model = mm.Model({**params},\n",
    "                        coords=coord,\n",
    "                        ephemerides_file=EPHEM_FILE\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1SlSsxvU1al"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 1.3 Event Objects </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VKhMYSTTDok"
   },
   "outputs": [],
   "source": [
    "#@title Example `Event` object\n",
    "\n",
    "Roman_event = mm.Event(datasets=data_Roman_W149, model=Roman_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fitting_framework"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 1.4 Fitting Framework </font>\n",
    "\n",
    "Now we'll set up a comprehensive fitting framework that can handle different model types. We'll use `emcee` for MCMC sampling and demonstrate various microlensing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fitting_setup",
    "outputId": "f409c22d-ba34-430e-c0dd-d7d5123130b1"
   },
   "outputs": [],
   "source": [
    "#@title Import additional fitting tools\n",
    "import emcee\n",
    "from scipy.optimize import minimize\n",
    "import corner\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "likelihood_function"
   },
   "outputs": [],
   "source": [
    "#@title Define likelihood function for fitting\n",
    "def log_likelihood(theta, event, parameters_to_fit):\n",
    "    \"\"\"Log likelihood function for emcee\"\"\"\n",
    "    try:\n",
    "        # Update model parameters\n",
    "        for i, param_name in enumerate(parameters_to_fit):\n",
    "            if param_name == 'log_rho':\n",
    "                # Convert log_rho back to rho\n",
    "                event.model.parameters.rho = 10**theta[i]\n",
    "            else:\n",
    "                setattr(event.model.parameters, param_name, theta[i])\n",
    "\n",
    "        # Fit the fluxes given the current model parameters\n",
    "        event.fit_fluxes()\n",
    "\n",
    "        # Get the source and blend fluxes\n",
    "        ([F_S], F_B) = event.get_flux_for_dataset(event.datasets[0])\n",
    "\n",
    "        # Calculate chi-squared\n",
    "        chi2 = event.get_chi2()\n",
    "\n",
    "        # Add flux priors if needed (optional)\n",
    "        penalty = 0.0\n",
    "        if F_B <= 0:\n",
    "            penalty = ((F_B / 100)**2)  # Penalize negative blend flux\n",
    "        if F_S <= 0 or (F_S + F_B) <= 0:\n",
    "            return -np.inf  # Return inf if fluxes are non-physical\n",
    "\n",
    "        return -0.5 * chi2 - penalty\n",
    "    except:\n",
    "        return -np.inf\n",
    "\n",
    "def run_mcmc_fit(event, parameters_to_fit, initial_guess, nwalkers=32, nsteps=1000, burnin=200):\n",
    "    \"\"\"Run MCMC fitting with emcee\"\"\"\n",
    "    ndim = len(parameters_to_fit)\n",
    "\n",
    "    # Initial positions (slightly perturbed from initial guess)\n",
    "    pos = np.array(initial_guess) + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "    # Use ThreadPool for notebook compatibility\n",
    "    n_cores = mp.cpu_count()\n",
    "    print(f\"Using {n_cores} threads for MCMC\")\n",
    "\n",
    "    with ThreadPool(processes=n_cores) as pool:\n",
    "        sampler = emcee.EnsembleSampler(\n",
    "            nwalkers, ndim, log_likelihood,\n",
    "            args=(event, parameters_to_fit),\n",
    "            pool=pool\n",
    "        )\n",
    "        sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_examples"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 1.5 Model Examples </font>\n",
    "\n",
    "Now let's demonstrate different microlensing models. We'll start with simple models and work our way up to more complex ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s1l_example"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 1.5.1 Single Lens Single Source (1S1L) with Finite Source Effects </font>\n",
    "\n",
    "Let's start with a simple single lens model that includes finite source effects (\u03c1 parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1s1l_setup",
    "outputId": "8000741f-ccf9-47cb-a23d-5e4d7c48106f"
   },
   "outputs": [],
   "source": [
    "#@title 1S1L Model Setup\n",
    "# Select a single lens event for demonstration\n",
    "event_idx = 0  # First single lens event\n",
    "data_file = merged_sl_df['lc_file_path_W149'][event_idx]\n",
    "ra = merged_sl_df['RA_(deg)'][event_idx]\n",
    "dec = merged_sl_df['Dec_(deg)'][event_idx]\n",
    "coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame='icrs')\n",
    "\n",
    "# Get truth parameters for comparison\n",
    "t_0_truth = float(merged_sl_df['t0'][event_idx]) + 2458234.0  # Convert to HJD\n",
    "u_0_truth = float(merged_sl_df['u0'][event_idx])\n",
    "t_E_truth = float(merged_sl_df['tE'][event_idx])\n",
    "rho_truth = float(merged_sl_df['rhos'][event_idx])\n",
    "\n",
    "print(f\"Event {event_idx}: Truth parameters\")\n",
    "print(f\"  t_0: {t_0_truth:.3f}\")\n",
    "print(f\"  u_0: {u_0_truth:.4f}\")\n",
    "print(f\"  t_E: {t_E_truth:.2f} days\")\n",
    "print(f\"  \u03c1: {rho_truth:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s1l_data"
   },
   "outputs": [],
   "source": [
    "#@title Load data for 1S1L example\n",
    "# Load the light curve data\n",
    "data_1s1l = mm.MulensData(file_name=data_file,\n",
    "                          phot_fmt='mag',\n",
    "                          ephemerides_file=EPHEM_FILE,\n",
    "                          plot_properties={'color': '#a859e4', 'label': 'Roman W149'},\n",
    "                          bandpass='H')\n",
    "\n",
    "# Create initial model with finite source effects\n",
    "params_1s1l = {\n",
    "    't_0': t_0_truth * 1.0,  # Start with truth values\n",
    "    'u_0': u_0_truth * 1.1,  # Slightly perturb\n",
    "    't_E': t_E_truth * 1.1,\n",
    "    'rho': rho_truth * 1.1,\n",
    "}\n",
    "\n",
    "model_1s1l = mm.Model(params_1s1l, coords=coord, ephemerides_file=EPHEM_FILE)\n",
    "event_1s1l = mm.Event(datasets=data_1s1l, model=model_1s1l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "U9Q_NwtfA6f1",
    "outputId": "8f7213eb-6919-43b4-83e2-6cc2e026b025"
   },
   "outputs": [],
   "source": [
    "# Plot the initial model\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_1s1l.plot_data()\n",
    "event_1s1l.plot_model(color='black', label='Initial Model')\n",
    "plt.title(f\"1S1L Event {event_idx} - Initial Model\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.xlim(2459810.0, 2459822.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "1s1l_fit",
    "outputId": "a84d255e-188a-4f7b-c31a-d1d3e8cf6cf9"
   },
   "outputs": [],
   "source": [
    "#@title Fit 1S1L model with MCMC\n",
    "# Set up parameters to fit\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor as ThreadPool\n",
    "\n",
    "parameters_to_fit_1s1l = [\"t_0\", \"u_0\", \"t_E\", \"rho\"]\n",
    "initial_guess_1s1l = [t_0_truth, u_0_truth, t_E_truth, rho_truth]\n",
    "\n",
    "print(\"Starting 1S1L MCMC fit...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run MCMC\n",
    "sampler_1s1l = run_mcmc_fit(event_1s1l, parameters_to_fit_1s1l, initial_guess_1s1l,\n",
    "                           nwalkers=32, nsteps=500, burnin=100)\n",
    "\n",
    "# Get results\n",
    "samples_1s1l = sampler_1s1l.chain[:, 100:, :].reshape((-1, 4))\n",
    "best_fit_1s1l = np.median(samples_1s1l, axis=0)\n",
    "uncertainties_1s1l = np.std(samples_1s1l, axis=0)\n",
    "\n",
    "fit_time = time.time() - start_time\n",
    "print(f\"\\n1S1L Fit completed in {fit_time:.1f} seconds\")\n",
    "print(\"\\nResults:\")\n",
    "for i, param in enumerate(parameters_to_fit_1s1l):\n",
    "    print(f\"  {param}: {best_fit_1s1l[i]:.6f} \u00b1 {uncertainties_1s1l[i]:.6f}\")\n",
    "\n",
    "# Compare with truth\n",
    "print(\"\\nComparison with truth:\")\n",
    "truth_values = [t_0_truth, u_0_truth, t_E_truth, rho_truth]\n",
    "for i, param in enumerate(parameters_to_fit_1s1l):\n",
    "    diff = best_fit_1s1l[i] - truth_values[i]\n",
    "    sigma_diff = abs(diff) / uncertainties_1s1l[i]\n",
    "    print(f\"  {param}: {diff:.6f} ({sigma_diff:.2f}\u03c3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s1l_results"
   },
   "outputs": [],
   "source": [
    "#@title Plot 1S2L results (Best Model)\n",
    "# Update model with best fit parameters\n",
    "for i, param_name in enumerate(parameters_to_fit_1s2l):\n",
    "    setattr(event_1s2l.model.parameters, param_name, best_fit_1s2l[i])\n",
    "event_1s2l.fit_fluxes()\n",
    "\n",
    "# Plot light curve with best fit\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_1s2l.plot_data()\n",
    "event_1s2l.plot_model(color='red', label='Best Fit')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"1S2L Synthetic Event - Best Fit Model\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot corner plot\n",
    "fig = corner.corner(samples_1s2l, labels=parameters_to_fit_1s2l,\n",
    "                    truths=truth_values_1s2l, quantiles=[0.16, 0.5, 0.84])\n",
    "plt.suptitle(\"1S2L Event - Parameter Posteriors\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Plot updated caustic structure\n",
    "plt.figure(figsize=(8, 8))\n",
    "event_1s2l.model.plot_caustics()\n",
    "plt.title(\"1S2L Event - Best Fit Caustic Structure\")\n",
    "plt.xlabel(\"x (Einstein radii)\")\n",
    "plt.ylabel(\"y (Einstein radii)\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s2l_example"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 1.5.2 Single Source Binary Lens (1S2L) with Finite Source Effects </font>\n",
    "\n",
    "Now let's demonstrate a binary lens model. We'll use a simulated binary lens event to show how to fit for the additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s2l_setup"
   },
   "outputs": [],
   "source": [
    "#@title 1S2L Model Setup\n",
    "# For this example, we'll create a simulated binary lens event\n",
    "# In practice, you would load real binary lens data\n",
    "\n",
    "# Create synthetic binary lens data\n",
    "def create_binary_lens_data(t_0=2459123.5, u_0=0.1, t_E=25.0, q=0.1, s=1.2, alpha=45.0, rho=0.001):\n",
    "    \"\"\"Create synthetic binary lens light curve\"\"\"\n",
    "    # Time array around the event\n",
    "    t = np.linspace(t_0 - 2*t_E, t_0 + 2*t_E, 200)\n",
    "\n",
    "    # Create binary lens model\n",
    "    params = {\n",
    "        't_0': t_0,\n",
    "        'u_0': u_0,\n",
    "        't_E': t_E,\n",
    "        'q': q,  # mass ratio\n",
    "        's': s,  # separation\n",
    "        'alpha': alpha,  # angle\n",
    "        'rho': rho  # finite source\n",
    "    }\n",
    "\n",
    "    model = mm.Model(params, coords=coord, ephemerides_file=EPHEM_FILE)\n",
    "\n",
    "    # Calculate magnification\n",
    "    magnification = model.get_magnification(t)\n",
    "\n",
    "    # Add noise\n",
    "    mag_0 = 18.0  # baseline magnitude\n",
    "    mag = mag_0 - 2.5 * np.log10(magnification)\n",
    "    mag_err = 0.01 * np.ones_like(mag)  # 1% photometric error\n",
    "\n",
    "    # Add some scatter\n",
    "    mag += np.random.normal(0, mag_err)\n",
    "\n",
    "    return t, mag, mag_err, params\n",
    "\n",
    "# Create the synthetic data\n",
    "t_binary, mag_binary, mag_err_binary, truth_params_binary = create_binary_lens_data()\n",
    "\n",
    "print(\"Synthetic 1S2L Event - Truth Parameters:\")\n",
    "for key, value in truth_params_binary.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s2l_data"
   },
   "outputs": [],
   "source": [
    "#@title Create 1S2L data object\n",
    "# Create data object from synthetic data\n",
    "data_list = [t_binary, mag_binary, mag_err_binary]\n",
    "data_1s2l = mm.MulensData(data_list=data_list,\n",
    "                          phot_fmt='mag',\n",
    "                          ephemerides_file=EPHEM_FILE,\n",
    "                          plot_properties={'color': '#a859e4', 'label': 'Synthetic Binary'},\n",
    "                          bandpass='H')\n",
    "\n",
    "# Create initial model (slightly perturbed from truth)\n",
    "params_1s2l = {k: v * (1.0 + 0.1 * np.random.randn()) for k, v in truth_params_binary.items()}\n",
    "model_1s2l = mm.Model(params_1s2l, coords=coord, ephemerides_file=EPHEM_FILE)\n",
    "event_1s2l = mm.Event(datasets=data_1s2l, model=model_1s2l)\n",
    "\n",
    "# Plot the initial model\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_1s2l.plot_data()\n",
    "event_1s2l.plot_model(color='red', label='Initial Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"1S2L Synthetic Event - Initial Model\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot caustic structure\n",
    "plt.figure(figsize=(8, 8))\n",
    "event_1s2l.plot_caustics()\n",
    "plt.title(\"1S2L Event - Caustic Structure\")\n",
    "plt.xlabel(\"x (Einstein radii)\")\n",
    "plt.ylabel(\"y (Einstein radii)\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s2l_fit"
   },
   "outputs": [],
   "source": [
    "#@title Fit 1S2L model with MCMC\n",
    "# Set up parameters to fit (excluding rho for simplicity in this example)\n",
    "parameters_to_fit_1s2l = [\"t_0\", \"u_0\", \"t_E\", \"q\", \"s\", \"alpha\"]\n",
    "initial_guess_1s2l = [params_1s2l[p] for p in parameters_to_fit_1s2l]\n",
    "\n",
    "print(\"Starting 1S2L MCMC fit...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run MCMC with more walkers and steps for binary lens\n",
    "sampler_1s2l = run_mcmc_fit(event_1s2l, parameters_to_fit_1s2l, initial_guess_1s2l,\n",
    "                           nwalkers=64, nsteps=1000, burnin=200)\n",
    "\n",
    "# Get results\n",
    "samples_1s2l = sampler_1s2l.chain[:, 200:, :].reshape((-1, 6))\n",
    "best_fit_1s2l = np.median(samples_1s2l, axis=0)\n",
    "uncertainties_1s2l = np.std(samples_1s2l, axis=0)\n",
    "\n",
    "fit_time = time.time() - start_time\n",
    "print(f\"\\n1S2L Fit completed in {fit_time:.1f} seconds\")\n",
    "print(\"\\nResults:\")\n",
    "for i, param in enumerate(parameters_to_fit_1s2l):\n",
    "    print(f\"  {param}: {best_fit_1s2l[i]:.6f} \u00b1 {uncertainties_1s2l[i]:.6f}\")\n",
    "\n",
    "# Compare with truth\n",
    "print(\"\\nComparison with truth:\")\n",
    "truth_values_1s2l = [truth_params_binary[p] for p in parameters_to_fit_1s2l]\n",
    "for i, param in enumerate(parameters_to_fit_1s2l):\n",
    "    diff = best_fit_1s2l[i] - truth_values_1s2l[i]\n",
    "    sigma_diff = abs(diff) / uncertainties_1s2l[i]\n",
    "    print(f\"  {param}: {diff:.6f} ({sigma_diff:.2f}\u03c3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s2l_results"
   },
   "outputs": [],
   "source": [
    "#@title Plot 1S2L results\n",
    "# Update model with best fit parameters\n",
    "for i, param_name in enumerate(parameters_to_fit_1s2l):\n",
    "    setattr(event_1s2l.model.parameters, param_name, best_fit_1s2l[i])\n",
    "event_1s2l.fit_fluxes()\n",
    "\n",
    "# Plot light curve with best fit\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_1s2l.plot_data()\n",
    "event_1s2l.plot_model(color='red', label='Best Fit')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"1S2L Synthetic Event - Best Fit Model\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot corner plot\n",
    "fig = corner.corner(samples_1s2l, labels=parameters_to_fit_1s2l,\n",
    "                    truths=truth_values_1s2l, quantiles=[0.16, 0.5, 0.84])\n",
    "plt.suptitle(\"1S2L Event - Parameter Posteriors\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Plot updated caustic structure\n",
    "plt.figure(figsize=(8, 8))\n",
    "event_1s2l.model.plot_caustics()\n",
    "plt.title(\"1S2L Event - Best Fit Caustic Structure\")\n",
    "plt.xlabel(\"x (Einstein radii)\")\n",
    "plt.ylabel(\"y (Einstein radii)\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s3l_example"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 1.5.3 Single Source Triple Lens (1S3L) </font>\n",
    "\n",
    "Triple lens systems are rare but important for detecting hierarchical planetary systems. Here we demonstrate how to set up a triple lens model.\n",
    "\n",
    "**Triple Lens Model Parameters:**\n",
    "- **Primary lens**: t_0, u_0, t_E\n",
    "- **First companion**: q1, s1, alpha1\n",
    "- **Second companion**: q2, s2, alpha2, phi\n",
    "- **Finite source**: rho\n",
    "- **Higher order**: pi_E_N, pi_E_E\n",
    "\n",
    "**Example Triple Lens Parameters (hierarchical system):**\n",
    "- t_0: 2459123.5\n",
    "- u_0: 0.1\n",
    "- t_E: 25.0\n",
    "- q1: 0.1 (First companion - planet)\n",
    "- s1: 1.2 (Separation of first companion)\n",
    "- alpha1: 45.0\n",
    "- q2: 0.01 (Second companion - moon)\n",
    "- s2: 0.1 (Separation of second companion relative to first)\n",
    "- alpha2: 30.0\n",
    "- phi: 60.0 (Angle between companions)\n",
    "- rho: 0.001 (Finite source)\n",
    "\n",
    "**Note**: This represents a hierarchical triple system with:\n",
    "- Primary lens (star)\n",
    "- First companion (planet at ~1.2 Einstein radii)\n",
    "- Second companion (moon at ~0.1 Einstein radii from planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s3l_setup"
   },
   "outputs": [],
   "source": [
    "#@title 1S3L Model Setup\n",
    "# Note: MulensModel doesn't natively support triple lenses\n",
    "# We'll demonstrate the concept and show how to extend it\n",
    "\n",
    "# For triple lenses, we need additional parameters:\n",
    "# - q2: mass ratio of second companion\n",
    "# - s2: separation of second companion\n",
    "# - alpha2: angle of second companion\n",
    "# - phi: angle between the two companions\n",
    "\n",
    "print(\"Triple Lens Model Parameters:\")\n",
    "print(\"  Primary lens: t_0, u_0, t_E\")\n",
    "print(\"  First companion: q1, s1, alpha1\")\n",
    "print(\"  Second companion: q2, s2, alpha2, phi\")\n",
    "print(\"  Finite source: rho\")\n",
    "print(\"  Higher order: pi_E_N, pi_E_E\")\n",
    "\n",
    "# Example triple lens parameters (hierarchical system)\n",
    "triple_lens_params = {\n",
    "    't_0': 2459123.5,\n",
    "    'u_0': 0.1,\n",
    "    't_E': 25.0,\n",
    "    'q1': 0.1,    # First companion (planet)\n",
    "    's1': 1.2,    # Separation of first companion\n",
    "    'alpha1': 45.0,\n",
    "    'q2': 0.01,   # Second companion (moon)\n",
    "    's2': 0.1,    # Separation of second companion (relative to first)\n",
    "    'alpha2': 30.0,\n",
    "    'phi': 60.0,  # Angle between companions\n",
    "    'rho': 0.001  # Finite source\n",
    "}\n",
    "\n",
    "print(\"\\nExample Triple Lens Parameters:\")\n",
    "for key, value in triple_lens_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nNote: This represents a hierarchical triple system with:\")\n",
    "print(\"  - Primary lens (star)\")\n",
    "print(\"  - First companion (planet at ~1.2 Einstein radii)\")\n",
    "print(\"  - Second companion (moon at ~0.1 Einstein radii from planet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s3l_implementation"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> Triple Lens Implementation Notes </font>\n",
    "\n",
    "MulensModel doesn't natively support triple lenses, but here's how you could extend it:\n",
    "\n",
    "**Triple Lens Implementation Options:**\n",
    "\n",
    "**1. Use VBMicrolensing (covered in section 3):**\n",
    "- Native support for multiple lenses\n",
    "- More efficient for complex lens systems\n",
    "- Better suited for hierarchical systems\n",
    "\n",
    "**2. Extend MulensModel:**\n",
    "- Create custom model class\n",
    "- Implement triple lens magnification calculation\n",
    "- Requires significant development effort\n",
    "\n",
    "**3. Use triplelens package:**\n",
    "- Specialized for triple lens calculations\n",
    "- GitHub: https://github.com/rkkuang/triplelens\n",
    "- Limited integration with other tools\n",
    "\n",
    "**4. Approximate approach:**\n",
    "- Treat as hierarchical binary systems\n",
    "- Fit inner binary first, then outer binary\n",
    "- May miss some triple lens effects\n",
    "\n",
    "**Framework for Triple Lens Fitting:**\n",
    "```python\n",
    "parameters_to_fit_1s3l = [\n",
    "    't_0', 'u_0', 't_E',\n",
    "    'q1', 's1', 'alpha1',\n",
    "    'q2', 's2', 'alpha2', 'phi',\n",
    "    'rho'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s1l_example"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 1.5.4 Binary Source Single Lens (2S1L) </font>\n",
    "\n",
    "Binary source events occur when the source star is actually a binary system. This can create distinctive light curve features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s1l_setup"
   },
   "outputs": [],
   "source": [
    "#@title 2S1L Model Setup\n",
    "# Binary source parameters:\n",
    "# - t_0, u_0, t_E: standard microlensing parameters\n",
    "# - q_S: flux ratio of source components\n",
    "# - rho_S: angular separation of source components\n",
    "# - theta_S: angle of source binary\n",
    "\n",
    "# Create synthetic binary source data\n",
    "def create_binary_source_data(t_0=2459123.5, u_0=0.1, t_E=25.0, q_S=0.8, rho_S=0.01, theta_S=45.0):\n",
    "    \"\"\"Create synthetic binary source light curve\"\"\"\n",
    "    # Time array around the event\n",
    "    t = np.linspace(t_0 - 2*t_E, t_0 + 2*t_E, 200)\n",
    "\n",
    "    # Create binary source model\n",
    "    params = {\n",
    "        't_0': t_0,\n",
    "        'u_0': u_0,\n",
    "        't_E': t_E,\n",
    "        'q_S': q_S,      # flux ratio\n",
    "        'rho_S': rho_S,  # angular separation\n",
    "        'theta_S': theta_S  # angle\n",
    "    }\n",
    "\n",
    "    # Note: MulensModel doesn't natively support binary sources\n",
    "    # This is a simplified approximation\n",
    "    model = mm.Model({'t_0': t_0, 'u_0': u_0, 't_E': t_E},\n",
    "                     coords=coord, ephemerides_file=EPHEM_FILE)\n",
    "\n",
    "    # Calculate magnification for single source\n",
    "    magnification = model.get_magnification(t)\n",
    "\n",
    "    # Approximate binary source effect\n",
    "    # This is a simplified treatment - real binary sources are more complex\n",
    "    mag_0 = 18.0\n",
    "    mag = mag_0 - 2.5 * np.log10(magnification)\n",
    "\n",
    "    # Add some binary source features (simplified)\n",
    "    # In reality, this would depend on the source binary orientation\n",
    "    binary_modulation = 0.1 * np.sin(2 * np.pi * (t - t_0) / t_E)\n",
    "    mag += binary_modulation\n",
    "\n",
    "    mag_err = 0.01 * np.ones_like(mag)\n",
    "    mag += np.random.normal(0, mag_err)\n",
    "\n",
    "    return t, mag, mag_err, params\n",
    "\n",
    "# Create the synthetic data\n",
    "t_binary_source, mag_binary_source, mag_err_binary_source, truth_params_binary_source = create_binary_source_data()\n",
    "\n",
    "print(\"Synthetic 2S1L Event - Truth Parameters:\")\n",
    "for key, value in truth_params_binary_source.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nNote: This is a simplified binary source model.\")\n",
    "print(\"Real binary source events require more sophisticated modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s1l_data"
   },
   "outputs": [],
   "source": [
    "#@title Create 2S1L data object\n",
    "# Create data object from synthetic data\n",
    "data_list_2s1l = [t_binary_source, mag_binary_source, mag_err_binary_source]\n",
    "data_2s1l = mm.MulensData(data_list=data_list_2s1l,\n",
    "                          phot_fmt='mag',\n",
    "                          ephemerides_file=EPHEM_FILE,\n",
    "                          plot_properties={'color': '#a859e4', 'label': 'Synthetic Binary Source'},\n",
    "                          bandpass='H')\n",
    "\n",
    "# Create initial model (treating as single source for now)\n",
    "params_2s1l = {\n",
    "    't_0': truth_params_binary_source['t_0'],\n",
    "    'u_0': truth_params_binary_source['u_0'],\n",
    "    't_E': truth_params_binary_source['t_E']\n",
    "}\n",
    "model_2s1l = mm.Model(params_2s1l, coords=coord, ephemerides_file=EPHEM_FILE)\n",
    "event_2s1l = mm.Event(datasets=data_2s1l, model=model_2s1l)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_2s1l.plot_data()\n",
    "event_2s1l.plot_model(color='red', label='Single Source Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"2S1L Synthetic Event - Data with Single Source Model\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: The residuals show the binary source signature.\")\n",
    "print(\"A proper binary source model would fit these features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "parallax_example"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 1.5.5 Parallax Effects </font>\n",
    "\n",
    "Parallax effects are crucial for space-based observations like Roman. They allow us to measure the lens mass and distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parallax_setup"
   },
   "outputs": [],
   "source": [
    "#@title Parallax Model Setup\n",
    "# Parallax parameters:\n",
    "# - pi_E_N: North component of parallax\n",
    "# - pi_E_E: East component of parallax\n",
    "# - t_0_par: reference time for parallax (usually same as t_0)\n",
    "\n",
    "# Let's use the original single lens event and add parallax\n",
    "print(\"Adding parallax to 1S1L event...\")\n",
    "\n",
    "# Create model with parallax\n",
    "params_parallax = {\n",
    "    't_0': t_0_truth,\n",
    "    'u_0': u_0_truth,\n",
    "    't_E': t_E_truth,\n",
    "    'rho': rho_truth,\n",
    "    'pi_E_N': 0.1,  # North component\n",
    "    'pi_E_E': 0.05, # East component\n",
    "    't_0_par': t_0_truth  # Reference time\n",
    "}\n",
    "\n",
    "model_parallax = mm.Model(params_parallax, coords=coord, ephemerides_file=EPHEM_FILE)\n",
    "event_parallax = mm.Event(datasets=data_1s1l, model=model_parallax)\n",
    "\n",
    "print(\"Parallax Model Parameters:\")\n",
    "for key, value in params_parallax.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate parallax magnitude and direction\n",
    "pi_E_mag = np.sqrt(params_parallax['pi_E_N']**2 + params_parallax['pi_E_E']**2)\n",
    "pi_E_angle = np.arctan2(params_parallax['pi_E_N'], params_parallax['pi_E_E']) * 180 / np.pi\n",
    "print(f\"\\nParallax magnitude: {pi_E_mag:.3f}\")\n",
    "print(f\"Parallax angle: {pi_E_angle:.1f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parallax_fit"
   },
   "outputs": [],
   "source": [
    "#@title Fit parallax model\n",
    "# Set up parameters to fit including parallax\n",
    "parameters_to_fit_parallax = [\"t_0\", \"u_0\", \"t_E\", \"rho\", \"pi_E_N\", \"pi_E_E\"]\n",
    "initial_guess_parallax = [params_parallax[p] for p in parameters_to_fit_parallax]\n",
    "\n",
    "print(\"Starting parallax MCMC fit...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run MCMC\n",
    "sampler_parallax = run_mcmc_fit(event_parallax, parameters_to_fit_parallax, initial_guess_parallax,\n",
    "                               nwalkers=64, nsteps=1000, burnin=200)\n",
    "\n",
    "# Get results\n",
    "samples_parallax = sampler_parallax.chain[:, 200:, :].reshape((-1, 6))\n",
    "best_fit_parallax = np.median(samples_parallax, axis=0)\n",
    "uncertainties_parallax = np.std(samples_parallax, axis=0)\n",
    "\n",
    "fit_time = time.time() - start_time\n",
    "print(f\"\\nParallax Fit completed in {fit_time:.1f} seconds\")\n",
    "print(\"\\nResults:\")\n",
    "for i, param in enumerate(parameters_to_fit_parallax):\n",
    "    print(f\"  {param}: {best_fit_parallax[i]:.6f} \u00b1 {uncertainties_parallax[i]:.6f}\")\n",
    "\n",
    "# Calculate derived quantities\n",
    "pi_E_mag_fit = np.sqrt(best_fit_parallax[4]**2 + best_fit_parallax[5]**2)\n",
    "pi_E_mag_err = np.sqrt((best_fit_parallax[4] * uncertainties_parallax[4])**2 +\n",
    "                       (best_fit_parallax[5] * uncertainties_parallax[5])**2) / pi_E_mag_fit\n",
    "print(f\"\\nDerived quantities:\")\n",
    "print(f\"  \u03c0_E magnitude: {pi_E_mag_fit:.3f} \u00b1 {pi_E_mag_err:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parallax_results"
   },
   "outputs": [],
   "source": [
    "#@title Plot parallax results\n",
    "# Update model with best fit parameters\n",
    "for i, param_name in enumerate(parameters_to_fit_parallax):\n",
    "    setattr(event_parallax.model.parameters, param_name, best_fit_parallax[i])\n",
    "event_parallax.fit_fluxes()\n",
    "\n",
    "# Plot light curve with parallax model\n",
    "plt.figure(figsize=(12, 6))\n",
    "event_parallax.plot_data()\n",
    "event_parallax.plot_model(color='red', label='Parallax Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"1S1L Event with Parallax - Best Fit Model\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot parallax parameter correlations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# pi_E_N vs pi_E_E\n",
    "axes[0,0].scatter(samples_parallax[:, 4], samples_parallax[:, 5], alpha=0.5)\n",
    "axes[0,0].set_xlabel('\u03c0_E_N')\n",
    "axes[0,0].set_ylabel('\u03c0_E_E')\n",
    "axes[0,0].set_title('Parallax Parameter Correlation')\n",
    "\n",
    "# pi_E magnitude distribution\n",
    "pi_E_mags = np.sqrt(samples_parallax[:, 4]**2 + samples_parallax[:, 5]**2)\n",
    "axes[0,1].hist(pi_E_mags, bins=30, alpha=0.7)\n",
    "axes[0,1].axvline(pi_E_mag_fit, color='red', linestyle='--', label=f'Best fit: {pi_E_mag_fit:.3f}')\n",
    "axes[0,1].set_xlabel('\u03c0_E magnitude')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "axes[0,1].set_title('Parallax Magnitude Distribution')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# t_E vs pi_E correlation\n",
    "axes[1,0].scatter(samples_parallax[:, 2], pi_E_mags, alpha=0.5)\n",
    "axes[1,0].set_xlabel('t_E (days)')\n",
    "axes[1,0].set_ylabel('\u03c0_E magnitude')\n",
    "axes[1,0].set_title('t_E vs \u03c0_E Correlation')\n",
    "\n",
    "# u_0 vs pi_E correlation\n",
    "axes[1,1].scatter(samples_parallax[:, 1], pi_E_mags, alpha=0.5)\n",
    "axes[1,1].set_xlabel('u_0')\n",
    "axes[1,1].set_ylabel('\u03c0_E magnitude')\n",
    "axes[1,1].set_title('u_0 vs \u03c0_E Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with and without parallax\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"Without parallax:\")\n",
    "print(f\"  \u03c7\u00b2: {event_1s1l.get_chi2():.2f}\")\n",
    "print(\"With parallax:\")\n",
    "print(f\"  \u03c7\u00b2: {event_parallax.get_chi2():.2f}\")\n",
    "print(f\"  Improvement: {event_1s1l.get_chi2() - event_parallax.get_chi2():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "higher_order_summary"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 1.6 Higher-Order Effects Summary </font>\n",
    "\n",
    "We've demonstrated several key higher-order effects in microlensing:\n",
    "\n",
    "**Finite Source Effects (\u03c1):**\n",
    "- Important when the source star is resolved\n",
    "- Provides constraints on the lens mass and distance\n",
    "- Essential for accurate parameter estimation\n",
    "\n",
    "**Parallax Effects (\u03c0_E):**\n",
    "- Crucial for space-based observations\n",
    "- Enables mass and distance measurements\n",
    "- Breaks degeneracies in lens properties\n",
    "\n",
    "**Binary Lens Effects:**\n",
    "- Creates caustic structures\n",
    "- Enables planet detection\n",
    "- Requires more complex fitting procedures\n",
    "\n",
    "**Other Effects (not demonstrated):**\n",
    "- **Lens Orbital Motion:** Changes in binary lens separation over time\n",
    "- **Xallarap:** Source orbital motion effects\n",
    "- **Limb Darkening:** Non-uniform source brightness\n",
    "- **Astrometry:** Position measurements during events\n",
    "\n",
    "**Best Practices:**\n",
    "- Start with simple models and add complexity gradually\n",
    "- Use MCMC for complex parameter spaces\n",
    "- Validate results against known parameters when possible\n",
    "- Consider computational efficiency for large surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_section"
   },
   "source": [
    "## 2. pyLIMA\n",
    "\n",
    "***\n",
    "\n",
    "pyLIMA (Python Lightcurve Interpretation and Microlensing Analysis) is the first open-source software for modeling microlensing events. It was developed by Etienne Bachelet and provides a flexible, modular framework for microlensing analysis. pyLIMA is particularly well-suited for space-based observations and offers advanced features like simulation capabilities and custom parameter definitions.\n",
    "\n",
    "**Key Features:**\n",
    "- Modular design with separate event, telescope, and model classes\n",
    "- Multiple fitting algorithms (LM, DE, MCMC, TRF)\n",
    "- Built-in simulation capabilities\n",
    "- Custom parameter definitions\n",
    "- Support for multiple telescopes and filters\n",
    "- Limb darkening and finite source effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_installation"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 2.1 Installation and Setup </font>\n",
    "\n",
    "pyLIMA can be installed via pip or conda. It has several dependencies including numpy, scipy, matplotlib, and astropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_install"
   },
   "outputs": [],
   "source": [
    "#@title Install pyLIMA\n",
    "# Import pyLIMA modules\n",
    "from pyLIMA import event, telescopes\n",
    "from pyLIMA.models import PSPL_model, FSPL_model\n",
    "from pyLIMA.fits import LM_fit, DE_fit, MCMC_fit, TRF_fit\n",
    "from pyLIMA.outputs import pyLIMA_plots\n",
    "from pyLIMA.simulations import simulator\n",
    "\n",
    "print(\"pyLIMA installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_basic"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 2.2 Basic Usage </font>\n",
    "\n",
    "pyLIMA uses a modular approach with three main components:\n",
    "1. **Event**: Contains all information about the microlensing event\n",
    "2. **Telescopes**: Individual telescope data and properties\n",
    "3. **Models**: The microlensing model to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_basic_setup"
   },
   "outputs": [],
   "source": [
    "#@title Basic pyLIMA Setup\n",
    "# Create a new event\n",
    "your_event = event.Event()\n",
    "your_event.name = 'pyLIMA Example Event'\n",
    "\n",
    "# Create synthetic data for demonstration\n",
    "# In practice, you would load real data from files\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Generate synthetic light curve data\n",
    "t0, u0, tE = 2459123.5, 0.1, 25.0\n",
    "times = np.linspace(t0 - 2*tE, t0 + 2*tE, 100)\n",
    "magnification = (u0**2 + 2) / (u0 * np.sqrt(u0**2 + 4))\n",
    "u = np.sqrt(u0**2 + ((times - t0) / tE)**2)\n",
    "A = (u**2 + 2) / (u * np.sqrt(u**2 + 4))\n",
    "\n",
    "# Add noise\n",
    "mag_0 = 18.0\n",
    "mag = mag_0 - 2.5 * np.log10(A)\n",
    "mag_err = 0.01 * np.ones_like(mag)\n",
    "mag += np.random.normal(0, mag_err)\n",
    "\n",
    "# Create telescope object\n",
    "lightcurve_data = np.column_stack([times, mag, mag_err])\n",
    "telescope_1 = telescopes.Telescope(\n",
    "    name='OGLE',\n",
    "    camera_filter='I',\n",
    "    lightcurve=lightcurve_data.astype(float),\n",
    "    lightcurve_names=['time', 'mag', 'err_mag'],\n",
    "    lightcurve_units=['JD', 'mag', 'mag']\n",
    ")\n",
    "\n",
    "# Add telescope to event\n",
    "your_event.telescopes.append(telescope_1)\n",
    "\n",
    "# Set survey telescope (for alignment)\n",
    "your_event.find_survey('OGLE')\n",
    "\n",
    "# Check event setup\n",
    "your_event.check_event()\n",
    "\n",
    "print(f\"Event '{your_event.name}' created with {len(your_event.telescopes)} telescope(s)\")\n",
    "print(f\"Data points: {len(telescope_1.lightcurve)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_models"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 2.3 Model Examples </font>\n",
    "\n",
    "pyLIMA supports various microlensing models. Let's start with the basic Point Source Point Lens (PSPL) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_pspl"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 2.3.1 Single Lens (PSPL) Example </font>\n",
    "\n",
    "The PSPL model is the simplest microlensing model with parameters: `t_0`, `u_0`, `t_E`, and flux parameters for each telescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_pspl_model"
   },
   "outputs": [],
   "source": [
    "#@title PSPL Model Setup and Fitting\n",
    "# Create PSPL model\n",
    "pspl = PSPL_model.PSPLmodel(your_event)\n",
    "\n",
    "# Initialize fit with Levenberg-Marquardt algorithm\n",
    "my_fit = LM_fit.LMfit(pspl)\n",
    "\n",
    "# Show initial parameters\n",
    "print(\"Initial fit parameters:\")\n",
    "print(my_fit.fit_parameters)\n",
    "\n",
    "# Run the fit\n",
    "print(\"\\nRunning LM fit...\")\n",
    "my_fit.fit()\n",
    "my_fit.fit_outputs()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFit Results:\")\n",
    "print(my_fit.fit_results['best_model'])\n",
    "print(f\"\\nChi-squared: {my_fit.fit_results['chi2']:.2f}\")\n",
    "\n",
    "# Plot the results\n",
    "pyLIMA_plots.plot_lightcurves(pspl, my_fit.fit_results['best_model'])\n",
    "plt.title(\"pyLIMA PSPL Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_pspl_mcmc"
   },
   "outputs": [],
   "source": [
    "#@title PSPL with MCMC\n",
    "# Use MCMC for better parameter estimation\n",
    "mcmc_fit = MCMC_fit.MCMCfit(pspl, rescale_photometry=True)\n",
    "\n",
    "# Use LM results as initial guess\n",
    "mcmc_fit.model_parameters_guess = my_fit.fit_results['best_model'][:3]  # t0, u0, tE\n",
    "\n",
    "print(\"Running MCMC fit (this may take a while)...\")\n",
    "mcmc_fit.fit()\n",
    "mcmc_fit.fit_outputs()\n",
    "\n",
    "# Get MCMC results\n",
    "MCMC_results = mcmc_fit.fit_results['MCMC_chains']\n",
    "burnin = 1000\n",
    "\n",
    "# Calculate statistics\n",
    "posterior_samples = MCMC_results[burnin:, :, :3]  # t0, u0, tE\n",
    "best_fit_mcmc = np.median(posterior_samples, axis=(0, 1))\n",
    "uncertainties_mcmc = np.std(posterior_samples, axis=(0, 1))\n",
    "\n",
    "print(\"\\nMCMC Results:\")\n",
    "param_names = ['t_0', 'u_0', 't_E']\n",
    "for i, name in enumerate(param_names):\n",
    "    print(f\"{name}: {best_fit_mcmc[i]:.4f} \u00b1 {uncertainties_mcmc[i]:.4f}\")\n",
    "\n",
    "# Compare with truth values\n",
    "print(\"\\nComparison with truth:\")\n",
    "truth_values = [t0, u0, tE]\n",
    "for i, name in enumerate(param_names):\n",
    "    diff = best_fit_mcmc[i] - truth_values[i]\n",
    "    sigma_diff = abs(diff) / uncertainties_mcmc[i]\n",
    "    print(f\"{name}: {diff:.4f} ({sigma_diff:.2f}\u03c3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_fspl"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 2.3.2 Finite Source (FSPL) Example </font>\n",
    "\n",
    "The FSPL model includes finite source effects with the \u03c1 parameter, which is crucial for high-magnification events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_fspl_model"
   },
   "outputs": [],
   "source": [
    "#@title FSPL Model with Finite Source Effects\n",
    "# Create FSPL model\n",
    "fspl = FSPL_model.FSPLmodel(your_event)\n",
    "\n",
    "# Set limb darkening coefficient\n",
    "your_event.telescopes[0].ld_gamma = 0.5\n",
    "\n",
    "# Use differential evolution for better exploration\n",
    "de_fit = DE_fit.DEfit(fspl, loss_function='chi2')\n",
    "\n",
    "print(\"Running DE fit for FSPL model...\")\n",
    "de_fit.fit()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFSPL Fit Results:\")\n",
    "print(de_fit.fit_results['best_model'])\n",
    "print(f\"\\nChi-squared: {de_fit.fit_results['chi2']:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "pyLIMA_plots.plot_lightcurves(fspl, de_fit.fit_results['best_model'])\n",
    "plt.title(\"pyLIMA FSPL Fit with Finite Source Effects\")\n",
    "plt.show()\n",
    "\n",
    "# Compare PSPL vs FSPL\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"PSPL \u03c7\u00b2: {my_fit.fit_results['chi2']:.2f}\")\n",
    "print(f\"FSPL \u03c7\u00b2: {de_fit.fit_results['chi2']:.2f}\")\n",
    "print(f\"Improvement: {my_fit.fit_results['chi2'] - de_fit.fit_results['chi2']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_simulation"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 2.3.3 Simulation Capabilities </font>\n",
    "\n",
    "One of pyLIMA's unique features is its built-in simulation capabilities, allowing you to generate realistic microlensing light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_sim_setup"
   },
   "outputs": [],
   "source": [
    "#@title pyLIMA Simulation Example\n",
    "# Create a new event for simulation\n",
    "sim_event = event.Event(ra=270, dec=-30)\n",
    "sim_event.name = 'Simulated Event'\n",
    "\n",
    "# Create a realistic telescope with observing constraints\n",
    "CTIO_I = simulator.simulate_a_telescope(\n",
    "    name='CTIO_I',\n",
    "    time_start=2457365.5,\n",
    "    time_end=2457965.5,\n",
    "    sampling=4,\n",
    "    location='Earth',\n",
    "    camera_filter='I',\n",
    "    uniform_sampling=False,\n",
    "    altitude=1000,\n",
    "    longitude=-109.285399,\n",
    "    latitude=-27.130,\n",
    "    bad_weather_percentage=10.0/100,\n",
    "    moon_windows_avoidance=30,\n",
    "    minimum_alt=30,\n",
    "    astrometry=False\n",
    ")\n",
    "\n",
    "sim_event.telescopes.append(CTIO_I)\n",
    "sim_event.check_event()\n",
    "\n",
    "print(f\"Simulated telescope '{CTIO_I.name}' created\")\n",
    "print(f\"Observation period: {CTIO_I.time_start:.1f} to {CTIO_I.time_end:.1f}\")\n",
    "print(f\"Sampling: {CTIO_I.sampling} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_sim_model"
   },
   "outputs": [],
   "source": [
    "#@title Generate Simulated Light Curve\n",
    "# Create PSPL model for simulation\n",
    "sim_pspl = PSPL_model.PSPLmodel(sim_event)\n",
    "\n",
    "# Generate random model parameters\n",
    "sim_parameters = simulator.simulate_microlensing_model_parameters(sim_pspl)\n",
    "print(\"Simulated Parameters:\")\n",
    "print(sim_parameters)\n",
    "\n",
    "# Convert to pyLIMA format\n",
    "pyLIMA_parameters = sim_pspl.compute_pyLIMA_parameters(sim_parameters)\n",
    "\n",
    "# Generate the light curve\n",
    "simulator.simulate_lightcurve(sim_pspl, pyLIMA_parameters)\n",
    "\n",
    "# Plot the simulated light curve\n",
    "pyLIMA_plots.plot_lightcurves(sim_pspl, sim_parameters)\n",
    "plt.title(\"pyLIMA Simulated Light Curve\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(CTIO_I.lightcurve)} data points\")\n",
    "print(f\"Time range: {CTIO_I.lightcurve['time'].min():.1f} to {CTIO_I.lightcurve['time'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_sim_fit"
   },
   "outputs": [],
   "source": [
    "#@title Fit Simulated Data\n",
    "# Now fit the simulated data to recover parameters\n",
    "sim_fit = LM_fit.LMfit(sim_pspl)\n",
    "sim_fit.fit()\n",
    "sim_fit.fit_outputs()\n",
    "\n",
    "print(\"Fit Results vs Truth:\")\n",
    "print(\"Parameter | Truth    | Fit      | Difference\")\n",
    "print(\"---------|----------|----------|-----------\")\n",
    "\n",
    "param_names = list(sim_pspl.model_dictionnary.keys())\n",
    "for i, name in enumerate(param_names[:3]):  # t0, u0, tE\n",
    "    truth = sim_parameters[i]\n",
    "    fit = sim_fit.fit_results['best_model'][i]\n",
    "    diff = fit - truth\n",
    "    print(f\"{name:8} | {truth:8.4f} | {fit:8.4f} | {diff:+8.4f}\")\n",
    "\n",
    "print(f\"\\nChi-squared: {sim_fit.fit_results['chi2']:.2f}\")\n",
    "\n",
    "# Plot fit results\n",
    "pyLIMA_plots.plot_lightcurves(sim_pspl, sim_fit.fit_results['best_model'])\n",
    "plt.title(\"Fitting Simulated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_advanced"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 2.4 Advanced Features </font>\n",
    "\n",
    "pyLIMA offers several advanced features including custom parameter definitions and multiple fitting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_custom_params"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 2.4.1 Custom Parameter Definitions </font>\n",
    "\n",
    "pyLIMA allows you to define custom parameter transformations, which can be useful for better sampling or physical interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_custom_setup"
   },
   "outputs": [],
   "source": [
    "#@title Custom Parameter Definitions\n",
    "# Define custom parameters: log_tE instead of tE\n",
    "class MyFancyParameters(object):\n",
    "    def __init__(self, fancy_parameters={'tE': 'log_tE'},\n",
    "                 fancy_boundaries={'log_tE': (0, 3)}):\n",
    "        self.fancy_parameters = fancy_parameters\n",
    "        self.fancy_boundaries = fancy_boundaries\n",
    "\n",
    "    def tE(self, fancy_params):\n",
    "        return 10**fancy_params['log_tE']\n",
    "\n",
    "    def log_tE(self, standard_params):\n",
    "        return np.log10(standard_params['tE'])\n",
    "\n",
    "# Create model with custom parameters\n",
    "my_pars = MyFancyParameters()\n",
    "fspl_custom = FSPL_model.FSPLmodel(your_event, fancy_parameters=my_pars)\n",
    "\n",
    "print(\"Custom parameter model created\")\n",
    "print(f\"Parameters: {fspl_custom.fit_parameters.keys()}\")\n",
    "print(f\"Custom boundaries: {my_pars.fancy_boundaries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_custom_fit"
   },
   "outputs": [],
   "source": [
    "#@title Fit with Custom Parameters\n",
    "# Use Trust Region Reflective algorithm\n",
    "trf_fit = TRF_fit.TRFfit(fspl_custom)\n",
    "\n",
    "# Set initial guess (convert tE to log_tE)\n",
    "guess_parameters = [t0, u0, np.log10(tE), 0.001]  # t0, u0, log_tE, rho\n",
    "trf_fit.model_parameters_guess = guess_parameters\n",
    "\n",
    "print(\"Running TRF fit with custom parameters...\")\n",
    "trf_fit.fit()\n",
    "\n",
    "print(\"\\nFit Results (custom parameters):\")\n",
    "print(trf_fit.fit_results['best_model'])\n",
    "print(f\"\\nChi-squared: {trf_fit.fit_results['chi2']:.2f}\")\n",
    "\n",
    "# Convert back to standard parameters for comparison\n",
    "custom_params = trf_fit.fit_results['best_model']\n",
    "standard_tE = 10**custom_params[2]  # Convert log_tE back to tE\n",
    "print(f\"\\nConverted t_E: {standard_tE:.2f} days\")\n",
    "print(f\"Truth t_E: {tE:.2f} days\")\n",
    "print(f\"Difference: {standard_tE - tE:.2f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_multiple_telescopes"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 2.4.2 Multiple Telescopes and Filters </font>\n",
    "\n",
    "pyLIMA excels at handling data from multiple telescopes and filters, which is common in modern microlensing surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_multi_tel"
   },
   "outputs": [],
   "source": [
    "#@title Multiple Telescope Example\n",
    "# Create a new event with multiple telescopes\n",
    "multi_event = event.Event()\n",
    "multi_event.name = 'Multi-Telescope Event'\n",
    "\n",
    "# Generate data for multiple telescopes with different properties\n",
    "telescopes_data = []\n",
    "telescope_names = ['OGLE', 'MOA', 'LCO']\n",
    "filters = ['I', 'R', 'V']\n",
    "offsets = [0.0, 0.1, -0.05]  # Different zero points\n",
    "noise_levels = [0.01, 0.015, 0.02]  # Different noise levels\n",
    "\n",
    "for i, (name, filt, offset, noise) in enumerate(zip(telescope_names, filters, offsets, noise_levels)):\n",
    "    # Generate light curve with different properties\n",
    "    mag_tel = mag + offset + np.random.normal(0, noise, len(mag))\n",
    "    mag_err_tel = noise * np.ones_like(mag)\n",
    "\n",
    "    lightcurve_data = np.column_stack([times, mag_tel, mag_err_tel])\n",
    "\n",
    "    telescope = telescopes.Telescope(\n",
    "        name=name,\n",
    "        camera_filter=filt,\n",
    "        lightcurve=lightcurve_data.astype(float),\n",
    "        lightcurve_names=['time', 'mag', 'err_mag'],\n",
    "        lightcurve_units=['JD', 'mag', 'mag']\n",
    "    )\n",
    "\n",
    "    multi_event.telescopes.append(telescope)\n",
    "    telescopes_data.append(lightcurve_data)\n",
    "\n",
    "# Set survey telescope\n",
    "multi_event.find_survey('OGLE')\n",
    "multi_event.check_event()\n",
    "\n",
    "print(f\"Created event with {len(multi_event.telescopes)} telescopes:\")\n",
    "for tel in multi_event.telescopes:\n",
    "    print(f\"  {tel.name} ({tel.camera_filter}-band): {len(tel.lightcurve)} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pylima_multi_fit"
   },
   "outputs": [],
   "source": [
    "#@title Fit Multi-Telescope Data\n",
    "# Create PSPL model for multi-telescope data\n",
    "multi_pspl = PSPL_model.PSPLmodel(multi_event)\n",
    "\n",
    "# Fit with differential evolution\n",
    "multi_fit = DE_fit.DEfit(multi_pspl, loss_function='chi2')\n",
    "multi_fit.fit()\n",
    "\n",
    "print(\"Multi-telescope fit results:\")\n",
    "print(multi_fit.fit_results['best_model'])\n",
    "print(f\"\\nChi-squared: {multi_fit.fit_results['chi2']:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "pyLIMA_plots.plot_lightcurves(multi_pspl, multi_fit.fit_results['best_model'])\n",
    "plt.title(\"Multi-Telescope pyLIMA Fit\")\n",
    "plt.show()\n",
    "\n",
    "# Show flux parameters for each telescope\n",
    "print(\"\\nFlux parameters for each telescope:\")\n",
    "param_names = list(multi_pspl.model_dictionnary.keys())\n",
    "for i, tel in enumerate(multi_event.telescopes):\n",
    "    flux_param = multi_fit.fit_results['best_model'][3 + i]  # Flux parameters start at index 3\n",
    "    print(f\"{tel.name} ({tel.camera_filter}): {flux_param:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pylima_performance"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 2.5 Performance and Best Practices </font>\n",
    "\n",
    "pyLIMA offers several fitting algorithms, each with different strengths:\n",
    "\n",
    "**Fitting Algorithms:**\n",
    "- **LM (Levenberg-Marquardt)**: Fast, good for initial fits\n",
    "- **DE (Differential Evolution)**: Robust, handles complex parameter spaces\n",
    "- **MCMC**: Best for uncertainty estimation and posterior sampling\n",
    "- **TRF (Trust Region Reflective)**: Good for constrained optimization\n",
    "\n",
    "**Best Practices:**\n",
    "- Start with LM for quick initial fits\n",
    "- Use DE for complex models or when LM fails\n",
    "- Use MCMC for final parameter estimation and uncertainties\n",
    "- Set appropriate parameter bounds for better convergence\n",
    "- Use custom parameters for better sampling (e.g., log_tE instead of tE)\n",
    "- Always check event setup with `check_event()`\n",
    "\n",
    "**Performance Tips:**\n",
    "- Use `rescale_photometry=True` in MCMC for better numerical stability\n",
    "- Set appropriate burn-in periods for MCMC chains\n",
    "- Use simulation capabilities to test your analysis pipeline\n",
    "- Consider using custom parameters for better sampling efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_section"
   },
   "source": [
    "## 3. RTModel\n",
    "\n",
    "***\n",
    "\n",
    "RTModel (Real-Time Model) is a sophisticated, hands-off microlensing modeling package developed by Valerio Bozza. Unlike other tools that require manual model selection, RTModel automatically determines the best model type for your event through a comprehensive grid search and template library approach. It's designed to be a \"set it up and press go\" solution that provides automated model interpretation.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated Model Selection**: Determines whether an event is single-lens, binary-lens, binary-source, etc.\n",
    "- **Template Library**: Uses pre-computed binary lens templates for efficient fitting\n",
    "- **Parallel Processing**: Exploits all available CPU cores for fast analysis\n",
    "- **Comprehensive Models**: Supports 7+ model categories including parallax and orbital motion\n",
    "- **Built-in Assessment**: Provides automatic interpretation of results\n",
    "- **Visualization Tools**: Includes plotting and animation capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_installation"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.1 Installation and Setup </font>\n",
    "\n",
    "RTModel requires a C++17 compiler and uses VBMicrolensing for calculations. It can be installed via pip or from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_install"
   },
   "outputs": [],
   "source": [
    "#@title Install RTModel\n",
    "\n",
    "# Import RTModel\n",
    "import RTModel\n",
    "import RTModel.plotmodel as plm\n",
    "\n",
    "print(\"RTModel installed and imported successfully!\")\n",
    "print(f\"RTModel version: {RTModel.__version__}\" if hasattr(RTModel, '__version__') else \"RTModel imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download RTModel example event (if needed)\n",
    "# Assumes stdlib helpers + requests were imported near the top (see \"General Imports\").\n",
    "EVENT_DIR = Path(\"rtmodel_event001\")\n",
    "USE_DONE_ZIP = True  # set False to use raw event only\n",
    "\n",
    "URL_RAW = \"https://raw.githubusercontent.com/valboz/RTModel/refs/heads/main/events/event001.zip\"\n",
    "URL_DONE = \"https://raw.githubusercontent.com/valboz/RTModel/refs/heads/main/events/event001done.zip\"\n",
    "url = URL_DONE if USE_DONE_ZIP else URL_RAW\n",
    "\n",
    "if (EVENT_DIR / \"Data\").exists():\n",
    "    print(f\"{EVENT_DIR} already present; skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading RTModel example data from: {url}\")\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        tmp_path = Path(tmp)\n",
    "        zip_path = tmp_path / \"event.zip\"\n",
    "\n",
    "        with requests.get(url, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(zip_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path) as zf:\n",
    "            names = [n for n in zf.namelist() if n and not n.endswith(\"/\")]\n",
    "            top_levels = sorted({n.split(\"/\", 1)[0] for n in names if \"/\" in n})\n",
    "            zf.extractall(tmp_path)\n",
    "\n",
    "        if len(top_levels) == 1 and (tmp_path / top_levels[0]).exists():\n",
    "            src = tmp_path / top_levels[0]\n",
    "            if EVENT_DIR.exists():\n",
    "                shutil.rmtree(EVENT_DIR)\n",
    "            shutil.move(str(src), str(EVENT_DIR))\n",
    "        else:\n",
    "            EVENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            for item in list(tmp_path.iterdir()):\n",
    "                if item.name == \"event.zip\":\n",
    "                    continue\n",
    "                if item.name == EVENT_DIR.name:\n",
    "                    if EVENT_DIR.exists():\n",
    "                        shutil.rmtree(EVENT_DIR)\n",
    "                    shutil.move(str(item), str(EVENT_DIR))\n",
    "                    break\n",
    "                shutil.move(str(item), str(EVENT_DIR / item.name))\n",
    "\n",
    "    print(f\"Ready: {EVENT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_data_prep"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.2 Data Preparation </font>\n",
    "\n",
    "RTModel requires a specific directory structure and data format. Each event needs its own directory with a `/Data` subdirectory containing the photometry files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_directory_setup"
   },
   "outputs": [],
   "source": [
    "#@title Create RTModel Directory Structure\n",
    "\n",
    "# Create event directory structure for RTModel\n",
    "event_dir = 'rtmodel_event001'\n",
    "data_dir = os.path.join(event_dir, 'Data')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Created RTModel directory structure:\")\n",
    "print(f\"  {event_dir}/\")\n",
    "print(f\"  {event_dir}/Data/\")\n",
    "\n",
    "# RTModel requires this specific structure:\n",
    "# event_dir/\n",
    "# \u251c\u2500\u2500 Data/\n",
    "# \u2502   \u251c\u2500\u2500 telescope1.dat\n",
    "# \u2502   \u251c\u2500\u2500 telescope2.dat\n",
    "# \u2502   \u2514\u2500\u2500 event001.coordinates\n",
    "# \u2514\u2500\u2500 (other files created by RTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_data_format"
   },
   "outputs": [],
   "source": [
    "#@title Create RTModel Data Files\n",
    "# RTModel expects data in a specific format:\n",
    "# # Mag err HJD-2450000\n",
    "# 19.0232 0.012 8370.1223\n",
    "# 19.0150 0.011 8370.2421\n",
    "# ...\n",
    "\n",
    "# If example data was downloaded above, it should already include an ogle.dat (or similar) file.\n",
    "# Only generate a synthetic one if it's missing.\n",
    "data_file = os.path.join(data_dir, 'ogle.dat')\n",
    "if os.path.exists(data_file):\n",
    "    print(f\"Found existing data file: {data_file} (skipping generation)\")\n",
    "else:\n",
    "    # Let's create a sample data file using our synthetic microlensing data\n",
    "    required = ['times', 'mag', 'mag_err']\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            \"RTModel demo data file is missing, and required variables were not found: \"\n",
    "            + \", \".join(missing)\n",
    "            + \".\\nRun the data-loading cells earlier in the notebook, or run the download cell above (USE_DONE_ZIP=True).\"\n",
    "        )\n",
    "\n",
    "    def create_rtmodel_data_file(filename, times, mags, mag_errs):\n",
    "        \"\"\"Create RTModel-compatible data file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"# Mag err HJD-2450000\\n\")\n",
    "            for t, m, e in zip(times, mags, mag_errs):\n",
    "                # Convert to HJD-2450000 format\n",
    "                hjd_offset = t - 2450000\n",
    "                f.write(f\"{m:.4f} {e:.3f} {hjd_offset:.4f}\\n\")\n",
    "\n",
    "    create_rtmodel_data_file(data_file, times, mag, mag_err)\n",
    "\n",
    "    print(f\"Created data file: {data_file}\")\n",
    "    print(f\"Data points: {len(times)}\")\n",
    "\n",
    "    # Show first few lines\n",
    "    with open(data_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(\"\\nFirst 5 lines of data file:\")\n",
    "        for i, line in enumerate(lines[:5]):\n",
    "            print(f\"  {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_coordinates"
   },
   "outputs": [],
   "source": [
    "#@title Create Coordinates File\n",
    "# RTModel requires event coordinates in a specific format\n",
    "# Format: HH:MM:SS.S +DD:PP:SS.S\n",
    "\n",
    "# If example data was downloaded above, it may already include event001.coordinates.\n",
    "coords_file = os.path.join(data_dir, 'event001.coordinates')\n",
    "if os.path.exists(coords_file):\n",
    "    print(f\"Found existing coordinates file: {coords_file} (skipping generation)\")\n",
    "else:\n",
    "    # Convert our coordinates to the required format\n",
    "    def decimal_to_hms(ra_deg, dec_deg):\n",
    "        \"\"\"Convert decimal degrees to HH:MM:SS.S +DD:PP:SS.S format\"\"\"\n",
    "        # Convert RA (hours)\n",
    "        ra_hours = ra_deg / 15.0  # Convert degrees to hours\n",
    "        ra_h = int(ra_hours)\n",
    "        ra_m = int((ra_hours - ra_h) * 60)\n",
    "        ra_s = ((ra_hours - ra_h - ra_m/60) * 3600)\n",
    "\n",
    "        # Convert Dec (degrees)\n",
    "        dec_sign = '+' if dec_deg >= 0 else '-'\n",
    "        dec_deg_abs = abs(dec_deg)\n",
    "        dec_d = int(dec_deg_abs)\n",
    "        dec_m = int((dec_deg_abs - dec_d) * 60)\n",
    "        dec_s = ((dec_deg_abs - dec_d - dec_m/60) * 3600)\n",
    "\n",
    "        ra_str = f\"{ra_h:02d}:{ra_m:02d}:{ra_s:04.1f}\"\n",
    "        dec_str = f\"{dec_sign}{dec_d:02d}:{dec_m:02d}:{dec_s:04.1f}\"\n",
    "\n",
    "        return f\"{ra_str} {dec_str}\"\n",
    "\n",
    "    required = ['ra', 'dec']\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            \"RTModel coordinates file is missing, and required variables were not found: \"\n",
    "            + \", \".join(missing)\n",
    "            + \".\\nRun the earlier cells that define ra/dec, or run the download cell above (USE_DONE_ZIP=True).\"\n",
    "        )\n",
    "\n",
    "    coords_str = decimal_to_hms(ra, dec)\n",
    "    with open(coords_file, 'w') as f:\n",
    "        f.write(coords_str)\n",
    "\n",
    "    print(f\"Created coordinates file: {coords_file}\")\n",
    "    print(f\"Coordinates: {coords_str}\")\n",
    "    print(f\"Original coordinates: RA={ra:.6f}\u00b0, Dec={dec:.6f}\u00b0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_basic_usage"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.3 Basic Usage </font>\n",
    "\n",
    "RTModel is designed to be simple to use - just set up your data and run the analysis. The tool automatically determines the best model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_initialization"
   },
   "outputs": [],
   "source": [
    "#@title Initialize RTModel\n",
    "# Create RTModel instance\n",
    "rtm = RTModel.RTModel(event_dir)\n",
    "\n",
    "# Check number of processors (RTModel uses all available by default)\n",
    "print(f\"RTModel initialized for event: {event_dir}\")\n",
    "print(f\"Available processors: {rtm.nprocessors}\")\n",
    "\n",
    "# You can limit the number of processors if needed\n",
    "# rtm.set_processors(4)  # Use only 4 processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-VGrCG-zjfz"
   },
   "source": [
    "RTModel will automatically:\n",
    "  1. Pre-process the data\n",
    "  2. Generate initial conditions\n",
    "  3. Fit all model categories\n",
    "  4. Select best models\n",
    "  5. Provide final assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_run"
   },
   "outputs": [],
   "source": [
    "#@title Run RTModel Analysis\n",
    "# This is the main command - RTModel does everything automatically!\n",
    "print(\"Starting RTModel analysis...\")\n",
    "print(\"This may take several minutes depending on your machine.\")\n",
    "print(\"RTModel will fit 7 different model categories:\")\n",
    "print(\"  PS: Single-lens-single-source\")\n",
    "print(\"  PX: Single-lens-single-source with parallax\")\n",
    "print(\"  BS: Single-lens-binary-source\")\n",
    "print(\"  BO: Single-lens-binary-source with xallarap\")\n",
    "print(\"  LS: Binary-lens-single-source\")\n",
    "print(\"  LX: Binary-lens-single-source with parallax\")\n",
    "print(\"  LO: Binary-lens-single-source with orbital motion\")\n",
    "\n",
    "# Uncomment the line below to actually run RTModel\n",
    "# rtm.run()\n",
    "\n",
    "print(\"\\nNote: RTModel run commented out to avoid long execution.\")\n",
    "print(\"Uncomment rtm.run() to perform the actual analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_output"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.4 Understanding RTModel Output </font>\n",
    "\n",
    "After running RTModel, you'll get several output files and directories. Let's explore what each contains.\n",
    "\n",
    "**RTModel creates the following output structure:**\n",
    "\n",
    "```\n",
    "<event_dir>/\n",
    "\u251c\u2500\u2500 Data/                    # Original input data\n",
    "\u251c\u2500\u2500 ini/                     # Configuration files\n",
    "\u251c\u2500\u2500 InitCond/                # Initial conditions for fitting\n",
    "\u251c\u2500\u2500 Models/                  # Selected models for each category\n",
    "\u251c\u2500\u2500 FinalModels/             # Best models from final assessment\n",
    "\u251c\u2500\u2500 LCToFit.txt              # Pre-processed data\n",
    "\u251c\u2500\u2500 FilterToData.txt         # Dataset mapping\n",
    "\u251c\u2500\u2500 spline.txt               # Spline approximation points\n",
    "\u2514\u2500\u2500 nature.txt               # Final assessment and best models\n",
    "```\n",
    "\n",
    "**Key output files:**\n",
    "  `nature.txt`: Contains the final assessment and list of best models\n",
    "  `FinalModels/`: Contains the best model files with parameters and uncertainties\n",
    "  `Models/`: Contains selected models for each category\n",
    "\n",
    "#### Understanding the nature.txt File\n",
    "\n",
    "The `nature.txt` file contains RTModel's assessment of the event\n",
    "\n",
    "**The nature.txt file contains:**\n",
    "\n",
    "1. Best chi-square for each model category:\n",
    "  - `PS`: Single-lens-single-source\n",
    "  - `PX`: Single-lens-single-source with parallax\n",
    "  - `BS`: Single-lens-binary-source\n",
    "  - `BO`: Single-lens-binary-source with xallarap\n",
    "  - `LS`: Binary-lens-single-source\n",
    "  - `LX`: Binary-lens-single-source with parallax\n",
    "  - `LO`: Binary-lens-single-source with orbital motion\n",
    "\n",
    "2. Final assessment of the event nature\n",
    "\n",
    "3. List of proposed best models\n",
    "\n",
    "4. Model comparison and interpretation\n",
    "\n",
    "**Example `nature.txt` content:**\n",
    "\n",
    "```nature.txt\n",
    "*********************\n",
    "****   RTModel   ****\n",
    "*********************\n",
    "\n",
    "Best chi-square for each category:\n",
    "PS: 1234.56 (Single-lens-single-source)\n",
    "PX: 1230.45 (Single-lens-single-source with parallax)\n",
    "BS: 1235.67 (Single-lens-binary-source)\n",
    "BO: 1232.34 (Single-lens-binary-source with xallarap)\n",
    "LS: 1238.90 (Binary-lens-single-source)\n",
    "LX: 1235.12 (Binary-lens-single-source with parallax)\n",
    "LO: 1236.78 (Binary-lens-single-source with orbital motion)\n",
    "\n",
    "Final Assessment:\n",
    "This appears to be a single-lens event with parallax effects.\n",
    "The best model is PX with chi-square = 1230.45\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_model_categories"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.5 Model Categories </font>\n",
    "\n",
    "RTModel fits 7 different model categories by default. Each has a specific label and parameter set.\n",
    "\n",
    "#### RTModel Model Categories\n",
    "\n",
    "| Label | Model | Parameters | Description |\n",
    "|-------|-------|------------|-------------|\n",
    "| `PS` | Single-lens-single-source | 4 | Basic microlensing (`t0`, `u0`, `tE`, `rho`) |\n",
    "| `PX` | Single-lens-single-source with parallax | 6 | Basic + parallax effects |\n",
    "| `BS` | Single-lens-binary-source | 7 | Two source stars, one lens |\n",
    "| `BO` | Single-lens-binary-source with xallarap | 10 | Binary source + orbital motion |\n",
    "| `LS` | Binary-lens-single-source | 7 | Two lens components (planetary) |\n",
    "| `LX` | Binary-lens-single-source with parallax | 9 | Binary lens + parallax |\n",
    "| `LO` | Binary-lens-single-source with orbital motion | 12 | Binary lens + orbital motion |\n",
    "\n",
    "#### Additional categories available:\n",
    "- `LK`: Binary-lens-single-source with eccentric orbital motion (14 parameters)\n",
    "\n",
    "**Parameter details for each model category:**\n",
    "\n",
    "| Parameter | PS | PX | BS | BO | LS | LX | LO | Notes |\n",
    "|-----------|----|----|----|----|----|----|----|-------|\n",
    "| `u0` | \u2705* | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | Impact parameter |\n",
    "| `tE` | \u2705* | \u2705* | \u2705* | \u2705* | \u2705* | \u2705* | \u2705* | Einstein time in days |\n",
    "| `t0` | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | Closest approach time in HJD |\n",
    "| `rho` | \u2705* | \u2705* | \u2705* | \u2705* | \u2705* | \u2705* | \u2705* | Source radius |\n",
    "| `piN` | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | Parallax component along North |\n",
    "| `piE` | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | Parallax component along East |\n",
    "| `s` | \u274c | \u274c | \u274c | \u274c | \u2705* | \u2705* | \u2705* | Separation between lenses |\n",
    "| `q` | \u274c | \u274c | \u274c | \u274c | \u2705* | \u2705* | \u2705* | Mass ratio |\n",
    "| `alpha` | \u274c | \u274c | \u274c | \u274c | \u2705 | \u2705 | \u2705 | Angle of source trajectory |\n",
    "| `q_S` | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | Source flux ratio |\n",
    "| `rho_S` | \u274c | \u274c | \u2705* | \u2705* | \u274c | \u274c | \u274c | Source separation |\n",
    "| `theta_S` | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | Source angle |\n",
    "\n",
    "> key: `*` - ln scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_visualization"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.6 Visualization and Results </font>\n",
    "\n",
    "RTModel includes built-in visualization tools through the `plotmodel` subpackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_plotting"
   },
   "outputs": [],
   "source": [
    "#@title RTModel Plotting\n",
    "# RTModel includes plotting capabilities\n",
    "\n",
    "# 1. Light curve plotting:\n",
    "plm.plotmodel(eventname, modelfile) # Plot light curve with model\n",
    "plm.plotmodel(eventname, modelfile, show_caustics=True) # Show caustics\n",
    "\n",
    "# 2. Animation capabilities:\n",
    "plm.animate_model(eventname, modelfile) # Create animated GIF\n",
    "\n",
    "# 3. Astrometric plots:\n",
    "plm.plot_astrometry(eventname, modelfile) # Plot astrometric trajectory\n",
    "\n",
    "# Example usage:\n",
    "# After running RTModel and getting results\n",
    "plm.plotmodel(eventname='rtmodel_event001', modelfile='FinalModels/PS0001.txt')\n",
    "plm.plotmodel(eventname='rtmodel_event001', modelfile='FinalModels/LS0001.txt', show_caustics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUsKSsM6zjf0"
   },
   "source": [
    "#### How to analyze RTModel results:\n",
    "\n",
    "1. Check nature.txt for the final assessment\n",
    "\n",
    "   - Best chi-square for each model category\n",
    "   - RTModel's interpretation of the event\n",
    "   - List of proposed best models\n",
    "\n",
    "2. Examine FinalModels/ directory\n",
    "\n",
    "   - Each file contains parameters and uncertainties\n",
    "   - First line: parameters + fluxes + chi-square\n",
    "   - Second line: parameter uncertainties\n",
    "   - Remaining lines: covariance matrix\n",
    "\n",
    "3. Compare models using chi-square\n",
    "\n",
    "   - Lower chi-square = better fit\n",
    "   - Consider degrees of freedom (more parameters = higher expected chi-square)\n",
    "   - Use F-test or AIC/BIC for model comparison\n",
    "\n",
    "4. Validate results\n",
    "\n",
    "   - Check parameter uncertainties\n",
    "   - Examine residuals\n",
    "   - Look for systematic effects\n",
    "   - Consider physical plausibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_advanced"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.7 Advanced Features </font>\n",
    "\n",
    "RTModel offers several advanced features for customization and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gSrs-LPzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 1. Model Category Selection:\n",
    "\n",
    "# Fit only specific model categories\n",
    "rtm.set_model_categories(['PS', 'LS'])  # Only single-lens and binary-lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ-lVXpBzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 2. Fitting Options:\n",
    "\n",
    "# Configure Levenberg-Marquardt fitting\n",
    "rtm.config_LevMar(nfits=10, timelimit=1200.0, maxsteps=100)\n",
    "#@title: 3. Parameter Constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPcvIE_fzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 3. Parameter Constraints:\n",
    "\n",
    "# Set gaussian constraints on parameters\n",
    "rtm.set_constraints('tE', 25.0, 2.0)  # tE = 25 \u00b1 2 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiklV_Dgzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 4. Parallel Processing:\n",
    "\n",
    "# Control number of processors\n",
    "rtm.set_processors(8)  # Use 8 processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxJrzHdHzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 5. Step-by-step Execution:\n",
    "\n",
    "# Run individual steps manually\n",
    "rtm.Reader()           # Data pre-processing\n",
    "rtm.InitCond()        # Generate initial conditions\n",
    "rtm.launch_fits('PS') # Fit specific category\n",
    "rtm.ModelSelection()  # Select best models\n",
    "rtm.FinalAssessment() # Final assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_best_practices"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.8 RTModel Best Practices </font>\n",
    "\n",
    "**1. Data Quality:**\n",
    "- Ensure data is properly formatted\n",
    "- Check for systematic errors\n",
    "- Verify coordinate accuracy\n",
    "\n",
    "**2. Computational Resources:**\n",
    "- Use all available processors for speed\n",
    "- Allow sufficient time (1-3 hours typical)\n",
    "- Ensure adequate disk space\n",
    "\n",
    "**3. Model Interpretation:**\n",
    "- Don't rely solely on chi-square\n",
    "- Consider physical plausibility\n",
    "- Check parameter uncertainties\n",
    "- Validate against known physics\n",
    "\n",
    "**4. Troubleshooting:**\n",
    "- Check ini/ directory for configuration\n",
    "- Examine error messages in output\n",
    "- Verify data format compliance\n",
    "- Use step-by-step execution for debugging\n",
    "\n",
    "**5. Results Validation:**\n",
    "- Compare with other fitting codes\n",
    "- Check for systematic residuals\n",
    "- Verify parameter correlations\n",
    "- Consider alternative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_comparison"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.9 RTModel vs Other Tools </font>\n",
    "\n",
    "RTModel offers a unique approach compared to other microlensing fitting tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_comparison_table"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> Comparison with Other Tools </font>\n",
    "\n",
    "**RTModel vs Other Microlensing Tools:**\n",
    "\n",
    "| Feature | RTModel | MulensModel | pyLIMA |\n",
    "|---------|---------|-------------|--------|\n",
    "| Model Selection | Automatic | Manual | Manual |\n",
    "| Ease of Use | Moderate set up <br> Very easy to run | Moderate | Moderate |\n",
    "| Speed | Fast (parallel) | Variable | Variable |\n",
    "| Automation | High | Low | Low |\n",
    "| Customization | Moderate | High | High |\n",
    "| Visualization | Built-in | Basic | Good |\n",
    "| Parallel Processing | Yes | Manual | Manual |\n",
    "\n",
    "**RTModel Advantages:**\n",
    "- Hands-off operation - just set up data and run\n",
    "- Automatic model selection and interpretation\n",
    "- Template library for efficient binary lens fitting\n",
    "- Built-in assessment and visualization\n",
    "- Parallel processing for speed\n",
    "\n",
    "**RTModel Limitations:**\n",
    "- Less customization than manual tools\n",
    "- Requires specific data format\n",
    "- Limited to supported model categories\n",
    "- May miss subtle effects in complex events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_astrometric"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 3.10 Astrophotometric Fitting (RTModel v3.0) </font>\n",
    "\n",
    "**NEW IN RTModel v3.0!** RTModel now supports combined photometric and astrometric fitting, making it the first tool to offer comprehensive astrophotometric microlensing analysis. This capability is crucial for space-based observations like Roman, where milliarcsecond astrometric precision enables measurement of the centroid trajectory during microlensing events.\n",
    "\n",
    "**Key Astrometric Features:**\n",
    "- **Combined Fitting**: Simultaneous photometric and astrometric parameter estimation\n",
    "- **Centroid Trajectory**: Models the astrometric shift of images during events\n",
    "- **Proper Motion**: Measures source and lens proper motions\n",
    "- **Einstein Angle**: Direct measurement of \u03b8_E from astrometry\n",
    "- **Source Parallax**: Geometric parallax of the source star\n",
    "- **Visualization**: Built-in astrometric plotting and trajectory visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_astrometric_data"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 3.10.1 Astrophotometric Data Format </font>\n",
    "\n",
    "Astrophotometric datasets include both photometric and astrometric information in a single file.\n",
    "\n",
    "Astrophotometric datasets have 7 columns: `Mag`, `err`, `HJD-2450000`, `Dec`, `errDec`, `RA`, `errRA`\n",
    "\n",
    "**Example astrophotometric data:**\n",
    "\n",
    "```\n",
    "# Mag err HJD-2450000 Dec errDec RA errRA\n",
    "19.0232 0.012 8370.1223 1.2534 1.0 0.0165 1.0\n",
    "19.0150 0.011 8370.2421 1.7510 1.1 -0.5422 1.1\n",
    "19.0034 0.011 8370.3697 1.1190 1.1 -0.1981 1.1\n",
    "18.9712 0.010 8370.4911 1.4281 1.0 0.2119 1.0\n",
    "18.9592 0.011 8370.6114 1.3005 0.9 0.3982 1.0\n",
    "18.9109 0.009 8370.8234 1.6233 1.0 0.5121 1.0\n",
    "18.8798 0.009 8371.0092 2.0223 1.2 0.9411 1.1\n",
    "```\n",
    "\n",
    "**Column descriptions:**\n",
    "\n",
    "  - `Mag`: Magnitude\n",
    "  - `err`: Photometric error\n",
    "  - `HJD-2450000`: Heliocentric Julian Date - 2450000\n",
    "  - `Dec`: Declination offset in milliarcseconds\n",
    "  - `errDec`: Declination error in milliarcseconds\n",
    "  - `RA`: Right Ascension offset in milliarcseconds\n",
    "  - `errRA`: Right Ascension error in milliarcseconds\n",
    "\n",
    "> Note: Dec and RA are angular displacements from a fixed reference point\n",
    "in the North and East directions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtmodel_astrometric_create"
   },
   "outputs": [],
   "source": [
    "#@title Create Astrophotometric Data\n",
    "# Create sample astrophotometric data\n",
    "def create_astrophotometric_data(filename, times, mags, mag_errs):\n",
    "    \"\"\"Create RTModel-compatible astrophotometric data file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"# Mag err HJD-2450000 Dec errDec RA errRA\\n\")\n",
    "        for t, m, e in zip(times, mags, mag_errs):\n",
    "            # Convert to HJD-2450000 format\n",
    "            hjd_offset = t - 2450000\n",
    "\n",
    "            # Generate synthetic astrometric data\n",
    "            # Simulate centroid motion during microlensing event\n",
    "            t0 = 2459123.5  # Peak time\n",
    "            tE = 25.0       # Einstein time\n",
    "\n",
    "            # Simple astrometric model (centroid shift)\n",
    "            u = abs(t - t0) / tE\n",
    "            if u < 0.1:  # Near peak\n",
    "                dec_offset = 2.0 * np.exp(-u**2)  # Peak shift\n",
    "                ra_offset = 1.5 * np.exp(-u**2)   # Peak shift\n",
    "            else:\n",
    "                dec_offset = 0.5 * np.exp(-u**2)  # Background\n",
    "                ra_offset = 0.3 * np.exp(-u**2)   # Background\n",
    "\n",
    "            # Add noise\n",
    "            dec_offset += np.random.normal(0, 0.1)\n",
    "            ra_offset += np.random.normal(0, 0.1)\n",
    "\n",
    "            # Astrometric errors (typically 0.5-1.0 mas)\n",
    "            dec_err = 0.8 + 0.2 * np.random.random()\n",
    "            ra_err = 0.8 + 0.2 * np.random.random()\n",
    "\n",
    "            f.write(f\"{m:.4f} {e:.3f} {hjd_offset:.4f} {dec_offset:.4f} {dec_err:.1f} {ra_offset:.4f} {ra_err:.1f}\\n\")\n",
    "\n",
    "# Create astrophotometric data file\n",
    "astro_data_file = os.path.join(data_dir, 'roman_astrometric.dat')\n",
    "create_astrophotometric_data(astro_data_file, times, mag, mag_err)\n",
    "\n",
    "print(f\"Created astrophotometric data file: {astro_data_file}\")\n",
    "print(f\"Data points: {len(times)}\")\n",
    "\n",
    "# Show first few lines\n",
    "with open(astro_data_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"\\nFirst 5 lines of astrophotometric data:\")\n",
    "    for i, line in enumerate(lines[:5]):\n",
    "        print(f\"  {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_astrometric_parameters"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 3.10.2 Astrophotometric Parameters </font>\n",
    "\n",
    "When RTModel detects astrophotometric data, it automatically adds 4 additional parameters to the fit:\n",
    "\n",
    "| Parameter | Meaning | Physical Significance |\n",
    "|-----------|---------|---------------------|\n",
    "| `muS_Dec` | Source proper motion component in North direction (mas/yr) | Source motion perpendicular to line of sight |\n",
    "| `muS_RA` | Source proper motion component in East direction (mas/yr) | Source motion perpendicular to line of sight |\n",
    "| `piS` | Geometric parallax of the source (mas) | Distance to source star |\n",
    "| `thetaE` | Einstein angle (mas) | Lens mass and distance (\u03b8_E = \u221a(4GM/c\u00b2D_L)) |\n",
    "\n",
    "**Model Categories for Astrophotometric Fits:**\n",
    "* `PX`: Single-lens-single-source with parallax (10 parameters)\n",
    "* `BO`: Single-lens-binary-source with xallarap (14 parameters)\n",
    "* `LX`: Binary-lens-single-source with parallax (13 parameters)\n",
    "* `LO`: Binary-lens-single-source with orbital motion (16 parameters)\n",
    "* `LK`: Binary-lens-single-source with eccentric orbital motion (18 parameters)\n",
    "\n",
    "> Note: No static models are fitted in astrophotometric mode because parallax is always included to model the centroid trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_astrometric_fitting"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 3.10.3 Astrophotometric Fitting </font>\n",
    "\n",
    "RTModel automatically detects astrophotometric datasets and switches to combined fitting mode.\n",
    "\n",
    "**RTModel Astrophotometric Fitting:**\n",
    "\n",
    "1. Automatic Detection:\n",
    "\n",
    "  RTModel automatically detects astrophotometric datasets by checking for 7-column format (vs 3-column for photometric)\n",
    "\n",
    "2. Combined Chi-Square:\n",
    "\n",
    "  * $\\chi^2_{total} = \\chi^2_{photometric} + \\chi^2_{astrometric}$\n",
    "  * Both contributions are weighted appropriately\n",
    "\n",
    "3. Enhanced Parameter Space:\n",
    "\n",
    "   - 4 additional astrometric parameters\n",
    "   - Parallax always included (no static models)\n",
    "   - More complex model selection\n",
    "\n",
    "4. Example Usage:\n",
    "\n",
    "  ```python\n",
    "  # RTModel automatically detects astrophotometric data\n",
    "  rtm = RTModel.RTModel('astro_event')\n",
    "  rtm.run()  # Automatically uses astrophotometric fitting\n",
    "  ```\n",
    "\n",
    "5. Output Structure:\n",
    "\n",
    "  * Model files contain: `nps + 4*ntel + 1` parameters\n",
    "   - `nps`: model parameters + 4 astrometric parameters\n",
    "   - `4*ntel`: fluxes and centroid positions for each telescope\n",
    "   - `+1`: total chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_astrometric_visualization"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 3.10.4 Astrometric Visualization </font>\n",
    "\n",
    "RTModel includes specialized plotting functions for astrometric data visualization.\n",
    "\n",
    "**RTModel Astrometric Plotting Functions:**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbOKvS0Qzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 1. Combined Light Curve and Astrometry\n",
    "myplot = plm.plotmodel(eventname, modelfile)\n",
    "# Shows both photometric light curve and astrometric parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKpxmm6Jzjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 2. Centroid Trajectory in the Sky:\")\n",
    "myplot.showastrometry()\n",
    "# Shows the centroid trajectory as a function of time\n",
    "# Displays RA vs Dec with error ellipses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc3OGZL3zjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 3. Individual Astrometric Components:\")\n",
    "myplot.showastrometryRA()  # RA vs time\n",
    "myplot.showastrometryDec() # Dec vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC21jJ1szjf0"
   },
   "outputs": [],
   "source": [
    "#@title: 4. Multiple Telescope Support:\")\n",
    "myplot.showastrometry(1)  # Show astrometry for telescope 1\n",
    "# Different telescopes may have different blending fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MezgDm8tzjf1"
   },
   "source": [
    "5. Key Features:\n",
    "\n",
    "   - Automatic error ellipse plotting\n",
    "   - Time evolution of centroid position\n",
    "   - Comparison with observed astrometric data\n",
    "   - Proper motion vector visualization\n",
    "\n",
    "6. Example Output:\n",
    "   - Centroid trajectory shows the astrometric microlensing signal\n",
    "   - Peak season data has highest precision\n",
    "   - Post-peak data constrains proper motion and parallax\n",
    "   - Error bars reflect astrometric precision (typically 0.5-1.0 mas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOvUtqTpzjf1"
   },
   "source": [
    "**Physical Interpretation of Astrophotometric Parameters:**\n",
    "\n",
    "1. Einstein Angle ($\\theta_E$)\n",
    "> $\\theta_E = \\sqrt{(4GM/c^2D_L)}$\n",
    "- Direct measurement of lens mass and distance\n",
    "- Breaks the mass-distance degeneracy\n",
    "- Typical values: 0.1-1.0 mas for stellar lenses\n",
    "\n",
    "2. Source Proper Motion ($\\mu_S$)\n",
    "> $\\mu_S = v_S / D_S$\n",
    "- Tangential velocity of source star\n",
    "- Helps constrain source distance and kinematics\n",
    "- Typical values: 1-10 mas/yr for Galactic sources\n",
    "\n",
    "3. Source Parallax ($\\pi_S$)\n",
    "$\\pi_S = 1/D_S$\n",
    "- Geometric parallax of source star\n",
    "- Provides independent distance measurement\n",
    "- Typical values: $0.1-1.0 \\,\\text{mas}$ for Galactic sources\n",
    "\n",
    "4. Centroid Trajectory\n",
    "- Shows the motion of the light centroid during the event\n",
    "- Amplitude depends on lens mass and source-lens separation\n",
    "- Shape reveals lens geometry (single vs binary)\n",
    "- Duration related to Einstein time\n",
    "\n",
    "5. Advantages of Astrophotometric Fitting\n",
    "- Breaks degeneracies in lens mass and distance\n",
    "- Provides independent constraints on source properties\n",
    "- Enables direct measurement of Einstein angle\n",
    "- Improves parameter precision and accuracy\n",
    "- Essential for space-based microlensing surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtmodel_astrometric_applications"
   },
   "source": [
    "#### <font face=\"Helvetica\" size=\"4\"> 3.10.5 Applications and Use Cases </font>\n",
    "\n",
    "Astrophotometric fitting is particularly valuable for specific types of observations and scientific goals.\n",
    "\n",
    "**Key Applications of Astrophotometric Microlensing:**\n",
    "\n",
    "1. Space-Based Surveys (Roman, Euclid)\n",
    "- Milliarcsecond astrometric precision\n",
    "- Wide-field astrometric capabilities\n",
    "- Long-term monitoring of events\n",
    "- Essential for mass and distance measurements\n",
    "\n",
    "2. Adaptive Optics Observations\n",
    "- Ground-based high-resolution imaging\n",
    "- Resolved source and lens components\n",
    "- Precise centroid measurements\n",
    "- Complementary to photometric data\n",
    "\n",
    "3. Planetary Microlensing\n",
    "- Direct measurement of planet masses\n",
    "- Breaking mass-distance degeneracies\n",
    "- Improved orbital parameter constraints\n",
    "- Better characterization of planetary systems\n",
    "\n",
    "4. Free-Floating Planets\n",
    "- Mass measurement without host star\n",
    "- Distance determination\n",
    "- Population statistics\n",
    "- Formation mechanism constraints\n",
    "\n",
    "5. Binary Lens Systems\n",
    "- Improved caustic crossing predictions\n",
    "- Better orbital motion constraints\n",
    "- Enhanced binary parameter determination\n",
    "- More accurate mass ratio measurements\n",
    "\n",
    "6. Galactic Structure Studies\n",
    "- Source and lens distance measurements\n",
    "- Proper motion distributions\n",
    "- Kinematic information\n",
    "- Population synthesis constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc7MwRc5zjf1"
   },
   "source": [
    "**Best Practices for Astrophotometric Fitting:**\n",
    "\n",
    "1. Data Quality Requirements\n",
    "- Astrometric precision: $< 1 \\,\\text{mas}$ (preferably $< 0.5 \\,\\text{mas}$)\n",
    "- Temporal coverage: Peak + post-peak monitoring\n",
    "- Reference frame stability\n",
    "- Proper error estimation\n",
    "\n",
    "2. Observational Strategy\n",
    "- High-cadence observations during peak\n",
    "- Long-term follow-up for proper motion\n",
    "- Multiple epochs for parallax measurement\n",
    "- Consistent reference frame\n",
    "\n",
    "3. Model Selection\n",
    "- Start with simple models (`PX`)\n",
    "- Add complexity as needed (`LX`, `LO`)\n",
    "- Consider eccentric orbital motion (`LK`)\n",
    "- Validate against physical constraints\n",
    "\n",
    "### 4. Parameter Validation\n",
    "- Check Einstein angle physical plausibility\n",
    "- Verify proper motion consistency\n",
    "- Validate parallax measurements\n",
    "- Compare with independent constraints\n",
    "\n",
    "### 5. Interpretation Challenges\n",
    "- Blending effects on centroid\n",
    "- Reference frame systematics\n",
    "- Proper motion vs parallax degeneracy\n",
    "- Binary lens complexity\n",
    "\n",
    "### 6. Future Prospects\n",
    "- Roman Space Telescope: $\\sim 0.1 \\,\\text{mas}$ precision\n",
    "- Euclid: Wide-field astrometry\n",
    "- Ground-based AO: High-resolution follow-up\n",
    "- Multi-wavelength astrometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxRwY_WMzjf1"
   },
   "source": [
    "## 4. popclass: Probabilistic Classification of Microlensing Lenses\n",
    "\n",
    "### What is popclass?\n",
    "\n",
    "**popclass** is a Python package developed by LLNL (lead: Peter McGill) for *probabilistic classification* of the lens in a gravitational microlensing event. It bridges the gap between event posteriors (from light curve modeling) and population synthesis simulations of the Milky Way, allowing you to infer the probability that a given event was caused by a star, white dwarf, neutron star, or black hole.\n",
    "\n",
    "- **Key use:** Given posterior samples (e.g., from MulensModel, pyLIMA, or any Bayesian fit) and a Galactic population model, popclass computes the probability that the lens belongs to each class.\n",
    "- **Why use it?** It enables population-level inference, e.g., identifying likely black hole events, and is designed for the era of large surveys (Roman, Rubin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi7iJf5Dzjf1"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 4.1 Installation </font>\n",
    "\n",
    "```bash\n",
    "pip install popclass\n",
    "```\n",
    "or via conda:\n",
    "```bash\n",
    "conda install -c conda-forge popclass\n",
    "```\n",
    "\n",
    "### <font face=\"Helvetica\" size=\"5\"> 4.2 Basic Usage Example </font>\n",
    "\n",
    "Suppose you have posterior samples for an event in log10(tE) and log10(piE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ledRhbJczjf1"
   },
   "outputs": [],
   "source": [
    "from popclass.posterior import Posterior\n",
    "from popclass.model import PopulationModel\n",
    "from popclass.classify import classify\n",
    "\n",
    "# Mock posterior samples\n",
    "NUM_POSTERIOR_SAMPLES = 10000\n",
    "logtE = np.random.normal(loc=2, scale=0.1, size=NUM_POSTERIOR_SAMPLES)\n",
    "logpiE = np.random.normal(loc=-1, scale=0.5, size=NUM_POSTERIOR_SAMPLES)\n",
    "posterior_samples = np.vstack((logtE, logpiE)).T\n",
    "prior_density = 0.028 * np.ones(NUM_POSTERIOR_SAMPLES)  # uniform prior\n",
    "\n",
    "# Wrap in popclass objects\n",
    "posterior = Posterior(samples=posterior_samples, parameter_labels=['log10tE', 'log10piE'])\n",
    "inference_data = posterior.to_inference_data(prior_density)\n",
    "\n",
    "# Load a pre-built population model (e.g., PopSyCLE)\n",
    "popsycle = PopulationModel.from_library('popsycle_singles_sukhboldn20')\n",
    "\n",
    "# Classify!\n",
    "classification = classify(population_model=popsycle, inference_data=inference_data, parameters=['log10tE', 'log10piE'])\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UOACuOuzjf1"
   },
   "source": [
    "**Output:**\n",
    "`{'black_hole': 0.09, 'neutron_star': 0.001, 'star': 0.71, 'white_dwarf': 0.20}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os268jhrzjf1"
   },
   "source": [
    "### <font face=\"Helvetica\" size=\"5\"> 4.3 How It Works </font>\n",
    "\n",
    "- **Inputs:** Posterior samples for event parameters (e.g., `tE`, `piE`), their prior densities, and a Galactic population model (e.g., from PopSyCLE).\n",
    "- **Method:** Uses Bayesian importance sampling to compare the event's posterior to simulated populations, computing the probability for each lens class.\n",
    "- **Output:** A dictionary of class probabilities (star, white dwarf, neutron star, black hole).\n",
    "\n",
    "**Mathematical summary:**\n",
    "\\[\n",
    "p(\\text{class}_L| \\boldsymbol{d}, \\mathcal{G}) = \\frac{p(\\text{class}_L| \\mathcal{G})}{p(\\boldsymbol{d}| \\mathcal{G})}\n",
    "    \\times \\frac{1}{S} \\sum _{c=0}^{S} \\frac{p(\\theta _c | \\text{class}_L, \\mathcal{G})}{\\pi(\\theta _{c})}\n",
    "\\]\n",
    "where:\n",
    "- \\( \\boldsymbol{d} \\): event data\n",
    "- \\( \\mathcal{G} \\): Galactic model\n",
    "- \\( \\theta_c \\): posterior samples\n",
    "- \\( \\pi(\\theta_c) \\): prior density\n",
    "\n",
    "### <font face=\"Helvetica\" size=\"5\"> 4.4 Population Models </font>\n",
    "\n",
    "popclass comes with several pre-built population models (from PopSyCLE), e.g.:\n",
    "- `popsycle_singles_sukhboldn20`\n",
    "- `popsycle_singles_raithel18`\n",
    "- `popsycle_singles_spera15`\n",
    "\n",
    "You can also load your own models in ASDF format.\n",
    "\n",
    "### <font face=\"Helvetica\" size=\"5\"> 4.5 Advanced Features </font>\n",
    "\n",
    "- **Flexible parameter spaces:** Use any set of event parameters (e.g., `tE`, `piE`, `thetaE`, `blend_fraction`).\n",
    "- **Custom population models:** Supply your own simulation data in the required format.\n",
    "- **Uncertainty quantification:** Includes a \"None\" class to flag events not well explained by the model.\n",
    "- **ArviZ and PyMultiNest integration:** For handling posteriors from common Bayesian tools.\n",
    "- **Plotting:** Built-in tools for visualizing classification results.\n",
    "\n",
    "### <font face=\"Helvetica\" size=\"5\"> 4.6 Best Practices & Caveats </font>\n",
    "\n",
    "- **Parameter consistency:** Ensure your posterior and population model use the same parameterization (e.g., both in `log10(tE)`, not `tE`).\n",
    "- **Prior density:** Must match the parameter space of the population model (apply Jacobian if transforming variables).\n",
    "- **Model completeness:** If your event is outside the simulated parameter space, the \"None\" class will help flag this.\n",
    "- **Interpretation:** Probabilities are only as good as the population model and the event posterior.\n",
    "\n",
    "### <font face=\"Helvetica\" size=\"5\"> 4.7 Further Reading </font>\n",
    "\n",
    "- [popclass documentation](https://popclass.readthedocs.io)\n",
    "- [PopSyCLE population models](https://github.com/jluastro/PopSyCLE)\n",
    "- Perkins et al. (2024), Kaczmarek et al. (2024) \u2014 theoretical background\n",
    "\n",
    "> **popclass** is a powerful tool for population-level inference in microlensing, enabling robust, probabilistic classification of lens types for large event samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Author(s):** Amber Malpas, Ali Crisp\n",
    "**Maintainers:** RGES-PIT Working Group 9  \n",
    "**Last Updated:** 20 Nov 2025  \n",
    "**Contact:** malpas.1@osu.edu\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `MulensModel`, `pyLIMA`, `RTModel`, `popClass`, or this notebook for published research, please cite the\n",
    "authors. Follow these links for more information about citing:\n",
    "\n",
    "* [Citing `MulensModel`](https://github.com/rpoleski/MulensModel/blob/master/CITATION.cff)\n",
    "* [Citing `pyLIMA`](https://github.com/ebachelet/pyLIMA?tab=readme-ov-file#citations)\n",
    "* [Citing `RTModel`](https://github.com/valboz/RTModel#attribution)\n",
    "* [Citing `popClass`](https://github.com/LLNL/popclass/blob/main/CITATION.cff)\n",
    "* [Citing **Roman Microlensing Data Challenge 2026 Notebooks**](https://github.com/rges-pit/data-challenge-notebooks/blob/main/zenodo.txt)\n",
    "\n",
    "[Top of Page](#top)\n",
    "<!-- Footer Start -->\n",
    "\n",
    "<!-- Footer End -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B5OeZIbHv2eB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rges-pit-dc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  },
  "rges_sync": {
   "source_id": "extras-microlensing-tools",
   "session": "Extras",
   "audiences": [
    "Data challenge teams",
    "Self-paced learners",
    "Roman Research Nexus participants"
   ],
   "website_render": "full_embed",
   "nexus_support": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
