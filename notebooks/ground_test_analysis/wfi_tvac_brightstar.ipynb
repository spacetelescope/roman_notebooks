{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e67e2dc-9d69-41dd-8b1c-c265856f6da1",
   "metadata": {},
   "source": [
    "# Diving Into WFI TVAC Bright Star Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc83a0-9fab-4e6d-b416-7a2b84274c75",
   "metadata": {},
   "source": [
    "## Server Information\n",
    "IMPORTANT: To run this tutorial, please make sure you are logged in the RRN with a medium or large server, as it requires >26 GB of memory to stream and process the data files used here.\n",
    "\n",
    "## Kernel Information and Read-Only Status\n",
    "\n",
    "To run this notebook, please select the \"Roman Research Nexus\" kernel at the top right of your window.\n",
    "\n",
    "This notebook is read-only. You can run cells and make edits, but you must save changes to a different location. We recommend saving the notebook within your home directory, or to a new folder within your home (e.g. <span style=\"font-variant:small-caps;\">file > save notebook as > my-nbs/nb.ipynb</span>). Note that a directory must exist before you attempt to add a notebook to it.\n",
    "\n",
    "## Imports\n",
    " Libraries used\n",
    "- *romancal* for running the processing pipeline\n",
    "- *roman_datamodels* for opening Roman WFI ASDF files\n",
    "- *asdf* for opening Roman WFI ASDF files\n",
    "- *os* for checking if files exist\n",
    "- *copy* for making copies of Python objects\n",
    "- *s3fs* for streaming files from an S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44ccc5-cae2-4391-be9b-fcb998f62973",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In Fall 2023 and Spring 2024, the Wide Field Instrument (WFI) underwent thermal vacuum (TVAC) at BAE Systems in Boulder, Colorado to collect performance and trending data in a flight-like environment. To learn more about TVAC, please visit the [RDox pages on the WFI Ground Testing Campaigns](https://roman-docs.stsci.edu/roman-instruments-home/wfi-imaging-mode-user-guide/wfi-detectors/detector-performance/wfi-ground-testing-campaigns#WFIGroundTestingCampaigns-WFI_Testing).\n",
    "\n",
    "As part of the TVAC campaign, the WFI Bright Star Saturation test was conducted to characterize how WFI Sensor Chip Assemblies (SCAs) respond to saturation from a wide range of bright, in-focus point sources, similar to those expected during Roman's science surveys. A telescope simulator was used to project nine simulated point sources with magnitudes ranging from ~4 to ~18 mag through the F146 filter, onto a grid of locations on SCAs 4 and 11. Note that due to the characteristics of the telescope simulator and ground test setup, the acquired data do not reproduce the optics of the observatory.\n",
    "\n",
    "In this notebook we explore a subset of the point source data. We apply a non-complete set of calibrations with the Roman WFI science calibration pipeline RomanCal (Python package name `romancal`) and visualize the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1b41b-f30e-40d9-9c90-30dbaf07bc51",
   "metadata": {},
   "source": [
    "## Tutorial Data\n",
    "In this tutorial, we use L1 WFI test data files stored in the Nexus S3 bucket. See the [Data Discovery and Access](../data_discovery_and_access/data_discovery_and_access.ipynb) and [Exposure Pipeline](../exposure_pipeline/exposure_pipeline.ipynb) tutorials for more information on how to pull data and run Romancal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data & Environment Setup ---\n",
    "# This cell configures reference data and environment variables.\n",
    "# On the Roman Research Nexus (RNN), everything is pre-configured and\n",
    "# this cell completes instantly.  For local or CI execution it will\n",
    "# download any missing reference data automatically.\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_DATA = []\n",
    "\n",
    "# Load the setup module ---------------------------------------------------\n",
    "try:\n",
    "    import notebook_data_dependencies as ndd\n",
    "except ImportError:\n",
    "    # Walk up from the notebook directory to find shared/notebook_data_dependencies.py\n",
    "    _here = Path(os.getcwd())\n",
    "    for _parent in [_here] + list(_here.parents):\n",
    "        _candidate = _parent / 'shared' / 'notebook_data_dependencies.py'\n",
    "        if _candidate.exists():\n",
    "            import importlib.util\n",
    "            _spec = importlib.util.spec_from_file_location('notebook_data_dependencies', _candidate)\n",
    "            ndd = importlib.util.module_from_spec(_spec)\n",
    "            _spec.loader.exec_module(ndd)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            'Cannot find shared/notebook_data_dependencies.py in any parent directory.\\n'\n",
    "            'Make sure you are running from within the roman_notebooks repo.'\n",
    "        )\n",
    "\n",
    "# Download missing reference data (no-op when env vars are already set) ---\n",
    "result = ndd.install_files(packages=REQUIRED_DATA) if REQUIRED_DATA else {}\n",
    "ndd.setup_env(result)  # Sets data paths + CRDS env vars\n"
   ],
   "id": "384dc2f7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea65843-e0e1-4887-b89e-1a6bab5c0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "import math\n",
    "import asdf\n",
    "import os\n",
    "import copy\n",
    "import roman_datamodels as rdm\n",
    "from roman_datamodels.dqflags import pixel as dqflags\n",
    "from romancal.saturation import SaturationStep\n",
    "from roman_datamodels.datamodels import SaturationRefModel\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import romancal\n",
    "from romancal.pipeline import ExposurePipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e2734a1-48cd-40e9-ba4d-f52e54873976",
   "metadata": {},
   "source": [
    "## Pull the data\n",
    "We will ingest data corresponding to:\n",
    "* a magnitude 4 (approximately) point source imaged through F146 on SCA 11\n",
    "* three subsequent darks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9d816f-d1b4-4837-8d2d-058fda317869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to open a given asdf file\n",
    "def open_asdf_datamodel(asdf_uri: str, anon: bool = True):\n",
    "    \"\"\"\n",
    "    Open an ASDF file from S3 or locally and return a Roman datamodel\n",
    "    \"\"\"\n",
    "    if asdf_uri.startswith(\"s3://\"):\n",
    "        fs = s3fs.S3FileSystem(anon=anon)\n",
    "        with fs.open(asdf_uri, \"rb\") as fb:\n",
    "            with asdf.open(fb) as af:\n",
    "                return rdm.open(af).copy()\n",
    "    else:\n",
    "        with open(asdf_uri, \"rb\") as fb:\n",
    "            with asdf.open(fb) as af:\n",
    "                return rdm.open(af).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebeb869d",
   "metadata": {},
   "source": [
    "Below, we point to a particular \"activity\" which corresponds to a given type of data collection. Typically, an activity refers to a set of exposures with the same attributes like commanded flux, filter, location and is constrained by the test planning software used for these ground tests. Please refer to this [table](activity_table.md) to see the mapping between activities in the test and type of collected test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887f2ac8-a6bc-40f4-b686-616267d0b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is hard-coded\n",
    "# Another option is to run a search for the right activity/mag/SCA.\n",
    "Mag = 4\n",
    "SCA = 11\n",
    "WFI_TAG = f'WFI{SCA:02d}'\n",
    "asdf_dir_uri = 's3://stpubdata/roman/nexus/tvac/'\n",
    "base = asdf_dir_uri + 'OTP00651_BrightStar_TV2b_R1_MCEB/'\n",
    "\n",
    "# Illuminated image\n",
    "# To find the full filename(s), you can perform a list command (`ls`) on the directory path that includes the activity number (e.g., `Activity_22`)\n",
    "asdf_file_uri_illum = base + f'Activity_22/TVAC2_TOHOTQUAL_WFISAT_20240506204523_{WFI_TAG}_uncal.asdf'\n",
    "\n",
    "dm_illum = open_asdf_datamodel(asdf_file_uri_illum, anon=True)\n",
    "\n",
    "# Final 3 dark exposures collected in this test\n",
    "asdf_file_uri_d1 = base + f'Activity_40/TVAC2_TOHOTQUAL_WFISAT_20240506225304_{WFI_TAG}_uncal.asdf'\n",
    "asdf_file_uri_d2 = base + f'Activity_40/TVAC2_TOHOTQUAL_WFISAT_20240506225611_{WFI_TAG}_uncal.asdf'\n",
    "asdf_file_uri_d3 = base + f'Activity_40/TVAC2_TOHOTQUAL_WFISAT_20240506225917_{WFI_TAG}_uncal.asdf'\n",
    "dm16 = open_asdf_datamodel(asdf_file_uri_d1, anon=True)\n",
    "dm17 = open_asdf_datamodel(asdf_file_uri_d2, anon=True)\n",
    "dm18 = open_asdf_datamodel(asdf_file_uri_d3, anon=True)\n",
    "\n",
    "# Close the files afterward\n",
    "dm_illum.close(); dm16.close(); dm17.close(); dm18.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3c5f7-6fb9-4696-a711-ab05fd3df0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of datamodel and info\n",
    "print(type(dm_illum))\n",
    "dm_illum.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955321a-8c47-4b29-9d9d-c602b7dfee02",
   "metadata": {},
   "source": [
    "## Calibrate the data\n",
    "We will run the illuminated data through Romancal. In particular we apply data quality initialization, saturation detection, reference pixel correction, and linearity. The rest of the steps are skipped. We do not process the darks in this way to save computing time and memory for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb26da8-6a97-4463-9b9b-9a07ebf0d944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put datamodels in a dictionary for utility\n",
    "exps = {\n",
    "    \"illum\": dm_illum,\n",
    "    \"exp16\": dm16,\n",
    "    \"exp17\": dm17,\n",
    "    \"exp18\": dm18,\n",
    "}\n",
    "\n",
    "# Pipeline options\n",
    "# Add more to change defaults on other steps and can also add an output directory\n",
    "pipe_kwargs = {\n",
    "    \"save_results\": False,\n",
    "    \"steps\": {\n",
    "        \"dark_current\": {\"skip\": True},\n",
    "        \"rampfit\": {\"skip\": True},\n",
    "        \"assign_wcs\": {\"skip\": True},\n",
    "        \"flatfield\": {\"skip\": True},\n",
    "        \"photom\": {\"skip\": True},\n",
    "        \"source_catalog\": {\"skip\": True},\n",
    "        \"tweakreg\": {\"skip\": True},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dictionary to save calibrated outputs\n",
    "results = {}\n",
    "for tag, f in exps.items():\n",
    "    try:\n",
    "        res = ExposurePipeline.call(f, **pipe_kwargs)\n",
    "        results[tag] = res\n",
    "        # Close model if it does not need to be stored in memory\n",
    "        # f.close()\n",
    "        # Adding a break to only do the illuminated exposure to limit computing time and storage space\n",
    "        break \n",
    "    except Exception as e:\n",
    "        print(f\"[{tag}] pipeline failed: {e}\")\n",
    "        results[tag] = None\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237294c8-f5a6-46c4-adc1-dedc19877697",
   "metadata": {},
   "source": [
    "### Optional: Super Bias from darks (diagnostic only — not applied)\n",
    "Data reductions usually include a step to compute and remove a superbias. This is a per-pixel intercept image typically obtained by fitting each pixel's ramp with a linear function and taking the intercept. These intercepts can be combined across multiple dark exposures to increase the signal to noise. We do not apply it in this tutorial because the Improved Roman Reference Correction (IRRC) step applied earlier should remove the bias offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31c188-2f0a-43a6-b883-81ba9fa91ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick function over chunks to determine the per-pixel biases \n",
    "def bias_intercept_from_cube(cube, drop_first=True, chunk=(512, 512), out_dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Compute per-pixel intercept (bias) from a cube shaped (N, rows, cols),\n",
    "    fitting y(t) = a + b t across frames for each pixel, using chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cube : np.ndarray\n",
    "        Array with shape (N, rows, cols).\n",
    "    drop_first : bool\n",
    "        If True, ignore frame 0 and fit frames 1..N-1.\n",
    "    chunk : (int, int)\n",
    "        Chunk size (rows, cols).\n",
    "    out_dtype : np.dtype\n",
    "        Output dtype for the bias image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    B : np.ndarray\n",
    "        Bias image (rows, cols) as out_dtype.\n",
    "    \"\"\"\n",
    "    cube = np.asarray(cube)  # (N, H, W)\n",
    "    N_total, rows, cols = cube.shape\n",
    "    start = 1 if drop_first else 0\n",
    "    t = np.arange(start, N_total, dtype=np.float64)\n",
    "    n = t.size\n",
    "    if n < 2:\n",
    "        raise ValueError(\"Need at least 2 frames (after dropping reset) to fit a line.\")\n",
    "\n",
    "    sum_t  = t.sum()\n",
    "    sum_t2 = (t * t).sum()\n",
    "    den = n * sum_t2 - sum_t * sum_t\n",
    "    if den == 0:\n",
    "        raise RuntimeError(\"Degenerate time vector; cannot fit.\")\n",
    "\n",
    "    B = np.empty((rows, cols), dtype=out_dtype)\n",
    "\n",
    "    rt, ct = chunk\n",
    "    for r0 in range(0, rows, rt):\n",
    "        r1 = min(rows, r0 + rt)\n",
    "        for c0 in range(0, cols, ct):\n",
    "            c1 = min(cols, c0 + ct)\n",
    "\n",
    "            # Load tile across all frames (avoid Python loops); keep memory modest with float32\n",
    "            arr = cube[start:, r0:r1, c0:c1].astype(np.float32, copy=False)  # (n, rt', ct')\n",
    "\n",
    "            # Accumulate sums (float64 math for accuracy)\n",
    "            sY  = arr.sum(axis=0, dtype=np.float64) # Sum(Y)\n",
    "            sTY = np.tensordot(t, arr, axes=(0, 0)) # Sum(T * Y)\n",
    "\n",
    "            slope = (n * sTY - sum_t * sY) / den                              # b\n",
    "            intercept = (sY - slope * sum_t) / n                              # a\n",
    "\n",
    "            B[r0:r1, c0:c1] = intercept.astype(out_dtype, copy=False)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04848f-be44-4e2f-851b-c747c10166f8",
   "metadata": {},
   "source": [
    "Now compute the bias for each of the three dark exposures, stack them, and plot the resulting \"superbias.\" These darks were collected as the final three exposures of a series of 18 55-frame darks collected after the bright source images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317dfb04-7cb3-4183-8297-fae0fd4feb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "B16 = bias_intercept_from_cube(dm16.data, drop_first=True, chunk=(512, 512), out_dtype=np.float32)\n",
    "B17 = bias_intercept_from_cube(dm17.data, drop_first=True, chunk=(512, 512), out_dtype=np.float32)\n",
    "B18 = bias_intercept_from_cube(dm18.data, drop_first=True, chunk=(512, 512), out_dtype=np.float32)\n",
    "\n",
    "# Combine into a \"superbias\"\n",
    "stack    = np.stack([B16, B17, B18], axis=0)        # (3, H, W)\n",
    "B_median = np.nanmedian(stack, axis=0).astype(np.float32)\n",
    "B_mean   = np.nanmean(stack, axis=0).astype(np.float32)\n",
    "\n",
    "# Visualization of the superbias\n",
    "vmin, vmax = np.nanpercentile(B_median, [1, 99])\n",
    "plt.imshow(B_median, vmin=vmin, vmax=vmax, origin=\"lower\")\n",
    "plt.title(\"Bias (Median of FinalDark exp16–18)\", fontweight=\"bold\", fontsize=12)\n",
    "cbar = plt.colorbar(); cbar.set_label(\"DN (1–99% stretch)\", fontweight=\"bold\")\n",
    "plt.xlabel(\"x [pix]\")\n",
    "plt.ylabel(\"y [pix]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8af80-2707-4808-b7e9-04486085555a",
   "metadata": {},
   "source": [
    "We can verify that the Romancal refpix step was indeed applied. If not, then we can subtract the superbias with\n",
    "\n",
    "`dm_cube = np.float32(results[\"illum\"].data) - B_median[None, :, :]`\n",
    "\n",
    "and continue to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86eb8d-5f7a-4bd4-9f75-1f950ec1b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the refpix step was indeed completed. If so, then we do not need\n",
    "# to apply another bias.\n",
    "cal = getattr(results[\"illum\"].meta, \"cal_step\", None)\n",
    "print(\"The refpix step is\", getattr(cal, \"refpix\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d2fec-6a55-47f6-a5d9-742da1ec2229",
   "metadata": {},
   "source": [
    "## Make  plots\n",
    "We will first look at the signal in three arbitary frames to see how the signal builds with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e92430-fe69-4dc8-bcce-1ee701beea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_cube = results[\"illum\"].data\n",
    "frames = [2, 27, 54]\n",
    "baseline_frame = 1\n",
    "\n",
    "# Compute per-frame differences and percentiles\n",
    "diffs, vmins, vmaxs = [], [], []\n",
    "for nframe in frames:\n",
    "    d = dm_cube[nframe] - dm_cube[baseline_frame]\n",
    "    diffs.append(d)\n",
    "    vmin_i = np.percentile(d, 2)\n",
    "    vmax_i = np.percentile(d, 98)\n",
    "    vmins.append(vmin_i)\n",
    "    vmaxs.append(vmax_i)\n",
    "    print(f\"frame {nframe}: 2nd={vmin_i:.3g}, 98th={vmax_i:.3g}\")\n",
    "\n",
    "# Plotting\n",
    "vmin = min(vmins)\n",
    "vmax = max(vmaxs)\n",
    "fig, axes = plt.subplots(1, len(frames), figsize=(5*len(frames), 4), constrained_layout=True)\n",
    "if len(frames) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "im = None\n",
    "for ax, nframe, d in zip(axes, frames, diffs):\n",
    "    im = ax.imshow(d, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(\n",
    "        f\"SCA {SCA}, Mag {Mag}, frames {nframe}-1\",\n",
    "        fontweight=\"bold\", fontsize=14\n",
    "    )\n",
    "    ax.set_xlabel(\"x [pix]\")\n",
    "    ax.set_ylabel(\"y [pix]\")\n",
    "\n",
    "# Shared colorbar\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.9)\n",
    "cbar.set_label(\"DN\", fontweight=\"bold\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f245123-cf9c-4326-bead-1cf401c997e0",
   "metadata": {},
   "source": [
    "### Cutouts around the bright source\n",
    "The rings you can see faintly in the leftmost image and increasing in signal going to the rightmost image come from the telescope simulator projector system and are not expected in flight. The dark core corresponds to pixels that saturate in the first frame of the ramp and is an artifact of the image processing.\n",
    "\n",
    "We will now take a closer look at a cutout around the bright source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16daea-50f3-4cb4-81cb-e5020b5063c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutout region\n",
    "col_0=1900\n",
    "row_0=1900\n",
    "q1=256\n",
    "q2=256\n",
    "# Get the cutouts for the raw and corrected data\n",
    "raw_cube_cutout = np.asarray(dm_illum.data[:, row_0-q1:row_0+q1, col_0-q2:col_0+q2])\n",
    "dm_cube_cutout = dm_cube[:,row_0-q1:row_0+q1, col_0-q2:col_0+q2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44fff5-7bd7-4712-b869-de61ef1cf4b5",
   "metadata": {},
   "source": [
    "#### Data quality flags and saturation\n",
    "One can use the MASK reference file to eliminate pixels with known issues. See the [RDox pages on the data quality (DQ) flags](https://roman-pipeline.readthedocs.io/en/latest/roman/dq_init/reference_files.html) for more details and the correspondence between values and pixel issue.\n",
    "\n",
    "For example, if you want to set mask bits for bad pixels, saturated pixels, data affected by guide windows we could do the following to identify pixels that do have flags for those issues:\n",
    "```\n",
    "mask_bits = 1 | 2 | 16\n",
    "good = (pixeldq & mask_bits) == 0\n",
    "```\n",
    "\n",
    "Here, we will take account of any flagged pixel. We can then visualize the cutouts and mark flagged pixels in white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f9e42-6859-43ad-ab9c-11b71aa59c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pixel DQ flags for the cutout\n",
    "pixeldq = results[\"illum\"].pixeldq[row_0-q1:row_0+q1, col_0-q2:col_0+q2]\n",
    "bad = (pixeldq != 0)\n",
    "good = ~bad\n",
    "print(\"Good pixel count:         \", good.sum(), \"of\", pixeldq.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0aad4-bc0e-42de-9ed6-25abecae4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do another set of plots for the cutouts and set the flagged pixels to white\n",
    "cmap = plt.cm.viridis.copy()\n",
    "cmap.set_bad(color=\"white\")\n",
    "\n",
    "# Compute diffs and per-frame percentiles over good pixels only\n",
    "diffs_masked, vmins, vmaxs = [], [], []\n",
    "for nframe in frames:\n",
    "    d = dm_cube_cutout[nframe] - dm_cube_cutout[baseline_frame]\n",
    "    dm = np.ma.array(d, mask=bad)\n",
    "    diffs_masked.append(dm)\n",
    "\n",
    "    if good.any():\n",
    "        vmin_i = np.percentile(d[good], 2)\n",
    "        vmax_i = np.percentile(d[good], 98)\n",
    "    else:\n",
    "        # if everything is masked\n",
    "        vmin_i, vmax_i = np.min(d), np.max(d)\n",
    "    vmins.append(vmin_i); vmaxs.append(vmax_i)\n",
    "    print(f\"frame {nframe}: 2nd={vmin_i:.3g}, 98th={vmax_i:.3g}\")\n",
    "\n",
    "# Shared limits across panels\n",
    "vmin, vmax = min(vmins), max(vmaxs)\n",
    "\n",
    "# Plot side-by-side with one shared colorbar\n",
    "fig, axes = plt.subplots(1, len(frames), figsize=(5*len(frames), 4), constrained_layout=True)\n",
    "if len(frames) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "im = None\n",
    "for ax, nframe, dm in zip(axes, frames, diffs_masked):\n",
    "    im = ax.imshow(dm, vmin=vmin, vmax=vmax, cmap=cmap, origin=\"lower\")\n",
    "    ax.set_title(f\"SCA {SCA}, Mag {Mag}, frames {nframe}-1\", fontweight=\"bold\", fontsize=14)\n",
    "    ax.set_xlabel(\"x [pix]\"); ax.set_ylabel(\"y [pix]\")\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, shrink=0.9)\n",
    "cbar.set_label(\"DN\", fontweight=\"bold\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82aeb04-b4bf-4c0b-86fd-f1117735c3f4",
   "metadata": {},
   "source": [
    "## Pixel-Level Diagnostics: Saturation and Relative Slope\n",
    "In the following section, we will identify saturated (and neighboring) pixels using the per-pixel DN limits and then visualize how each frame's instantaneous slope deviates from the average slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566807c2-0e42-4a33-b40a-7a6ebf397003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saturation thresholds (DN) from the reference file\n",
    "sat_path = os.path.expanduser(\"~/crds_cache/references/roman/wfi/roman_wfi_saturation_0030.asdf\")\n",
    "print(sat_path)\n",
    "\n",
    "with SaturationRefModel(sat_path) as satref:\n",
    "    # 4096 x 4096 array thresholds in DN\n",
    "    sat_dn = np.asarray(satref.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ed14f-cee8-48ce-8818-931b7b05b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to identify saturation\n",
    "def _align_sat_dn(sat_dn, frames_hw):\n",
    "    \"\"\"\n",
    "    Make sat_dn match the shape of the data cube\n",
    "    If sat_dn is 4096x4096 and frames are 4088x4088 (no ref pixels),\n",
    "    crop 4 pixels off each edge.\n",
    "    \"\"\"\n",
    "    H, W = frames_hw\n",
    "    sh, sw = sat_dn.shape\n",
    "    if (sh, sw) == (H, W):\n",
    "        return sat_dn\n",
    "    if (sh - H == 8) and (sw - W == 8):\n",
    "        # trim reference pixels: center-crop by 4 on each side\n",
    "        return sat_dn[4:-4, 4:-4]\n",
    "    raise ValueError(f\"sat_dn shape {sat_dn.shape} does not match frames {(H, W)}\")\n",
    "\n",
    "def make_saturation_mask_dn(frames_dn, sat_dn):\n",
    "    \"\"\"\n",
    "    DN-based saturation masks using a per-pixel DN threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frames_dn : (N, H, W) float/uint array\n",
    "        Non-destructive-read frames in DN.\n",
    "    sat_dn : (H, W) float array\n",
    "        Per-pixel saturation limits in DN. NaNs are treated as 'no check' (never saturate).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sat_mask        : (N, H, W) bool  -- pixels >= sat_dn in each frame\n",
    "    nearby_sat_mask : (N, H, W) bool  -- 4-connected neighbors of saturated pixels (excluding the saturated ones)\n",
    "    new_sat_mask    : (N, H, W) bool  -- pixels that become saturated in current frame (unsat in prev)\n",
    "    diag_sat_mask   : (N, H, W) bool  -- diagonal neighbors (excluding sat + nearby)\n",
    "    \"\"\"\n",
    "    assert frames_dn.ndim == 3, \"frames_dn must be (N,H,W)\"\n",
    "    N, H, W = frames_dn.shape\n",
    "\n",
    "    # Align and sanitize sat_dn\n",
    "    sat_dn_use = _align_sat_dn(np.asarray(sat_dn), (H, W))\n",
    "    \n",
    "    # Treat NaN thresholds as never saturating\n",
    "    sat_dn_use = np.where(np.isfinite(sat_dn_use), sat_dn_use, np.inf).astype(frames_dn.dtype, copy=False)\n",
    "\n",
    "    # Core saturation check\n",
    "    sat_mask = frames_dn >= sat_dn_use  # (N,H,W) bool\n",
    "\n",
    "    # Neighborhood kernels (no center in 4-neighborhood kernel)\n",
    "    kernel_4 = np.array([[0,1,0],\n",
    "                         [1,0,1],\n",
    "                         [0,1,0]], dtype=np.uint8)\n",
    "    kernel_diag = np.array([[1,0,1],\n",
    "                            [0,0,0],\n",
    "                            [1,0,1]], dtype=np.uint8)\n",
    "\n",
    "    nearby_sat_mask = np.zeros_like(sat_mask, dtype=bool)\n",
    "    diag_sat_mask   = np.zeros_like(sat_mask, dtype=bool)\n",
    "    new_sat_mask    = np.zeros_like(sat_mask, dtype=bool)\n",
    "\n",
    "    for i in range(N):\n",
    "        s = sat_mask[i]\n",
    "\n",
    "        # Convolve boolean as uint8 to count neighboring saturated pixels\n",
    "        conv4  = convolve2d(s.astype(np.uint8), kernel_4,  mode='same', boundary='fill', fillvalue=0) > 0\n",
    "        convdg = convolve2d(s.astype(np.uint8), kernel_diag, mode='same', boundary='fill', fillvalue=0) > 0\n",
    "\n",
    "        # neighbors that are not themselves saturated\n",
    "        nearby_sat_mask[i] = (~s) & conv4\n",
    "        # diagonal neighbors that are neither saturated nor 4-neighbors\n",
    "        diag_sat_mask[i]   = (~s) & (~nearby_sat_mask[i]) & convdg\n",
    "\n",
    "        # newly saturated in this frame (unsaturated previously)\n",
    "        if i == 0:\n",
    "            new_sat_mask[i] = s\n",
    "        else:\n",
    "            new_sat_mask[i] = s & (~sat_mask[i-1])\n",
    "\n",
    "    return sat_mask, nearby_sat_mask, new_sat_mask, diag_sat_mask\n",
    "\n",
    "\n",
    "def make_mean_debiased_difference_image_dn(frames_dn, raw_frames_dn, sat_dn):\n",
    "    \"\"\"\n",
    "    Compute the mean over time of frame-to-frame differences (DN) while excluding\n",
    "    saturated pixels and their 4-neighbors based on per-pixel DN limits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frames_dn     : (N,H,W) array in DN\n",
    "        Pre-processed frames (e.g., bias-subtracted) used to compute differences.\n",
    "    raw_frames_dn : (N,H,W) array in DN\n",
    "        Raw (or minimally processed) frames aligned with `frames_dn` used for saturation masking.\n",
    "    sat_dn        : (H,W) array in DN\n",
    "        Per-pixel saturation limits in DN.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_diff_frame : (H,W) float32 array\n",
    "        Mean of diffs along time, excluding saturated + nearby pixels.\n",
    "        NaN where no valid samples remain.\n",
    "    \"\"\"\n",
    "    assert frames_dn.shape == raw_frames_dn.shape, \"frames_dn and raw_frames_dn must align (N,H,W)\"\n",
    "    # Differences along time axis\n",
    "    frame_diffs = np.diff(frames_dn, axis=0)  # shape (N-1,H,W)\n",
    "\n",
    "    # Build masks from the raw data (DN)\n",
    "    sat_mask, nearby_mask, _, _ = make_saturation_mask_dn(raw_frames_dn, sat_dn)\n",
    "    # Align mask count with diffs (diffs start at frame 1)\n",
    "    valid = ~(sat_mask[1:] | nearby_mask[1:])  # (N-1,H,W) bool\n",
    "\n",
    "    # Weighted mean with binary weights; safe against zero-division\n",
    "    w = valid.astype(np.float32)\n",
    "    num = (frame_diffs * w).sum(axis=0, dtype=np.float64)\n",
    "    den = w.sum(axis=0, dtype=np.float64)\n",
    "\n",
    "    mean_diff_frame = np.divide(\n",
    "        num, den,\n",
    "        out=np.full(den.shape, np.nan, dtype=np.float32),\n",
    "        where=den > 0\n",
    "    ).astype(np.float32, copy=False)\n",
    "\n",
    "    return mean_diff_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb745d1a-ff73-44f1-9c95-b2332b58cdbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a cutout for the saturation array to match the earlier raw and calibrated cutouts\n",
    "sat_dn_cutout = sat_dn[row_0-q1:row_0+q1, col_0-q2:col_0+q2]\n",
    "\n",
    "# Calculate the Mean slope image\n",
    "mean_slope_image = make_mean_debiased_difference_image_dn(dm_cube_cutout, raw_cube_cutout, sat_dn_cutout)\n",
    "\n",
    "# Get the 3D saturation mask\n",
    "sat_mask, nearby_sat_mask, new_sat_mask, diag_mask= make_saturation_mask_dn(raw_cube_cutout, sat_dn_cutout)\n",
    "\n",
    "# Get the difference images (AKA the instantaneous slope)\n",
    "difference_images = np.diff(dm_cube_cutout, axis=0)                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79134c68-935c-455a-9f42-2e9472c3c684",
   "metadata": {},
   "source": [
    "### Final visualizations of saturation and slope deviations\n",
    "For each of the three representative frames (the 3rd, the middle, and the last), we will plot the following:\n",
    "- **Left**: the raw image for that frame (scaled by $10^3$ for display).\n",
    "- **Center**: a relative slope map defined as (difference_images[i-1] / mean_diff) to highlight pixels whose per-frame change deviates from the average slope. Note that saturation and nonlinear effects can push values below 1.\n",
    "- **Right**: visualization of the combined mask encoding saturated/nearby/corrupted pixels via a weighted sum.\n",
    "We see trends in these plotted quantities as the exposure time progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b5b8e-cf25-4fd0-a8ef-c04f62ee90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of frames\n",
    "N = len(dm_cube_cutout)\n",
    "\n",
    "# Choose three frames: first usable (2), middle, and last\n",
    "frames_to_plot = sorted({2, max(2, N//2), N-1})\n",
    "print(\"Plotting frames:\", frames_to_plot)\n",
    "\n",
    "for i in frames_to_plot:\n",
    "    if i < 2 or i >= N:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 3), sharex=True, sharey=True)\n",
    "\n",
    "    # Left: current frame signal (rescale to ~same range as the original)\n",
    "    cax1 = axes[0].imshow(dm_cube_cutout[i] / 1e3, cmap='inferno_r', origin='lower', vmin=0, vmax=80)\n",
    "    cbar1 = plt.colorbar(cax1, ax=axes[0], label='Signal [$\\\\times 10^3\\\\, DN$]')\n",
    "\n",
    "    # Center: difference / mean slope (robust to zeros/NaNs)\n",
    "    ratio = np.divide(\n",
    "        difference_images[i-1], # assumes difference_images is length N-1\n",
    "        mean_slope_image,\n",
    "        out=np.full_like(difference_images[i-1], np.nan, dtype=np.float32),\n",
    "        where=np.isfinite(mean_slope_image) & (mean_slope_image != 0)\n",
    "    )\n",
    "    cax2 = axes[1].imshow(ratio, cmap='RdYlBu', vmin=0.75, vmax=1.25, origin='lower')\n",
    "    cbar2 = plt.colorbar(cax2, ax=axes[1], label='Frame Slope / Mean Slope')\n",
    "\n",
    "    # Right: masks (saturated, nearby saturated, corrupted)\n",
    "    cax3 = axes[2].imshow(sat_mask[i]*3. + nearby_sat_mask[i]*2 + diag_mask[i], origin='lower', cmap='magma_r')\n",
    "    cbar3 = plt.colorbar(cax3, ax=axes[2], label='Mask')\n",
    "    \n",
    "    # Titles & annotation\n",
    "    axes[0].set_title('Image')\n",
    "    axes[1].set_title('Difference / Mean Slope')\n",
    "    axes[2].set_title('Saturated or Corrupted')\n",
    "\n",
    "    axes[0].text(\n",
    "        0.05, 0.95, f'Frame {i}', ha='left', va='top', transform=axes[0].transAxes,\n",
    "        bbox=dict(facecolor='w', edgecolor='0.8', pad=5.0)\n",
    "    )\n",
    "\n",
    "    # Zoom window settings\n",
    "    #for ax in axes:\n",
    "    #    ax.set_xlim(6, 36)\n",
    "    #    ax.set_ylim(6, 36)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb067e10-2c63-4304-a689-6b90380b7e4b",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Schlieder et al. 2024](https://ui.adsabs.harvard.edu/abs/2024SPIE13092E..0SS/abstract)\n",
    "- [TVAC2 Bright Star Saturation Test Summary](https://asd.gsfc.nasa.gov/roman/WFI_Bright_Star/TVAC2_Bright_star_saturation_summary_release.pdf)\n",
    "- [romancal](https://roman-pipeline.readthedocs.io/en/latest/index.html)\n",
    "- [Roman Documentation](https://roman-docs.stsci.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6189b7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595cd2b-964a-4f68-8270-1292b3ad9ed8",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "**Author:** Dana Louie, Robby Wilson, Ami Choi\\\n",
    "**Updated On:** 2025-09-29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc374b4-90f1-49b5-9a38-d5c83858362c",
   "metadata": {},
   "source": [
    "<table width=\"100%\" style=\"border:none; border-collapse:collapse;\">\n",
    "  <tr style=\"border:none;\">\n",
    "    <td style=\"border:none; width:180px; white-space:nowrap;\">\n",
    "       <a href=\"#top\" style=\"text-decoration:none; color:#0066cc;\">↑ Top of page</a> \n",
    "    </td>\n",
    "    <td style=\"border:none; text-align:center;\">\n",
    "       <img src=\"../../roman_logo.png\" width=\"50\">\n",
    "    </td>\n",
    "    <td style=\"border:none; text-align:right;\">\n",
    "       <img src=\"../../stsci_logo2.png\" width=\"90\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
