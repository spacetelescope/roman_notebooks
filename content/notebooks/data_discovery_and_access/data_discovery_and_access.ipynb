{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Roman Research Nexus Data Discovery and Access in the Cloud \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Information and Read-Only Status\n",
    "\n",
    "To run this notebook, please select the \"Roman Calibration\" kernel at the top right of your window.\n",
    "\n",
    "This notebook is read-only. You can run cells and make edits, but you must save changes to a different location. We recommend saving the notebook within your home directory, or to a new folder within your home (e.g. <span style=\"font-variant:small-caps;\">file > save notebook as > my-nbs/nb.ipynb</span>). Note that a directory must exist before you attempt to add a notebook to it.\n",
    "\n",
    "## Imports\n",
    "Here we import the required packages for our data access examples including:\n",
    "- *asdf* for accessing ASDF files\n",
    "- *astropy.io fits* for accessing FITS files\n",
    "- *astropy.mast Observations* for accessing, searching, and selecting data from other missions\n",
    "- *s3fs* for streaming in data directly from the cloud\n",
    "- *roman_datamodels* for opening Roman ASDF files. You can find additional information on how to work with ASDF files in the Working with ASDF notebook tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import asdf\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "import s3fs\n",
    "import roman_datamodels as rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "This notebook is designed to provide examples of accessing data from the Research Nexus. Due to its survey nature, the Roman Space Telescope will produce large volumes of data that will need to be easily and quickly accessed to perform scientific tasks like creating catalogs, performing difference imaging, generating light curves, etc. Downloading all the required data would burden most users by requiring excessive data storage solutions (likely >10TB).\n",
    "\n",
    "This notebook demonstrates how to stream data from the cloud directly into memory, bypassing the need to download the data locally and use excess storage. This method of cloud-based data access is *HIGHLY* recommended. However, we understand that some use-cases will require downloading the data locally, so we provide an example at the end of the notebook.\n",
    "\n",
    "During operations, each Roman data file will be given a Unique Resource Identifier (URI), an analog to an online filepath that is similar to a URL, which points to where the data is hosted on the AWS cloud. Users will retrieve these URIs from one of several sources including MAST (see [Accessing WFI Data](https://roman-docs.stsci.edu/data-handbook-home/accessing-wfi-data) for more information) and will be able to use the URI to access the desired data from the cloud. \n",
    "\n",
    "Here-in we examine how to download data from two types of sources:\n",
    "- The STScI MAST server which hosts data for in-flight telescopes including Hubble, TESS, and JWST and will host Roman data in the future\n",
    "- Simulated Roman Space Telescope data hosted in storage containers on the AWS cloud\n",
    "\n",
    "### Defining terms\n",
    "- *Cloud computing*: the practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than using a local server or a personal computer.\n",
    "- *AWS*: Amazon Web Services (AWS) is the cloud computing platform provided by Amazon.\n",
    "- *URI*: a Universal Resource Identifier (URI) is a sequence of characters that identifies a name or a unique resource on the Internet. URLs for websites are a subclass of URIs.\n",
    "- *AWS S3*: Amazon Simple Storage Service (S3) is a scalable and cost-effective object storage service on the AWS cloud platform. Storage containers within S3 are knwon as \"buckets,\" so we often refer to these storage devices as \"S3 buckets\" or \"S3 servers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing MAST Data\n",
    "In this section, we will go through the steps to retreive archived MAST data from the cloud including how to query the archive and stream the files directly from the cloud, as well as download them locally.\n",
    "\n",
    "### Enabling Cloud Access\n",
    "The most important step for accessing data from the cloud is to enable *astroquery* to retreive URIs and other relevant cloud information. Even if we are working locally and plan to download the data files (not recommended for Roman data), we need to use this command to copy the file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Observations.enable_cloud_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying MAST\n",
    "Now we are ready to begin our query. This example is rather simple, but it is quick and easy to reproduce. We will be querying HST WFC3/IR data of M85. In practice, the science platform should primarily be used for analyzing and exploring Roman data products. However due to the smaller file sizes, HST WFC3/IR data provides a nice example. The process is identical regardless of which space telescope is used.\n",
    "\n",
    "In our query, we specify that we want to look at HST data using the F160W filter and WFC3/IR. We also specify the proposal id to easily get the data of interest. Once we get the desired observations, we gather the list of products that go into the observations. We then filter the products to gather all the level 3 science data products associated with a specific project which still leaves us with 60 data products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query MAST for matching observations\n",
    "obs = Observations.query_criteria(obs_collection='HST',\n",
    "                                  filters='F160W',\n",
    "                                  instrument_name='WFC3/IR',\n",
    "                                  proposal_id=['11360'],\n",
    "                                  dataRights='PUBLIC')\n",
    "# get the list of products (files)\n",
    "products = Observations.get_product_list(obs)\n",
    "\n",
    "# filter the products\n",
    "filtered = Observations.filter_products(products,\n",
    "                                        calib_level=[3], \n",
    "                                        productType=['SCIENCE'], \n",
    "                                        dataproduct_type=['image'], \n",
    "                                        project=['CALWF3'])\n",
    "print('Filtered data products:\\n', filtered, '\\n')\n",
    "\n",
    "# filter for just one product\n",
    "single =  Observations.filter_products(filtered,\n",
    "                                       obsID='24797441')\n",
    "print('Single data product:\\n', single, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our desired products, we can gather the URIs for each of the files which indicate their locations in the MAST AWS S3 servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = Observations.get_cloud_uris(filtered)\n",
    "uris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_cloud_uris` method checks for duplicates in the provided products to minimize the data access volume. It is also important to note that `get_cloud_uris` will always return a list. Thus, we need to extract an individual URI string to access the file. Here we choose the first URI, but in practice you would select the URI associated with the desired file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = uris[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming files directly into memory\n",
    "Here, we will use `fsspec` to directly access the data stored in the AWS S3 servers. Because the URI points to a FITS file, we can use `fits.open` to access the information in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(uri, 'readonly', fsspec_kwargs={\"anon\":True}) as HDUlist:\n",
    "    HDUlist.info()\n",
    "    sci = HDUlist[1].data\n",
    "    \n",
    "type(sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming from the Roman Research Nexus S3 Bucket\n",
    "\n",
    "Though Roman data will eventually be available through MAST, we currently offer a small set of simulated data available via an AWS Open Data Program S3 bucket. These files can be streamed in exactly the same way as the HST FITS file above. Additionally, we can browse the available files similarly to a Unix terminal. A full list of commands can be found in the `s3fs` documentation [here](https://s3fs.readthedocs.io/en/latest/api.html#).\n",
    "\n",
    "We can view the files in the S3 bucket by performing a list command (`ls`) on the main Nexus directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "asdf_dir_uri = 's3://stpubdata/roman/nexus/soc_simulations/tutorial_data/'\n",
    "fs.ls(asdf_dir_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fs.ls()` command allows us to list the contents of the URI. In the above example, the `s3://stpubdata/roman/nexus/soc_simulations/tutorial_data/` bucket contains numerous files for the notebook tutorials.\n",
    "\n",
    "In the next subsection, we will explore opening data files made using the Roman image simulator \"Roman I-Sim.\" These simulations are saved in the same file formats as Roman data and are useful to help develop file ingestion pipelines. More Roman I-Sim simulated data will be made available in the future.\n",
    "\n",
    "For more information on the available data products, please visit the [Simulated Data Products](../../../markdown/simulated-data.md) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening Roman I-Sim Data\n",
    "\n",
    "Diving into the S3 bucket, we find several different files:\n",
    "- `*_uncal.asdf`: Level 1 (L1; ucalibrated ramp cube) files.\n",
    "- `*_cal.asdf`: Level 2 (L2; calibrated rate image) files.\n",
    "- `*_coadd.asdf`: Level 3 (L3; resampled image) files.\n",
    "- Some additional miscellaneous files for various tutorials.\n",
    "\n",
    "To learn how these files were generated, please see the [Roman I-Sim](../romanisim/romanisim.ipynb) tutorial notebook.\n",
    "\n",
    "As you can see, Roman WFI data are stored in Advanced Scientific Data Format (ASDF) files. See the tutorial notebook [Working with ASDF](../working_with_asdf/working_with_asdf.ipynb) for more information. Regarding file names, note that the first element (separated by underscores) of the file name string of L1* and L2* files denotes programmatic information (e.g., program ID, visit ID, etc.), while the second element gives the exposure number within the visit. Thus, `r0003201001001001004_0001_wfi02_f106_cal.asdf` and `r0003201001001001004_0002_wfi02_f106_cal.asdf` are exposure level (L2) files from the same visit but represent exposures 1 and 2, respectively, of the detector WFI02. For simulated data, such as the files used for these tutorials, the first element of the file name string has been chosen simply as an example. The file naming convention for Roman is quite elaborate as each includes all the relevant information about the observation. For more information on the file naming conventions, please see the [Data Levels and Products](https://roman-docs.stsci.edu/data-handbook-home/wfi-data-format/data-levels-and-products) Roman documentation page.\n",
    "\n",
    "**Note:** Archival file names for the mosaic images (L3 products) are still being finalized. The L3 file in the S3 bucket has a generic file name (`my_roman_mosaic_coadd.asdf`) that does not follow any naming convention.\n",
    "\n",
    "Below, we use `roman_datamodels` to read the ASDF file corresponding to a dense region. To simplify the workflow, we are providing a URI to the data. Once the data will be available through MAST during operations, users will need to retrieve the URIs using astroquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf_file_uri = asdf_dir_uri + 'r0003201001001001004_0001_wfi11_f106_cal.asdf'\n",
    "\n",
    "with fs.open(asdf_file_uri, 'rb') as f:\n",
    "    dm = rdm.open(f)\n",
    "    dm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening OpenUniverse Simulated Data\n",
    "\n",
    "The OpenUniverse data is hosted in its own S3 bucket (see the [OpenUniverse AWS Open Data](https://registry.opendata.aws/openuniverse2024/) website for more information). Additionally, IPAC has created two [OpenUniverse notebooks](https://irsa.ipac.caltech.edu/docs/notebooks/) showcasing how to interact with the original data and catalog files. In this notebook, we focus on how to access the files.\n",
    "\n",
    "The simulations are natively saved as FITS files and are divided by survey (the Wide Area Survey (WAS) or the Time Domain Survey (TDS)), optical element, and HEALPix cell ([HEALPix](https://healpix.sourceforge.io) is a commonly used way to uniformly discretize the area of a sphere). Please see [Simulated Data Products](../../../markdown/simulated-data.md) for more information about specific OpenUniverse data products.\n",
    "\n",
    "Below we provide an example of streaming a simulated \"calibrated\" image FITS file from an S3 bucket using an alternate method. Instead of initializing our own `S3FileSystem`, we pass the credentials (anonymous credentails in this case, as the data is public) directly to `fits.open`, allowing it to create the file system. This shorthand is covenient when the URI is explicitly provided, but it does not allow exploration of the S3 directory structure without intitializing the `S3FileSystem` separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3bucket = 's3://nasa-irsa-simulations/openuniverse2024/roman/preview/RomanWAS/images/simple_model'\n",
    "band = 'F184'\n",
    "hpix = '15297'\n",
    "sensor = 11\n",
    "s3fpath = s3bucket+f'/{band}/{hpix}/Roman_WAS_simple_model_{band}_{hpix}_{sensor}.fits.gz'\n",
    "\n",
    "fits_file = fits.open(s3fpath, fsspec_kwargs={'anon':True})\n",
    "print(fits_file.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Files (not recommended)\n",
    "\n",
    "Downloading Roman data products is not recommended due to their large file sizes and the high volume expected from the mission's survey nature. Instead, users are encouraged to adopt workflows that utilize the file streaming services described above for an optimal experience.\n",
    "\n",
    "However, in specific science cases, downloading files may be necessary. To do so, you can use the URIs along with the `S3FileSystem.get` function (documentation available [here](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem.get)). The code snippet below demonstrates how to download data to your personal instance of the Research Nexus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out as this use case is not recommended and should only be needed in rare circumstances\n",
    "# from pathlib import Path\n",
    "# URI =  ## Set this to the URI string you want to download.\n",
    "# local_file_path = Path('data/')\n",
    "# local_file_path.mkdir(parents=True, exist_ok=True)\n",
    "# fs = s3fs.S3FileSystem()\n",
    "# fs.get(URI, local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "Additional information can be found at the following links:\n",
    "\n",
    "- [`s3fs` Documentation](https://s3fs.readthedocs.io/en/latest/api.html#)\n",
    "- [Working with ASDF Notebook](../working_with_asdf/working_with_asdf.ipynb)\n",
    "- [OpenUniverse AWS Open Data](https://registry.opendata.aws/openuniverse2024/)\n",
    "- [OpenUniverse notebooks](https://irsa.ipac.caltech.edu/docs/notebooks/)\n",
    "- [Simulated Data Products Document](../../../markdown/simulated-data.md)\n",
    "- [MNRAS paper detailing Open Universe data simulation methods (Troxel et al 2021)](https://ui.adsabs.harvard.edu/abs/2021MNRAS.501.2044T/abstract)\n",
    "- [MNRAS paper detailing the previewed Open Universe data (Troxel et al 2023)](https://ui.adsabs.harvard.edu/abs/2023MNRAS.522.2801T/abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this Notebook\n",
    "The data streaming information from this notebook largely builds off of the TIKE data-acces notebook by Thomas Dutkiewicz.\n",
    "\n",
    "**Author:** Will C. Schultz, Tyler Desjardins \\\n",
    "**Updated On:** 2025-09-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Roman Calibration",
   "language": "python",
   "name": "roman-cal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
