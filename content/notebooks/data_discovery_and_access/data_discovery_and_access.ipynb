{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Roman Science Platform Data Discovery and Access in the Cloud \n",
    "***\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "This notebook is designed to walk you through accessing data from the [Mikulski Archive for Space Telescopes (MAST)](https://archive.stsci.edu/) and retriving simulated [Roman Wide Field Instrument (WFI)](https://roman-docs.stsci.edu/roman-instruments-home) images.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\\- [0. Introduction](#intro)\n",
    "\n",
    "\\- [1. Accessing MAST Data](#mast_data)\n",
    "\n",
    "\\- [2. Streaming from the Roman Science Platform S3 Bucket](#s3_bucket)\n",
    "\n",
    "\\- [3. Downloading Files (**not recommended**)](#download)\n",
    "\n",
    "***\n",
    "\n",
    "<a id = intro></a>\n",
    "# 0. Introduction\n",
    "This notebook is designed to provide examples of accessing data from the Roman Science Platform (RSP). Due to the survey nature of the Roman Space Telescope, it will produce large data volumes of data that will need to be easily and quickly accessed to perform scientific tasks like creating catalogs, difference imaging, generating light curves, etc. Downloading all the required data would burden most users by requiring excessive data storage solutions (likely `>10TB`).\n",
    "\n",
    "This notebook demonstrates how to stream data from the cloud directly into memory, bypassing the need to download the data locally and use excess storage. This method of cloud-based data access is **HIGHLY** recommended. However, we understand that some use-cases will require downloading the data locally, so we also provide an example of how to do this at the end of the notebook.\n",
    "\n",
    "During operations, each Roman data file will be given a Unique Resource Identifier (URI), an analog to an online filepath that is similar to a URL, which points to where the data is hosted on the Amazon Web Services (AWS) cloud. Users will retrieve these URIs from one of several sources including MAST (see [Accessing WFI Data](https://roman-docs.stsci.edu/data-handbook-home/accessing-wfi-data) for more information) and will be able to use the URI to access the desired data from the cloud. \n",
    "\n",
    "Herein we examine how to download data from two types of sources:\n",
    "\n",
    "- The [STScI MAST server](https://archive.stsci.edu/) which hosts data for in-flight telescopes including Hubble, TESS, and JWST and will host Roman data in the future\n",
    "\n",
    "- Simulated Roman Space Telescope data hosted in storage containers on the AWS cloud\n",
    "\n",
    "***\n",
    "\n",
    "### Kernel Information\n",
    "\n",
    "To run this notebook on the Roman Science Platform, please select the \"Roman Calibration\" kernel at the top right of your window.\n",
    "\n",
    "### Imports\n",
    "Here we import the required packages for our data access examples including:\n",
    "\n",
    "- `asdf` for accessing `ASDF` files\n",
    "\n",
    "- `astropy.io fits` for accessing `FITS` files\n",
    "\n",
    "- `astropy.mast Observations` for accessing, searching, and selecting data from other missions\n",
    "\n",
    "- `s3fs` for streaming in data directly from the cloud\n",
    "\n",
    "- `roman_datamodels` for opening Roman `ASDF` files. You can find additional information on how to work with `ASDF` files in the [Working with ASDF](https://github.com/spacetelescope/roman_notebooks/tree/main/content/notebooks/working_with_asdf) notebook tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import asdf\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "import s3fs\n",
    "import roman_datamodels as rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = mast_data></a>\n",
    "# 1. Accessing MAST Data\n",
    "In this section, we will go through the steps to retrieve archived MAST data from the cloud, including how to query the archive and stream the files directly from the cloud.\n",
    "\n",
    "## Enabling Cloud Access\n",
    "The most important step for accessing data from the cloud is to enable `astroquery` to retrieve URIs and other relevant cloud information. Even if we are working locally and plan to download the data files (**not recommended for Roman data**), we need to use this command to copy the file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Observations.enable_cloud_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying MAST\n",
    "We are ready to begin our query. This example is rather simple, but it is quick and easy to reproduce. We will be querying HST WFC3/IR data of [M85](https://science.nasa.gov/mission/hubble/science/explore-the-night-sky/hubble-messier-catalog/messier-85/). In practice, the science platform should primarily be used for analyzing and exploring Roman data products. However, due to the smaller file sizes, HST WFC3/IR data provides a nice example. The process is identical regardless of which space telescope is used.\n",
    "\n",
    "In our query, we specify that we want to look at HST data using the `F160W` filter and WFC3/IR. We also specify the `proposal_id` to easily get the data of interest. Once we get the desired observations, we gather the list of products that go into the observations. We then filter the products to gather all the Level 3 science data products associated with a specific project which still leaves us with sixty data products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query MAST for matching observations\n",
    "obs = Observations.query_criteria(obs_collection='HST',\n",
    "                                  filters='F160W',\n",
    "                                  instrument_name='WFC3/IR',\n",
    "                                  proposal_id=['11360'],\n",
    "                                  dataRights='PUBLIC')\n",
    "\n",
    "# Get the list of products (files)\n",
    "products = Observations.get_product_list(obs)\n",
    "\n",
    "# Filter the products\n",
    "filtered = Observations.filter_products(products,\n",
    "                                        calib_level=[3], \n",
    "                                        productType=['SCIENCE'], \n",
    "                                        dataproduct_type=['image'], \n",
    "                                        project=['CALWF3'])\n",
    "\n",
    "print(f'Filtered data products:\\n{filtered}\\n')\n",
    "\n",
    "# Filter for just one product\n",
    "single =  Observations.filter_products(filtered,\n",
    "                                       obsID='24797441')\n",
    "\n",
    "print(f'Single data product:\\n{single}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our desired products, we can gather the URIs for each of the files which indicate their locations in the MAST AWS Simple Storage Service servers.\n",
    "\n",
    "Amazon Simple Storage Service (S3) is a scalable and cost-effective object storage service on the AWS cloud platform. Storage containers within S3 are known as \"buckets,\" so we often refer to these storage devices as \"S3 buckets\" or \"S3 servers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = Observations.get_cloud_uris(filtered)\n",
    "uris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_cloud_uris` method checks for duplicates in the provided products to minimize the data access volume. It is also important to note that `get_cloud_uris` will always return a list. Thus, we need to extract an individual URI string to access the file. Here we choose the first URI, but in practice, you would select the URI associated with the desired file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = uris[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming files directly into memory\n",
    "Here, we will use `fsspec` to directly access the data stored in the AWS S3 servers. Because the URI points to a `FITS` file, we can use `fits.open` to access the information in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(uri, 'readonly', fsspec_kwargs={\"anon\":True}) as HDUlist:\n",
    "    HDUlist.info()\n",
    "    sci = HDUlist[1].data\n",
    "    \n",
    "type(sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id = s3_bucket></a>\n",
    "# 2. Streaming from the Roman Science Platform S3 Bucket\n",
    "\n",
    "Though Roman data will eventually be available through MAST, we currently offer a small set of simulated data available in a separate S3 bucket. These files can be streamed in the exact same way as the HST `FITS` file above. Additionally, we can browse the available files similarly to a Unix terminal. A full list of commands can be found in the [`s3fs` documentation](https://s3fs.readthedocs.io/en/latest/api.html#).\n",
    "\n",
    "The S3 bucket containing the data is currently only open to the public on the science platform where we have managed the permissions so none need to be specified explicitly. Because of the required permissions, many of the below cells will not work on a private computer.\n",
    "\n",
    "There are currently three different data sources within the Roman science platform. We can view them by performing a list command (`ls`) on the the main science platform directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "asdf_dir_uri = 's3://roman-sci-test-data-prod-summer-beta-test/'\n",
    "fs.ls(asdf_dir_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fs.ls()` command allows us to list the contents of the URI. In the above example, the `roman-sci-test-data-prod-summer-beta-test` S3 bucket contains three directories:\n",
    "\n",
    "- `ROMANISIM` contains the simulated WFI-imaging mode Roman Space Telescope data used in this suite of notebooks. See the [`ROMANISM` notebook tutorial](https://github.com/spacetelescope/roman_notebooks/tree/main/content/notebooks/romanisim) in this repo for more information.\n",
    "\n",
    "- `STIPS` contains data for the [Space Telescope Image Product Simulator (STIPS) notebook](https://github.com/spacetelescope/roman_notebooks/tree/main/content/notebooks/stips).\n",
    "\n",
    "- `OPEN_UNIVERSE` contains data from the OpenUniverse 2024 Matched Rubin and Roman Simulation preview provided by NASA/IPAC Infrared Science Archive (IRSA) at Caltech. \n",
    "\n",
    "In the next subsection we will explore opening data files made using Roman I-Sim, which are stored in the `ROMANISIM` S3 directory. These simulations are saved in the same file formats as observed Roman data will be and thus are useful to help develop file ingestion pipelines. Unfortunately, Roman I-Sim has not been used to extensively simulate survey data. \n",
    "\n",
    "In the final subsection, we will explore how to open the OpenUniverse preview data (in the `OPEN_UNIVERSE` S3 directory). The OpenUniverse collaboration has simulated extensive datasets from two core community surveys: the High Latitude Time Domain and Wide Area Surveys (HLTDS and HLWAS). Though they have only provided a preview of the full simulation suite, the quantity of data is still sufficient to start creating data pipelines to analyze Roman data.\n",
    "\n",
    "A full description of the provided data products and simulation methodologies can be found in the two linked Monthly Notices of the Royal Astronomical Society (MRNAS) papers in [Additional Resources](#additional_res) below, and an overview is provided in [Simulated Data Products](https://github.com/spacetelescope/roman_notebooks/blob/main/markdown/simulated-data.md).\n",
    "\n",
    "## Opening Roman I-Sim Models\n",
    "\n",
    "Diving into the `ROMANISIM` directory, we find three folders:\n",
    "\n",
    "- `CATALOGS_SCRIPTS`: contains stellar and galactic catalogs used to create the simulated data stored in the other directories\n",
    "  \n",
    "- `DENSE_REGION`: contains calibrated and uncalibrated simulated data of dense stellar fields obtained with different filters for all the eighteen WFI detectors. The data are separated into two directories, each with a different pointing. Filenames in these directories use the prefixes `r0000101001001001001*` and `r0000101001001001002*`, which correspond to the use of the `F158` and `F129` optical elements respectively.\n",
    "  \n",
    "- `GALAXIES`: contains one calibrated, simulated image of a galaxy field obtained using the `F158` optical element.\n",
    "\n",
    "Below, we use `roman_datamodels` to read the `ASDF` file corresponding to the dense region as an example. To simplify the workflow we are providing a URI to the sample Roman data. During operations, the data would be referenced using the URI when performing queries through MAST or other data access methods that are currently under development.\n",
    "\n",
    "The file naming convention for Roman is quite elaborate as each includes all the relevant information about the observation. Please see the [Data Levels and Products](https://roman-docs.stsci.edu/data-handbook-home/wfi-data-format/data-levels-and-products) Roman documentation page for more information on the file naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf_file_uri =  f'{asdf_dir_uri}ROMANISIM/DENSE_REGION/R0.5_DP0.5_PA0/r0000101001001001001_01101_0001_WFI01_cal.asdf'\n",
    "\n",
    "with fs.open(asdf_file_uri, 'rb') as f:\n",
    "    dm = rdm.open(f)\n",
    "    \n",
    "print(dm.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening OpenUniverse Models\n",
    "\n",
    "The subset of data that IPAC has shared is hosted in their own S3 bucket, detailed on the [OpenUniverse AWS Open Data](https://registry.opendata.aws/openuniverse2024/) website. Additionally, IPAC has created two [OpenUniverse notebooks](https://irsa.ipac.caltech.edu/docs/notebooks/) that highlight how you can interact with their image data and catalog files. In this notebook, we focus on how to access the files and leave the linked notebooks as resources for the user to explore.\n",
    "\n",
    "The simulations are natively saved as `FITS` files and are divided by survey, optical element, and HEALPix cell. [HEALPix](https://healpix.sourceforge.io) is a commonly used way to discretize the area of a sphere uniformly. Please see [Simulated Data Products](https://github.com/spacetelescope/roman_notebooks/blob/main/markdown/simulated-data.md) for more information about the specific products provided in the Open Universe data.\n",
    "\n",
    "Below we provide an example of streaming a simulated \"calibrated\" image `FITS` file from their S3 bucket using an alternate way of streaming a `FITS` file. Instead of initializing our own `S3FileSystem`, we pass the credentials (anonymous credentials in this case as the data is public) to `fits.open` and allow it to create the file system. This shorthand is convenient when the URI is specifically provided, but it is impossible to explore the S3 directory structure without initializing the `S3FileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3bucket = 's3://nasa-irsa-simulations/openuniverse2024/roman/preview/RomanWAS/images/simple_model'\n",
    "band = 'F184'\n",
    "hpix = '15297'\n",
    "sensor = 11\n",
    "s3fpath = f'{s3bucket}/{band}/{hpix}/Roman_WAS_simple_model_{band}_{hpix}_{sensor}.fits.gz'\n",
    "\n",
    "fits_file = fits.open(s3fpath, fsspec_kwargs={'anon':True})\n",
    "print(fits_file.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we have converted all the simulated \"calibrated\" images from `FITS` to `ASDF` files and are hosting them on the science platform's S3 bucket. In addition to the original files' data, we have also included two new features to the `ASDF` file:\n",
    "\n",
    "1. We unpacked the WCS information from the `FITS` metadata and created a `gwcs.WCS` object that is saved in `asdf_file['roman']['wcs']`.\n",
    "\n",
    "2. We queried the provided source catalogs and included all point sources, galaxies, and transients within the detector's field of view, storing them in `astropy.table.Table` objects directly within the `ASDF` files.\n",
    "\n",
    "Below is an example of accessing the same file that we opened with the `FITS` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3bucket = 's3://roman-sci-test-data-prod-summer-beta-test/OPEN_UNIVERSE/WAS/simple_model'\n",
    "band = 'F184'\n",
    "hpix = '15297'\n",
    "sensor = 11\n",
    "s3fpath = s3bucket+f'/{band}/{hpix}/roman_was_{band}_{hpix}_wfi{sensor:02d}_simple.asdf'\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "with fs.open(s3fpath, 'rb') as file_path:\n",
    "    asdf_file = asdf.open(file_path)\n",
    "\n",
    "print(asdf_file.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference when printing the file information between `FITS` and `ASDF`. `ASDF` provides more detail about the contents in a hierarchical structure to `FITS` native printing. Additionally we can index the `asdf_file` object similarly to a Python dictionary to access the contents.\n",
    "\n",
    "Below we print the pre-prepared source catalog of galaxies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asdf_file['roman']['catalogs']['galaxies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded Roman data into a datamodel, please review the [Working with ASDF Notebook](https://github.com/spacetelescope/roman_notebooks/tree/main/content/notebooks/working_with_asdf) notebook to explore how to use them.\n",
    "***\n",
    "<a id = download></a>\n",
    "# 3. Downloading Files (**not recommended**)\n",
    "\n",
    "It is **not recommended** for users to download Roman data products due to the large file size and the number of files that are expected from the survey nature of the mission. Instead, users are encouraged to construct and adopt workflows that utilize the file streaming services described above for the best experience.\n",
    "\n",
    "However, there may be instances where data files must be downloaded for certain specific science cases. To do that, we can use the URIs and the `S3FileSystem.get` function (documentation [here](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem.get)). Running the below cell will download the data to your personal instance of the science platform. However, the preliminary, simulated sample of Roman data on the science platform are currently not accessible outside of the science platform.\n",
    "\n",
    "**NOTE**: MAST data can be downloaded on your private computer using `anon=True` in the `S3FileSystem` initialization. However, the preliminary, simulated sample of Roman data on the science platform are currently not accessible outside of the science platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out as this use case is not recommended and should only be needed in rare circumstances\n",
    "# from pathlib import Path\n",
    "# URI =  ## Set this to the URI string you want to download.\n",
    "# local_file_path = Path('data/')\n",
    "# local_file_path.mkdir(parents=True, exist_ok=True)\n",
    "# fs = s3fs.S3FileSystem()\n",
    "# fs.get(URI, local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id = \"additional_res\"></a>\n",
    "## Additional Resources\n",
    "Additional information can be found at the following links:\n",
    "\n",
    "- [`s3fs` Documentation](https://s3fs.readthedocs.io/en/latest/api.html#)\n",
    "\n",
    "- [OpenUniverse AWS Open Data](https://registry.opendata.aws/openuniverse2024/)\n",
    "\n",
    "- [OpenUniverse notebooks](https://irsa.ipac.caltech.edu/docs/notebooks/)\n",
    "\n",
    "- [Simulated Data Products Document](../../../markdown/simulated-data.md)\n",
    "\n",
    "- [MNRAS paper detailing Open Universe data simulation methods (Troxel et al 2021)](https://ui.adsabs.harvard.edu/abs/2021MNRAS.501.2044T/abstract)\n",
    "\n",
    "- [MNRAS paper detailing the previewed Open Universe data (Troxel et al 2023)](https://ui.adsabs.harvard.edu/abs/2023MNRAS.522.2801T/abstract)\n",
    "\n",
    "\n",
    "**For additional support, please contact the [Roman Help Desk at STScI](https://stsci.service-now.com/roman).**\n",
    "\n",
    "## About this notebook\n",
    "The data streaming information from this notebook largely builds off of the TIKE data-access notebook by Thomas Dutkiewicz.\n",
    "\n",
    "**Author:** Will C. Schultz\n",
    "\n",
    "**Updated On:** 2024-11-06\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roman_science_platform24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
