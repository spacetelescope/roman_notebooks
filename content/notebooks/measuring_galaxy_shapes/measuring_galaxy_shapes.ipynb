{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring Galaxy Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Information and Read-Only Status\n",
    "\n",
    "To run this notebook, please select the \"Roman Calibration\" kernel at the top right of your window.\n",
    "\n",
    "This notebook is read-only. You can run cells and make edits, but you must save changes to a different location. We recommend saving the notebook within your home directory, or to a new folder within your home (e.g. <span style=\"font-variant:small-caps;\">file > save notebook as > my-nbs/nb.ipynb</span>). Note that a directory must exist before you attempt to add a notebook to it.\n",
    "\n",
    "## Imports\n",
    "Below we list the libraries we'll be using in the tutorial:\n",
    "- *astropy.coordinates* to perform sky-to-detector coordinate transformations\n",
    "- *astropy.stats* to estimate the sky background level\n",
    "- *astropy.table* for manipulating tabular data\n",
    "- *astropy.units* for working with units\n",
    "- *astropy.visualization* for normalizing images for plotting\n",
    "- *coord* for working with angles\n",
    "- *matplotlib.pyplot* for plotting and visualizing data\n",
    "- *galsim* to measure source moments and generate Sérsic profiles\n",
    "- *numpy* for array manipulation and mathematical operations\n",
    "- *roman_datamodels* for opening WFI ASDF files\n",
    "- *asdf* for opening WFI ASDF files\n",
    "- *s3fs* for streaming simulated WFI images from an S3 bucket\n",
    "- *scipy.optimize* to fit our data\n",
    "- *synphot* for synthetic photometry\n",
    "- *stsynphot* to access WFI throughput information\n",
    "- *stpsf* to access WFI point spread functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "from astropy.visualization import simple_norm\n",
    "import coord\n",
    "import matplotlib.pyplot as plt\n",
    "import galsim\n",
    "from galsim.roman import collecting_area\n",
    "import numpy as np\n",
    "import roman_datamodels as rdm\n",
    "import asdf\n",
    "import s3fs\n",
    "from scipy.optimize import curve_fit\n",
    "import synphot as syn\n",
    "import stsynphot as stsyn\n",
    "import stpsf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "The main goal of this notebook is to illustrate a typical use case of Roman images: performing shape measurements of astronomical sources. \n",
    "\n",
    "We are going to perform two sets of measurements. First, we rely on [GalSim](https://galsim-developers.github.io/) to perform ellipticity measurements. In particular, we use the\n",
    "[REGAUSS method in its HSM module](https://galsim-developers.github.io/GalSim/_build/html/hsm.html) ([Hirata & Seljak 2003](https://ui.adsabs.harvard.edu/abs/2003MNRAS.343..459H/abstract), [Mandelbaum et al. 2005](https://ui.adsabs.harvard.edu/abs/2005MNRAS.361.1287M/abstract)). Second, we fit a Sérsic model to a galaxy cutout.\n",
    "\n",
    "In this notebook, we will be building on previous tutorials. We recommend consulting the following tutorials prior to this one:\n",
    "- Data Access and Discovery\n",
    "- Working with ASDF\n",
    "- Data Visualization\n",
    "- STPSF\n",
    "- Synphot\n",
    "\n",
    "### Defining Terms\n",
    "\n",
    "- ASDF: Advanced Scientific Data Format — the data format for the Roman Wide Field Instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Data\n",
    "The first step of the analysis is to read the Roman WFI image data, which are stored in ASDF format. For this example, we start with a calibrated Level 2 (L2) simulated image created with [Roman I-Sim](https://romanisim.readthedocs.io). For more information about Roman's data products check the [Roman User Documentation](roman-docs.stsci.edu). We use `roman_datamodels` to open the simulated image, which we stream into memory from an S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Open an image from an S3 bucket\n",
    "asdf_dir_uri = 's3://roman-sci-test-data-prod-summer-beta-test/'\n",
    "fs = s3fs.S3FileSystem()\n",
    "asdf_file_uri_l2 = asdf_dir_uri + 'AAS_WORKSHOP/r0003201001001001004_0001_wfi01_f106_cal.asdf'\n",
    "\n",
    "with fs.open(asdf_file_uri_l2, 'rb') as f:\n",
    "    af = asdf.open(f)\n",
    "    file = rdm.open(af).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To characterize the shape of our sources, we need to get cutouts of individual targets, and perform our measurements. We choose to use the input catalog from the simulation software to define our cutouts. In a more realistic scenario, we would run a source detection algorithm to identify the locations of the sources.\n",
    "\n",
    "We start by saving the image array in the `image` variable. Remember that the `.data` datablock in WFI ASDF files contains the data array, and this is an `numpy.ndarray` object with values of data numbers (DN) per second (DN/s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data array as an numpy.ndarray object\n",
    "image = file.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the input catalog using `astropy.table.Table()`. More details about the format of these catalogs are available [here](https://romanisim.readthedocs.io/en/latest/romanisim/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the catalog used for the simulation as an astropy.table.Table\n",
    "cat_uri = asdf_dir_uri + 'AAS_WORKSHOP/full_catalog.ecsv'\n",
    "with fs.open(cat_uri, 'rb') as catalog_file_stream:\n",
    "    catalog = Table.read(cat_uri, format='ascii.ecsv').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick sanity check we plot the positions of 1% of the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of the positions of a subselection of catalog entries.\n",
    "plt.scatter(catalog['ra'][::100], catalog['dec'][::100], s=0.1)\n",
    "plt.xlabel('RA [deg]', fontsize=16)\n",
    "plt.ylabel('Dec [deg]', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Cutouts and Retrieving the PSF\n",
    "\n",
    "The positions are in sky coordinates. In order to make the cutouts we need the pixel coordinates in the image. To do that transformation, we will use the generalized World Coordinate System (GWCS) object in the simulated file's metadata.\n",
    "For more information about GWCS please check the documentation [here](https://gwcs.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the gwcs object from the ASDF file into a variable\n",
    "wcs = file.meta.wcs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the GWCS object to transform from the catalog right ascension (RA) and declination (DEC) coordinates into pixel positions within the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the catalog coordinates in a SkyCoord object and \n",
    "# convert to pixel positions.\n",
    "coords = SkyCoord(ra=catalog['ra'] * u.deg, dec=catalog['dec'] * u.deg)\n",
    "y, x = wcs.world_to_array_index_values(coords)\n",
    "\n",
    "# Save the x and y coordinates in the catalog table for later.\n",
    "catalog['x'] = x\n",
    "catalog['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, we focus on relatively bright ($18 < {m}_\\mathrm{AB} < 19$) galaxies (marked as type = SER in the catalog). First, we need to know the optical element used for the observation, which we can get from the file metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = file.meta.instrument.optical_element\n",
    "print(f'The simulated bandpass is {band}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's trim our catalog based on our magnitude cuts and selection of extended sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set magnitude limits.\n",
    "mag_hi = 19\n",
    "mag_lo = 18\n",
    "\n",
    "# Compute the AB magnitude of the F106 filter in the catalog.\n",
    "mag_ab = -2.5 * np.log10(catalog['F106'])\n",
    "\n",
    "# Select sources within our magnitude limits and that are extended.\n",
    "bright = (mag_ab > mag_lo) & (mag_ab < mag_hi) & (catalog['type'] == 'SER')\n",
    "\n",
    "# The input catalog may contain sources that are not on the WFI detector.\n",
    "# Identify where sources are on the detector. The padding variable will\n",
    "# ensure that sources are not too close to the edge (and possibly cut off).\n",
    "padding = 10\n",
    "inchip = (x > padding) & (x < image.shape[0] - padding) & (y > padding) & (y < image.shape[1] - padding)\n",
    "\n",
    "# Our final selection is the intersection of these two lists of indices.\n",
    "# The variable gal_tab contains a subselection of the original catalog\n",
    "# that meets our selection criteria.\n",
    "galaxies = (bright) & (inchip)\n",
    "gal_tab = catalog[galaxies]\n",
    "\n",
    "print(np.count_nonzero(galaxies), 'galaxies pass these selection criteria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most morphological measurements require high-accuracy knowledge of the PSF. In the case of our simulated scene, we know that the PSF was generated via `stpsf`. For simplicity, we will be working with a single PSF, but users interested in high accuracy fits will likely want to generate a PSF at the pixel location of each of the sources being fit. See the [STPSF tutorial](../stpsf/stpsf.ipynb) tutorial for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the WFI instrument object in STPSF, set the optical element, and generate a PSF.\n",
    "wfi = stpsf.WFI()\n",
    "wfi.filter = band\n",
    "psf = wfi.calc_psf(fov_pixels=64)\n",
    "\n",
    "# Generate a galsim image object of the simulated PSF. We specify the pixel scale for which\n",
    "# the WFI has 0.11 arcsecond pixels, and the PSF from WebbPSF is oversampled by a factor of 4.\n",
    "# Save the PSF as an interpolated image so we can convolve and deconvolve later on.\n",
    "psf_img = galsim.Image(psf['OVERSAMP'].data, scale=0.11 / 4)\n",
    "psf_obj = galsim.InterpolatedImage(psf_img, flux=1)  \n",
    "\n",
    "# Show the simulated PSF.\n",
    "plt.imshow(psf_img.array, norm=simple_norm(psf_img.array, 'log', percent=95), origin='lower');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the PSF is generated for detector WFI01 at (x, y) = (2048, 2048) in the science coordinate system. See the [RDox article on WFI coordinate systems]( https://roman-docs.stsci.edu/data-handbook-home/wfi-data-format/coordinate-systems) for more information.\n",
    "\n",
    "These attributes can be confirmed and modified via the `wfi.detector` and `wfi.detector_position` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'WebbPSF detector = {wfi.detector}')\n",
    "print(f'WebbPSF position = {wfi.detector_position}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before creating a cutout, we need to estimate the sky background level, since the WFI L2 images are not background subtracted. For our example we'll use the sigma-clipped median of the image. While this background estimate is simplistic, it is effective for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sigma-clipped median as an estimate of the sky background\n",
    "_, med_bkg, _ = sigma_clipped_stats(image)\n",
    "print(f'Median background = {med_bkg:0.5f} DN / s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a cutout of our galaxy by creating a small postage stamp centered on the x and y coordinates we previously computed from the input simulation catalog. We will adapt the size of the postage stamp to the half-light radius of the galaxy from the input simulation catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to index our table of galaxies. Changing this variable will\n",
    "# change the galaxy that we are fitting.\n",
    "igal = 2\n",
    "\n",
    "# Get the dimensions of cutout based on the half-light radius. The scale variable\n",
    "# will set the multiples of the half-light radius used for the cutout. \n",
    "scale = 4\n",
    "\n",
    "# Multiply the half-light radius in arcseconds by the scale factor, and then\n",
    "# divide by the pixel scale of 0.11 arcseconds per pixel to get the half-light\n",
    "# radius in units of pixels. Add 1 pixel for padding so that small galaxies have\n",
    "# a sufficiently sized cutout.\n",
    "size = int((gal_tab['half_light_radius'][igal] * 4 / 0.11) + 1)\n",
    "\n",
    "# Determine the x and y positions at the corners of our cutout.\n",
    "xmin = int(gal_tab['x'][igal] - size//2)\n",
    "xmax = xmin + size + 1\n",
    "ymin = int(gal_tab['y'][igal] - size//2)\n",
    "ymax = ymin + size + 1\n",
    "\n",
    "# Make the cutout as a galsim Image object. Also subtract the sky background estimate,\n",
    "# and set the pixel scale.\n",
    "cutout = galsim.Image(image[ymin:ymax, xmin:xmax] - med_bkg, scale=0.11)\n",
    "\n",
    "# Display the cutout with and without the background subtraction.\n",
    "# Set the image normalization. To compare the cutout before and after\n",
    "# background subtraction, we manually set the limits.\n",
    "norm = simple_norm(cutout.array + med_bkg, 'linear', vmin= -med_bkg, vmax=np.max(cutout.array))\n",
    "\n",
    "# Setup the plot area.\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "# Original cutout (add the sky background back in)\n",
    "cut = axs[0].imshow(cutout.array + med_bkg, origin='lower', norm=norm)\n",
    "axs[0].set_title('Cutout')\n",
    "axs[0].set_xlabel('X [pix]')\n",
    "axs[0].set_ylabel('Y [pix]')\n",
    "\n",
    "# Background-subtracted cutout\n",
    "bkg_cut = axs[1].imshow(cutout.array, origin='lower', norm=norm)\n",
    "axs[1].set_title('Bkgrnd Sub')\n",
    "axs[1].set_xlabel('X [pix]')\n",
    "axs[1].set_ylabel('Y [pix]')\n",
    "\n",
    "# Assign color bars. The color bars and normalization are the same\n",
    "# for both cutouts.\n",
    "fig.colorbar(cut, ax=axs[0], label='DN / s')\n",
    "fig.colorbar(bkg_cut, ax=axs[1], label='DN / s')\n",
    "plt.tight_layout(h_pad=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the background estimate seems to be reasonable. We could improve it by performing a local background estimate rather than using the whole image. For on-orbit observations, a 2-D local background model would likely provide the highest fidelity background estimate. Note that estimating the background should be treated with care, since it will affect the fitting results. \n",
    "\n",
    "## Estimating the Source Moments\n",
    "\n",
    "Now that we have our cutout saved as a `galsim.Image` object we can just use the `hsm` module to estimate the moments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = galsim.hsm.FindAdaptiveMom(cutout, strict=False)\n",
    "shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` variable contains the source moments estimated from the best-fit elliptical Gaussian. For example, we can see in the printed result that the `.observed_shape` attribute contains a `galsim.Shear` object. From that object, we can retrieve shape parameters such as the position angle (beta) and the minor-to-major axis ratio (q):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the position angle (beta) and wrap it to the range [0, 2pi) radians and convert to degrees\n",
    "beta = shape.observed_shape.beta.wrap(center = 180 * coord.degrees).deg\n",
    "\n",
    "# Get the minor-to-major axis ratio (q)\n",
    "q = shape.observed_shape.q\n",
    "\n",
    "print(f'position angle: {beta:.5f} deg')\n",
    "print(f'minor-to-major axis ratio: {q:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the PSF model we can also try to estimate the PSF-corrected galaxy shear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape2 = galsim.hsm.EstimateShear(cutout, psf_img, strict=False)\n",
    "shape2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once again extract information about the position angle and minor-to-major axis ratio as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = shape2.observed_shape.beta.wrap(center = 180 * coord.degrees).deg\n",
    "q = shape2.observed_shape.q\n",
    "print(f'position angle: {beta:.5f} deg')\n",
    "print(f'minor-to-major axis ratio: {q:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that the values are the same with and without the PSF-correction.\n",
    "\n",
    "## Fitting a Sérsic Profile to a Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another typical morphological analysis consists of fitting a source with a [Sérsic profile](https://en.wikipedia.org/wiki/S%C3%A9rsic_profile) ([Sérsic 1963](https://ui.adsabs.harvard.edu/abs/1963BAAA....6...41S/abstract)). We are going to rely on the Sérsic implementation in the `galsim` package.\n",
    "\n",
    "In our example we will ignore the pixel mask (data quality array), but will use the error image.\n",
    "\n",
    "First, we need to convert our source flux units to photons / s / cm$^2$. The catalog fluxes are stored as maggies, which are linear flux units such that:\n",
    "\n",
    "$m_\\mathrm{AB} = -2.5\\times\\log_{10}(f),$ \n",
    "\n",
    "where $m_\\mathrm{AB}$ is the AB magnitude and $f$ is the flux in maggies. As Roman I-Sim, which was used to generate the catalog and simulate the image, does not create sources with realistic colors, let's make a simplifying assumption that our source has a spectral energy distribution (SED) that is flat in frequency space, at least over the bandpass of interest (F106). We will use the `synphot` package to synthetically observe the source spectrum, normalized to the AB magnitude from the catalog, and integrate over the bandpass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AB magnitude of the source.\n",
    "abmag = -2.5 * np.log10(gal_tab['F106'][igal])\n",
    "\n",
    "# Create the spectrum and bandpass objects, and then observe the spectrum\n",
    "# through the bandpass.\n",
    "spec = syn.SourceSpectrum(syn.models.ConstFlux1D, amplitude=abmag * u.ABmag)\n",
    "band = stsyn.band('roman,wfi,f106')\n",
    "obs = syn.Observation(spec, band)\n",
    "\n",
    "# Integrate over the bandpass. By default, calls to the obs variable, which\n",
    "# stores a synphot Observation object, will return units of photlam, which\n",
    "# is shorthand for photons / s / cm^2 / Angstrom. The binset attribute contains\n",
    "# the default wavelengths used in the Observation object, which have units of\n",
    "# Angstroms. Thus, the integral of the Observation over the wavelengths will\n",
    "# give units of photons / s / cm^2.\n",
    "waves = obs.binset\n",
    "flux = np.trapezoid(obs(waves), x=waves).value\n",
    "\n",
    "print(f'Source flux: {flux:.5f} photon / s / cm^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have instead determined a scaling factor to apply to any input flux from the catalog by normalizing the spectrum to an amplitude of 1 maggie. Synphot does not allow us to use the `astropy.units.maggy` unit, but we can just as effectively use the definition of AB magnitudes to write this in units of Jankys:\n",
    "\n",
    "$m_\\mathrm{AB} = -2.5 \\times log_{10}(f_\\mathrm{Jy}) + 8.90,$\n",
    "\n",
    "and thus when $m_\\mathrm{AB} = 0$ mag, which is defined when $f = 1$ maggie, we find that $f_\\mathrm{Jy} = 10^{(8.90 / 2.5)} \\approx 3630.78055~\\mathrm{Jy}$. Determining such a scaling factor would be useful if we wanted to perform our shape analysis at scale.\n",
    "\n",
    "Then we create our model as a callable function for later fitting. The variables are:\n",
    "* n = Sérsic index\n",
    "* hlr = half-light radius in arcseconds\n",
    "* pa = position angle in degrees\n",
    "* x0 = shift in x pixels to center of the source\n",
    "* y0 = shift in y pixels to center of the source\n",
    "* q = minor-to-major axis ratio\n",
    "\n",
    "We will set the gain and area options in the `drawImage()` method on the `galsim.Sersic` object we create. This ensures that the model of our galaxy can return pixels with values like our observed image that has units of DN / s. We set the gain to 1.8, which is a reasonable value for a general WFI detector, and we use the obscured collecting area from the `galsim.roman` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sersic_mod(cutout, n, hlr, influx, pa, x0, y0, q):\n",
    "    nx, ny = cutout.array.shape\n",
    "    ser = galsim.Sersic(n, half_light_radius=hlr, flux=influx)\n",
    "    ser = ser.shear(q=q, beta=pa * galsim.degrees)  # change PA and axis ratio\n",
    "    offset = galsim.PositionD(x=x0, y=y0)  # potentially shift a bit the profile\n",
    "    ser = galsim.convolve.Convolve(ser, psf_obj)  # add PSF\n",
    "    img = galsim.ImageD(nx, ny, scale=0.11)  \n",
    "    ser.drawImage(image=img, offset=offset, scale=0.11, gain=1.8, area=galsim.roman.collecting_area)\n",
    "    \n",
    "    return img.array.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the model with the truth parameters. The Sérsic index, half-light radius, position angle, and minor-to-major axis ratio come from the input simulation catalog. The flux is the value computed above in the correct units of photons / s / cm$^2$. The x0 and y0 shifts are assumed to both be 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the truth paramters.\n",
    "p0 = [gal_tab['n'][igal], gal_tab['half_light_radius'][igal], flux, gal_tab['pa'][igal], 0., 0., gal_tab['ba'][igal]]\n",
    "\n",
    "# Create the model with our truth parameters\n",
    "fid_model = sersic_mod(cutout, *p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the syntax `*p0` above. This is the same as explicitly writing out `p0[0], p0[1], ..., p0[6]` and is a convenient way in Python to unpack all of the values in a list in order as inputs to a function's positional arguments.\n",
    "\n",
    "We also obtain the error array in the location of the cutout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_img = file.err[ymin:ymax, xmin:xmax].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model using `scipy.curve_fit()`. We add bounds to the parameter space in order avoid software issues and unphysical values — GalSim only supports $0.3 < n < 6.2$.\n",
    "\n",
    "**NOTE:** If the following code cell produces an exception, you may have to adjust the bounds of the fit. This can happen, for example, if you changed any of the filtering we did above on the source catalog (e.g., a different magnitude range cut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout, pcov = curve_fit(sersic_mod, cutout, cutout.array.flatten(), sigma=err_img,\n",
    "          p0=p0, bounds=([0.3, 0.01, 1e-11, 0, -size/2, -size/2, 0], [6.2, 4.0, 0.1, 360, size/2, size/2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the resulting best-fit parameters of the fit and compare to our initial input model parameters (the `p0` variable above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]}:')\n",
    "    print(f'\\tInitial: {p0[i]:.5f}')\n",
    "    print(f'\\tFitted: {pout[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the input and best-fit parameters, we see good agreement.\n",
    "\n",
    "Let's create an image of our best-fit model and visualize the original cutout, initial input model, best-fit output model, and fit residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an image of the best-fit model and compute the fit residuals.\n",
    "model_out = sersic_mod(cutout, *pout)\n",
    "residual = cutout.array - model_out.reshape(cutout.array.shape)\n",
    "\n",
    "# Plot the original cutout, initial input model, best-fit model, and fit residuals.\n",
    "# Plot all images with the same normalization for comparison. As we may have negative\n",
    "# residual values, we use a linear normalization.\n",
    "norm = simple_norm(cutout.array, 'linear', vmin= -med_bkg, vmax=np.max(cutout.array))\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "# Original image\n",
    "orig = axs[0][0].imshow(cutout.array, norm=norm, origin='lower')\n",
    "axs[0][0].set_title('Image Cutout')\n",
    "\n",
    "# Fiducial (initial input) model\n",
    "axs[0][1].imshow(fid_model.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[0][1].set_title('Init Model')\n",
    "\n",
    "# Best-fitting model\n",
    "axs[1][0].imshow(model_out.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[1][0].set_title('Best-Fit')\n",
    "\n",
    "# Fit residuals\n",
    "axs[1][1].imshow(residual, norm=norm, origin='lower')\n",
    "axs[1][1].set_title('Fit Residuals')\n",
    "\n",
    "# Set the colorbar\n",
    "fig.colorbar(orig, ax=axs[0][0])\n",
    "fig.colorbar(orig, ax=axs[0][1])\n",
    "fig.colorbar(orig, ax=axs[1][0])\n",
    "fig.colorbar(orig, ax=axs[1][1])\n",
    "\n",
    "# Other plot settings to clean it up.\n",
    "plt.tight_layout(h_pad=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fit looks really good. Let's take a quick look at the statistics of the fit residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Residual median: {np.median(residual):.5f} DN / s')\n",
    "print(f'Residual std dev: {np.std(residual):.5f} DN / s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We benefitted greatly from knowing a lot of these parameters from the simulated input catalog used to generate the image. What if we didn't know these values so well? Would we have gotten such a good fit result? Let's test that now by setting the fiducial model to something less certain and fit it again. \n",
    "\n",
    "We'll set our `p1` variable to contain our new input parameters. Recall the input parameter order is Sérsic index, half-light radius in arcseconds, flux in photons / s / cm$^2$, position angle in degrees, x shift, y shift, and minor-to-major axis ratio. Let's assume we can measure the flux well, and that we estimate the half-light radius to be approximately 4 WFI pixels, or 0.44 arcseconds. We'll set the other values to uninformed guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [1., 0.44, flux, 0., 0., 0., 0.5]\n",
    "\n",
    "# Create the model with our truth parameters\n",
    "fid_model_new = sersic_mod(cutout, *p1)\n",
    "\n",
    "pout_new, pcov_new = curve_fit(sersic_mod, cutout, cutout.array.flatten(), sigma=err_img,\n",
    "          p0=p1, bounds=([0.3, 0.01, 1e-11, 0, -size/2, -size/2, 0], [6.2, 4.0, 0.1, 360, size/2, size/2, 1]))\n",
    "\n",
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]}:')\n",
    "    print(f'\\tCatalog (p0): {p0[i]:.5f}')\n",
    "    print(f'\\tNew inputs (p1): {p1[i]:.5f}')\n",
    "    print('')\n",
    "    print(f'\\tOld fit: {pout[i]:.5f}')\n",
    "    print(f'\\tNew fit: {pout_new[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best-fitting model is quite similar to the previous one with the exceptions of the flux, minor-to-major axis ratio, and the position angle. In fact, on closer examination, we can see that the position angle and minor-to-major axis ratio in our earlier fit is identical to the input values we obtained from the catalog.\n",
    "\n",
    "Let's try again, but this time let's use the shear estimate shape measurements we previously performed (recall that beta and q are the position angle and minor-to-major axis ratio, respectively, from the PSF-corrected shear estimate in the `shape2` variable above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [1., 0.44, flux, beta, 0., 0., q]\n",
    "\n",
    "# Create the model with our truth parameters\n",
    "fid_model_new = sersic_mod(cutout, *p1)\n",
    "\n",
    "pout_new, pcov_new = curve_fit(sersic_mod, cutout, cutout.array.flatten(), sigma=err_img,\n",
    "          p0=p1, bounds=([0.3, 0.01, 1e-11, 0, -size/2, -size/2, 0], [6.2, 4.0, 0.1, 360, size/2, size/2, 1]))\n",
    "\n",
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]}:')\n",
    "    print(f'\\tCatalog (p0): {p0[i]:.5f}')\n",
    "    print(f'\\tNew inputs (p1): {p1[i]:.5f}')\n",
    "    print('')\n",
    "    print(f'\\tOld fit: {pout[i]:.5f}')\n",
    "    print(f'\\tNew fit: {pout_new[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now the best-fit flux agrees well with the catalog value. We also see that we got similar values of the other best-fit parameters other than the position angle and minor-to-major axis ratio. It seems that these two parameters are not well constrained by the data using the Sérsic model fit.\n",
    "\n",
    "For completeness, let's also re-plot the cutout, models, and fit residuals with out new inputs and best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an image of the best-fit model and compute the fit residuals.\n",
    "model_out_new = sersic_mod(cutout, *pout_new)\n",
    "residual_new = cutout.array - model_out_new.reshape(cutout.array.shape)\n",
    "\n",
    "# Plot the original cutout, initial input model, best-fit model, and fit residuals.\n",
    "# Plot all images with the same normalization for comparison. As we may have negative\n",
    "# residual values, we use a linear normalization.\n",
    "norm = simple_norm(cutout.array, 'linear', vmin= -med_bkg, vmax=np.max(cutout.array))\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "# Original image\n",
    "orig = axs[0][0].imshow(cutout.array, norm=norm, origin='lower')\n",
    "axs[0][0].set_title('Image Cutout')\n",
    "\n",
    "# Fiducial (initial input) model\n",
    "axs[0][1].imshow(fid_model_new.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[0][1].set_title('Init Model')\n",
    "\n",
    "# Best-fitting model\n",
    "axs[1][0].imshow(model_out_new.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[1][0].set_title('Best-Fit')\n",
    "\n",
    "# Fit residuals\n",
    "axs[1][1].imshow(residual_new, norm=norm, origin='lower')\n",
    "axs[1][1].set_title('Fit Residuals')\n",
    "\n",
    "# Set the colorbar\n",
    "fig.colorbar(orig, ax=axs[0][0])\n",
    "fig.colorbar(orig, ax=axs[0][1])\n",
    "fig.colorbar(orig, ax=axs[1][0])\n",
    "fig.colorbar(orig, ax=axs[1][1])\n",
    "\n",
    "# Other plot settings to clean it up.\n",
    "plt.tight_layout(h_pad=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the initial model is quite different from our previous one based on inputs from the catalog, but our best-fit model and residuals still look quite good. Finally, the statistics of our residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Residual median: {np.median(residual_new):.5f} DN / s')\n",
    "print(f'Residual std dev: {np.std(residual_new):.5f} DN / s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these values with our earlier fit, we find that the median and standard deviation of the residuals of both fits are identical to within four decimal places despite differences in the input and best-fit model parameters.\n",
    "\n",
    "From `scipy.optimize.curve_fit()`, we also have the estimated approximate covariance information for each of the fitted parameters. From the `scipy` [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html), we can use that information to compute the 1-$\\sigma$ uncertainties on the fitted parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr = np.sqrt(np.diag(pcov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `pcov` is the covariance of the fit (recall that `pout` is the fitted values above). Now we can print the fitted values with the uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]} = {pout[i]:.5f} +/- {perr[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Roman User Documentation (RDox)](https://roman-docs.stsci.edu/)\n",
    "- [Roman Notebooks](https://github.com/spacetelescope/roman_datamodels)\n",
    "- [Roman I-Sim documentation](https://romanisim.readthedocs.io/)\n",
    "- [WebbPSF documentation](https://webbpsf.readthedocs.io/)\n",
    "- [GalSim HSM shape fitting documentation](https://galsim-developers.github.io/GalSim/_build/html/hsm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About This Notebook\n",
    "\n",
    "**Author:** Javier Sánchez, Amethyst Barnes, Ami Choi, Tyler Desjardins  \n",
    "**Updated On:** 2025-01-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Roman Calibration",
   "language": "python",
   "name": "roman-cal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
