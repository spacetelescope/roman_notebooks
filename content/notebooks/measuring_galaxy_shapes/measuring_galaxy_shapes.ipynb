{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring galaxy shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Information and Read-Only Status\n",
    "\n",
    "To run this notebook, please select the \"Roman Calibration\" kernel at the top right of your window.\n",
    "\n",
    "This notebook is read-only. You can run cells and make edits, but you must save changes to a different location. We recommend saving the notebook within your home directory, or to a new folder within your home (e.g. <span style=\"font-variant:small-caps;\">file > save notebook as > my-nbs/nb.ipynb</span>). Note that a directory must exist before you attempt to add a notebook to it.\n",
    "\n",
    "## Imports\n",
    "Below we list the libraries we'll be using in the tutorial:\n",
    "- *astropy.coordinates* to perform sky-to-detector coordinate transformations\n",
    "- *astropy.stats* to estimate the sky background level\n",
    "- *astropy.table* for manipulating tabular data\n",
    "- *astropy.units* for working with units\n",
    "- *astropy.visualization* for normalizing images for plotting\n",
    "- *coord* for working with angles\n",
    "- *matplotlib.pyplot* for plotting and visualizing data\n",
    "- *galsim* to measure source momens and generate Sérsic profiles\n",
    "- *numpy* for array manipulation and mathematical operations\n",
    "- *roman_datamodels* for opening WFI ASDF files\n",
    "- *s3fs* for streaming simulated WFI images from an S3 bucket\n",
    "- *scipy.optimize* to fit our data\n",
    "- *synphot* for synthetic photometry\n",
    "- *stsynphot* to access WFI throughput information\n",
    "- *webbpsf* to access WFI point spread functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "from astropy.visualization import simple_norm\n",
    "import coord\n",
    "import matplotlib.pyplot as plt\n",
    "import galsim\n",
    "from galsim.roman import collecting_area\n",
    "import numpy as np\n",
    "import roman_datamodels as rdm\n",
    "import s3fs\n",
    "from scipy.optimize import curve_fit\n",
    "import synphot as syn\n",
    "import stsynphot as stsyn\n",
    "import webbpsf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "The main goal of this notebook is to illustrate a typical use case of Roman images, which is performing shape measurements of astronomical sources. \n",
    "\n",
    "We are going to perform two sets of measurements. First, we rely on [GalSim](https://galsim-developers.github.io/) to perform ellipticity measurements. In particular, we use the\n",
    "[REGAUSS method in its HSM module](https://galsim-developers.github.io/GalSim/_build/html/hsm.html) ([Hirata & Seljak 2003](https://ui.adsabs.harvard.edu/abs/2003MNRAS.343..459H/abstract), [Mandelbaum et al. 2005](https://ui.adsabs.harvard.edu/abs/2005MNRAS.361.1287M/abstract)). Second, we fit a Sérsic model to a galaxy cutout.\n",
    "\n",
    "In this notebook, we will be building on previous tutorials. We recommend consulting the following tutorials prior to this one:\n",
    "- Data Access and Discovery\n",
    "- Working with ASDF\n",
    "- Data Visualization\n",
    "- WebbPSF\n",
    "- Synphot\n",
    "\n",
    "### Defining Terms\n",
    "\n",
    "- ASDF: Advanced Scientific Data Format — the data format for the Roman Wide Field Instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Data\n",
    "The first step of the analysis is to read the Roman WFI image data, which are stored in ASDF format. For this example, we start with a calibrated Level 2 (L2) simulated image created with [Roman I-Sim](https://romanisim.readthedocs.io). For more information about Roman's data products check the [Roman User Documentation](roman-docs.stsci.edu). We use `roman_datamodels` to open the simulated image, which we stream into memory from an S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Open an image from an S3 bucket\n",
    "asdf_dir_uri = 's3://roman-sci-test-data-prod-summer-beta-test/'\n",
    "fs = s3fs.S3FileSystem()\n",
    "asdf_file_uri_l2 = asdf_dir_uri + 'ROMANISIM/GALAXIES/r0003201001001001004_01101_0001_WFI01_cal.asdf'\n",
    "\n",
    "with fs.open(asdf_file_uri_l2, 'rb') as f:\n",
    "    file = rdm.open(f).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To characterize the shape of our sources, we need to get cutouts of individual targets, and perform our measurements. We choose to use the input catalog from the simulation software to define our cutouts. In a more realistic scenario, we would run a source detection algorithm to identify the locations of the sources.\n",
    "\n",
    "We start by saving the image array in the `image` variable. Remember that the `.data` datablock in WFI ASDF files contains the data array, and this is an `astropy.quantity.Quantity` object with units and values. In this case, we are just interested in the `.value` attribute of the image (in units of DN / s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data array as an numpy.ndarray object\n",
    "image = file.data.value\n",
    "print('The units of the original data are: ', file.data.unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the input catalog using `astropy.table.Table()`. More details about the format of these catalogs are available [here](https://romanisim.readthedocs.io/en/latest/romanisim/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the catalog used for the simulation as an astropy.table.Table\n",
    "cat_uri = asdf_dir_uri + 'ROMANISIM/CATALOGS_SCRIPTS/galfield.ecsv'\n",
    "with fs.open(cat_uri, 'r') as catalog_file_stream:\n",
    "    catalog = Table.read(cat_uri, format='ascii.ecsv').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick sanity check we plot the positions of 1% of the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of the positions of a subselection of catalog\n",
    "# entries. Since the catalog is centered around (ra, dec) = (0, 0),\n",
    "# subtract 360 degrees from any coordinate > 180 degrees for better\n",
    "# plotting.\n",
    "catalog['ra'][catalog['ra'] > 180] -= 360\n",
    "plt.scatter(catalog['ra'][::100], catalog['dec'][::100], s=0.1)\n",
    "plt.xlabel('RA [deg]', fontsize=16)\n",
    "plt.ylabel('Dec [deg]', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Cutouts and Retrieving the PSF\n",
    "\n",
    "The positions are in sky coordinates. In order to make the cutouts we need the pixel coordinates in the image. To do that transformation, we will use the generalized World Coordinate System (GWCS) object in the simulated file's metadata.\n",
    "For more information about GWCS please check the documentation [here](https://gwcs.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the gwcs object from the ASDF file into a variable\n",
    "wcs = file.meta.wcs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the GWCS object to transform from the catalog right ascension (RA) and declination (dec) coordinates into pixel positions within the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the catalog coordinates in a SkyCoord object and \n",
    "# convert to pixel positions.\n",
    "coords = SkyCoord(ra=catalog['ra'] * u.deg, dec=catalog['dec'] * u.deg)\n",
    "y, x = wcs.world_to_array_index_values(coords)\n",
    "\n",
    "# Save the x and y coordinates in the catalog table for later.\n",
    "catalog['x'] = x\n",
    "catalog['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, we focus on relatively bright objects ($18 < {m}_\\mathrm{AB} < 19$), and we will be interested in galaxies (marked as type = SER in the catalog). First, we need to know the optical element used for the observation, which we can get from the file metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = file.meta.instrument.optical_element\n",
    "print(f'The simulated bandpass is {band}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's trim our catalog based on our magnitude cuts and selection of extended sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set magnitude limits.\n",
    "mag_hi = 19\n",
    "mag_lo = 18\n",
    "\n",
    "# Compute the AB magnitude of the F106 filter in the catalog.\n",
    "mag_ab = -2.5 * np.log10(catalog['F106'])\n",
    "\n",
    "# Select sources within our magnitude limits and that are extended.\n",
    "bright = (mag_ab > mag_lo) & (mag_ab < mag_hi) & (catalog['type'] == 'SER')\n",
    "\n",
    "# The input catalog may contain sources that are not on the WFI detector.\n",
    "# Identify where sources are on the detector. The padding variable will\n",
    "# ensure that sources are not too close to the edge (and possibly cut off).\n",
    "padding = 10\n",
    "inchip = (x > padding) & (x < image.shape[0] - padding) & (y > padding) & (y < image.shape[1] - padding)\n",
    "\n",
    "# Our final selection is the intersection of these two lists of indices.\n",
    "# The variable gal_tab contains a subselection of the original catalog\n",
    "# that meets our selection criteria.\n",
    "galaxies = (bright) & (inchip)\n",
    "gal_tab = catalog[galaxies]\n",
    "\n",
    "print(np.count_nonzero(galaxies), 'galaxies pass these selection criteria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most morphological measurements require high-accuracy knowledge of the PSF. In the case of our simulated scene, we know that the PSF that was used to generate was obtained via `webbpsf`. We are generating a single PSF for our example so that it can be used for multiple galaxies without needing to generate many PSFs, but users interested in the highest accuracy fits will likely want to generate a PSF at the pixel location of the source being fit. See the WebbPSF tutorial for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the WFI instrument object in WebbPSF, set the optical element, and generate a PSF.\n",
    "wfi = webbpsf.WFI()\n",
    "wfi.filter = band\n",
    "psf = wfi.calc_psf(fov_pixels=64)\n",
    "\n",
    "# Generate a galsim image object of the simulated PSF. We specify the pixel scale for which\n",
    "# the WFI has 0.11 arcsecond pixels, and the PSF from WebbPSF is oversampled by a factor of 4.\n",
    "# Save the PSF as an interpolated image so we can convolve and deconvolve later on.\n",
    "psf_img = galsim.Image(psf['OVERSAMP'].data, scale=0.11 / 4)\n",
    "psf_obj = galsim.InterpolatedImage(psf_img, flux=1)  \n",
    "\n",
    "# Show the simulated PSF.\n",
    "plt.imshow(psf_img.array, norm=simple_norm(psf_img.array, 'log', percent=95), origin='lower');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the PSF is generated for WFI01 at (x, y) = 2048, 2048 in the science coordinate system. For more information about WFI coordinate systems, see the RDox article on [PySIAF for Roman](https://roman-docs.stsci.edu/simulation-tools-handbook-home/simulation-development-utilities/pysiaf-for-roman).\n",
    "\n",
    "These attributes can be confirmed and modified via the `wfi.detector` and `wfi.detector_position` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'WebbPSF detector = {wfi.detector}')\n",
    "print(f'WebbPSF position = {wfi.detector_position}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before the next step of creating a cutout, we need to make an estimate of the sky background level. Calibrated WFI images are not background subtracted by the pipeline, so we need to do this ourselves. There are various ways to accomplish this, but for our example we'll use the sigma_clipped median of the image. This background estimate is simplistic, but effective for our demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sigma-clipped median as an estimate of the sky background\n",
    "_, med_bkg, _ = sigma_clipped_stats(image)\n",
    "print(f'Median background = {med_bkg:0.5f} DN / s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a cutout of our galaxy. Again, there may be a few ways to do this, but here we will make a small postage stamp centered on the x and y coordinates we previously computed from the input simulation catalog. We will change the size of the postage stamp relative to the half-light radius of the galaxy in the input simulation catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to index our table of galaxies. Changing this variable will\n",
    "# change the galaxy that we are fitting.\n",
    "igal = 2\n",
    "\n",
    "# Get the dimensions of cutout based on the half-light radius. The scale variable\n",
    "# will set the multiples of the half-light radius used for the cutout. \n",
    "scale = 4\n",
    "\n",
    "# Multiply the half-light radius in arcseconds by the scale factor, and then\n",
    "# divide by the pixel scale of 0.11 arcseconds per pixel to get the half-light\n",
    "# radius in units of pixels. Add 1 pixel for padding so that small galaxies have\n",
    "# a sufficiently sized cutout.\n",
    "size = int((gal_tab['half_light_radius'][igal] * 4 / 0.11) + 1)\n",
    "\n",
    "# Determine the x and y positions at the corners of our cutout.\n",
    "xmin = int(gal_tab['x'][igal] - size//2)\n",
    "xmax = xmin + size + 1\n",
    "ymin = int(gal_tab['y'][igal] - size//2)\n",
    "ymax = ymin + size + 1\n",
    "\n",
    "# Make the cutout as a galsim Image object. Also subtract the sky background estimate,\n",
    "# and set the pixel scale.\n",
    "cutout = galsim.Image(image[ymin:ymax, xmin:xmax] - med_bkg, scale=0.11)\n",
    "\n",
    "# Display the cutout with and without the background subtraction.\n",
    "# Set the image normalization. To compare the cutout before and after\n",
    "# background subtraction, we manually set the limits.\n",
    "norm = simple_norm(cutout.array + med_bkg, 'linear', vmin= -med_bkg, vmax=np.max(cutout.array))\n",
    "\n",
    "# Setup the plot area.\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "# Original cutout (add the sky background back in)\n",
    "cut = axs[0].imshow(cutout.array + med_bkg, origin='lower', norm=norm)\n",
    "axs[0].set_title('Cutout')\n",
    "axs[0].set_xlabel('X [pix]')\n",
    "axs[0].set_ylabel('Y [pix]')\n",
    "\n",
    "# Background-subtracted cutout\n",
    "bkg_cut = axs[1].imshow(cutout.array, origin='lower', norm=norm)\n",
    "axs[1].set_title('Bkgrnd Sub')\n",
    "axs[1].set_xlabel('X [pix]')\n",
    "axs[1].set_ylabel('Y [pix]')\n",
    "\n",
    "# Assign color bars. The color bars and normalization are the same\n",
    "# for both cutouts.\n",
    "fig.colorbar(cut, ax=axs[0], label='DN / s')\n",
    "fig.colorbar(bkg_cut, ax=axs[1], label='DN / s')\n",
    "plt.tight_layout(h_pad=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the background estimate seems to be reasonable. We likely could improve this by doing a local background estimate rather than using the whole image. In  real, on-orbit observations, a 2-D local background model would likely provide the highest fidelity background estimate. A significant change in our background subtraction would affect the results of our later model fitting, and so background estimation should be treated with care.\n",
    "\n",
    "## Estimating the Source Moments\n",
    "\n",
    "Now that we have our cutout saved as a `galsim.Image` object we can just use the `hsm` module to estimate the moments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = galsim.hsm.FindAdaptiveMom(cutout, strict=False)\n",
    "shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` variable contains the source moments estimated from the best-fit elliptical Gaussian. For example, we can see in the printed result that the `.observed_shape` attribute contains a `galsim.Shear` object. From that object, we can retrieve shape parameters such as the position angle (beta) and the minor-to-major axis ratio (q):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the position angle (beta) and wrap it to the range [0, 2pi) radians and convert to degrees\n",
    "beta = shape.observed_shape.beta.wrap(center = 180 * coord.degrees).deg\n",
    "\n",
    "# Get the minor-to-major axis ratio (q)\n",
    "q = shape.observed_shape.q\n",
    "\n",
    "print(f'position angle: {beta:.5f} deg')\n",
    "print(f'minor-to-major axis ratio: {q:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the PSF model we can also try to estimate the PSF-corrected galaxy shear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape2 = galsim.hsm.EstimateShear(cutout, psf_img, strict=False)\n",
    "shape2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once again extract information about the position angle and minor-to-major axis ratio as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = shape2.observed_shape.beta.wrap(center = 180 * coord.degrees).deg\n",
    "q = shape2.observed_shape.q\n",
    "print(f'position angle: {beta:.5f} deg')\n",
    "print(f'minor-to-major axis ratio: {q:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that the values are the same with and without the PSF-correction.\n",
    "\n",
    "## Fitting a Sérsic Profile to a Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another typical morphological analysis consists of fitting a source with a [Sérsic profile](https://en.wikipedia.org/wiki/S%C3%A9rsic_profile) ([Sérsic 1963](https://ui.adsabs.harvard.edu/abs/1963BAAA....6...41S/abstract)). We are going to rely on the Sérsic implementation in the `galsim` package instead of doing our own.\n",
    "\n",
    "In our example we will ignore the pixel mask (data quality array), but will use the error image.\n",
    "\n",
    "First, we need to convert our source flux units to photons / s / cm$^2$. The catalog fluxes are stored as maggies, which are linear flux units such that:\n",
    "\n",
    "$m_\\mathrm{AB} = -2.5\\times\\log_{10}(f),$ \n",
    "\n",
    "where $m_\\mathrm{AB}$ is the AB magnitude and $f$ is the flux in maggies. As Roman I-Sim, which was used to generate the catalog and simulate the image, does not create sources with realistic colors, let's make a simplifying assumption that our source has a spectral energy distribution (SED) that is flat in frequency space, at least over the bandpass of interest (F106). We will use the `synphot` package to synthetically observe the source spectrum, normalized to the AB magnitude from the catalog, and integrate over the bandpass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AB magnitude of the source.\n",
    "abmag = -2.5 * np.log10(gal_tab['F106'][igal])\n",
    "\n",
    "# Create the spectrum and bandpass objects, and then observe the spectrum\n",
    "# through the bandpass.\n",
    "spec = syn.SourceSpectrum(syn.models.ConstFlux1D, amplitude=abmag * u.ABmag)\n",
    "band = stsyn.band('roman,wfi,f106')\n",
    "obs = syn.Observation(spec, band)\n",
    "\n",
    "# Integrate over the bandpass. By default, calls to the obs variable, which\n",
    "# stores a synphot Observation object, will return units of photlam, which\n",
    "# is shorthand for photons / s / cm^2 / Angstrom. The binset attribute contains\n",
    "# the default wavelengths used in the Observation object, which have units of\n",
    "# Angstroms. Thus, the integral of the Observation over the wavelengths will\n",
    "# give units of photons / s / cm^2.\n",
    "waves = obs.binset\n",
    "flux = np.trapz(obs(waves), x=waves).value\n",
    "\n",
    "print(f'Source flux: {flux:.5f} photon / s / cm^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have instead determined a scaling factor to apply to any input flux from the catalog by normalizing the spectrum to an amplitude of 1 maggie. Synphot does not allow us to use the `astropy.units.maggy` unit, but we can just as effectively use the definition of AB magnitudes to write this in units of Jankys:\n",
    "\n",
    "$m_\\mathrm{AB} = -2.5 \\times log_{10}(f_\\mathrm{Jy}) + 8.90,$\n",
    "\n",
    "and thus when $m_\\mathrm{AB} = 0$ mag, which is defined when $f = 1$ maggie, we find that $f_\\mathrm{Jy} = 10^{(8.90 / 2.5)} \\approx 3630.78055~\\mathrm{Jy}$. Determining such a scaling factor would be useful if we wanted to perform our shape analysis at scale.\n",
    "\n",
    "Then we create our model as a callable function for later fitting. The variables are:\n",
    "* n = Sérsic index\n",
    "* hlr = half-light radius in arcseconds\n",
    "* pa = position angle in degrees\n",
    "* x0 = shift in x pixels to center of the source\n",
    "* y0 = shift in y pixels to center of the source\n",
    "* q = minor-to-major axis ratio\n",
    "\n",
    "We will set the gain and area options in the `drawImage()` method on the `galsim.Sersic` object we create. This ensures that the model of our galaxy can return pixels with values like our observed image that has units of DN / s. We set the gain to 1.8, which is a reasonable value for a general WFI detector, and we use the obscured collecting area from the `galsim.roman` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sersic_mod(cutout, n, hlr, influx, pa, x0, y0, q):\n",
    "    nx, ny = cutout.array.shape\n",
    "    ser = galsim.Sersic(n, half_light_radius=hlr, flux=influx)\n",
    "    ser = ser.shear(q=q, beta=pa * galsim.degrees)  # change PA and axis ratio\n",
    "    offset = galsim.PositionD(x=x0, y=y0)  # potentially shift a bit the profile\n",
    "    ser = galsim.convolve.Convolve(ser, psf_obj)  # add PSF\n",
    "    img = galsim.ImageD(nx, ny, scale=0.11)  \n",
    "    ser.drawImage(image=img, offset=offset, scale=0.11, gain=1.8, area=galsim.roman.collecting_area)\n",
    "    \n",
    "    return img.array.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the model with the truth parameters. The Sérsic index, half-light radius, position angle, and minor-to-major axis ratio come from the input simulation catalog. The flux is the value computed above in the correct units of photons / s / cm$^2$. The x0 and y0 shifts are assumed to both be 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the truth paramters.\n",
    "p0 = [gal_tab['n'][igal], gal_tab['half_light_radius'][igal], flux, gal_tab['pa'][igal], 0., 0., gal_tab['ba'][igal]]\n",
    "\n",
    "# Create the model with our truth parameters\n",
    "fid_model = sersic_mod(cutout, *p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the syntax `*p0` above. This is the same as explicitly writing out `p0[0], p0[1], ..., p0[6]` and is a convenient way in Python to unpack all of the values in a list in order as inputs to a function's positional arguments.\n",
    "\n",
    "We also obtain the error array in the location of the cutout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_img = file.err[ymin:ymax, xmin:xmax].value.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model using `scipy.curve_fit()`. We add bounds to the parameter space in order avoid software issues and unphysical values — GalSim only supports $0.3 < n < 6.2$.\n",
    "\n",
    "**NOTE:** If the following code cell produces an exception, you may have to adjust the bounds of the fit. This can happen, for example, if you changed any of the filtering we did above on the source catalog (e.g., a different magnitude range cut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout, pcov = curve_fit(sersic_mod, cutout, cutout.array.flatten(), sigma=err_img,\n",
    "          p0=p0, bounds=([0.3, 0.01, 1e-11, 0, -size/2, -size/2, 0], [6.2, 4.0, 0.1, 360, size/2, size/2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the resulting best-fit parameters of the fit and compare to our initial input model parameters (the `p0` variable above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]}:')\n",
    "    print(f'\\tInitial: {p0[i]:.5f}')\n",
    "    print(f'\\tFitted: {pout[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the input and best-fit parameters, we see good agreement.\n",
    "\n",
    "Let's create an image of our best-fit model and visualize the original cutout, initial input model, best-fit output model, and fit residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an image of the best-fit model and compute the fit residuals.\n",
    "model_out = sersic_mod(cutout, *pout)\n",
    "residual = cutout.array - model_out.reshape(cutout.array.shape)\n",
    "\n",
    "# Plot the original cutout, initial input model, best-fit model, and fit residuals.\n",
    "# Plot all images with the same normalization for comparison. As we may have negative\n",
    "# residual values, we use a linear normalization.\n",
    "norm = simple_norm(cutout.array, 'linear', vmin= -med_bkg, vmax=np.max(cutout.array))\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "# Original image\n",
    "orig = axs[0][0].imshow(cutout.array, norm=norm, origin='lower')\n",
    "axs[0][0].set_title('Image Cutout')\n",
    "\n",
    "# Fiducial (initial input) model\n",
    "axs[0][1].imshow(fid_model.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[0][1].set_title('Init Model')\n",
    "\n",
    "# Best-fitting model\n",
    "axs[1][0].imshow(model_out.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[1][0].set_title('Best-Fit')\n",
    "\n",
    "# Fit residuals\n",
    "axs[1][1].imshow(residual, norm=norm, origin='lower')\n",
    "axs[1][1].set_title('Fit Residuals')\n",
    "\n",
    "# Set the colorbar\n",
    "fig.colorbar(orig, ax=axs[0][0])\n",
    "fig.colorbar(orig, ax=axs[0][1])\n",
    "fig.colorbar(orig, ax=axs[1][0])\n",
    "fig.colorbar(orig, ax=axs[1][1])\n",
    "\n",
    "# Other plot settings to clean it up.\n",
    "plt.tight_layout(h_pad=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fit looks really good. In fact, despite the presence of another bright source in the cutout, `scipy.curve_fit` did a good job of fitting the model to the data. Let's take a quick look at the statistics of the fit residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Residual median: {np.median(residual):.5f} DN / s')\n",
    "print(f'Residual std dev: {np.std(residual):.5f} DN / s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We benefitted greatly from knowing a lot of these parameters from the simulated input catalog used to generate the image. What if we didn't know these values so well? Would we have gotten such a good fit result? Let's test that now by setting the fiducial model to something less certain and fit it again. \n",
    "\n",
    "We'll set our `p1` variable to contain our new input parameters. Recall the input parameter order is Sérsic index, half-light radius in arcseconds, flux in photons / s / cm$^2$, position angle in degrees, x shift, y shift, and minor-to-major axis ratio. Let's assume we can measure the flux well, and that we estimate the half-light radius to be approximately 4 WFI pixels, or 0.44 arcseconds. We'll set the other values to uninformed guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [1., 0.44, flux, 0., 0., 0., 0.5]\n",
    "\n",
    "# Create the model with our truth parameters\n",
    "fid_model_new = sersic_mod(cutout, *p1)\n",
    "\n",
    "pout_new, pcov_new = curve_fit(sersic_mod, cutout, cutout.array.flatten(), sigma=err_img,\n",
    "          p0=p1, bounds=([0.3, 0.01, 1e-11, 0, -size/2, -size/2, 0], [6.2, 4.0, 0.1, 360, size/2, size/2, 1]))\n",
    "\n",
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]}:')\n",
    "    print(f'\\tCatalog (p0): {p0[i]:.5f}')\n",
    "    print(f'\\tNew inputs (p1): {p1[i]:.5f}')\n",
    "    print('')\n",
    "    print(f'\\tOld fit: {pout[i]:.5f}')\n",
    "    print(f'\\tNew fit: {pout_new[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best-fitting model is quite similar to the previous one with the exceptions of the flux, minor-to-major axis ratio, and the position angle. In fact, on closer examination, we can see that the position angle and minor-to-major axis ratio in our earlier fit is identical to the input values we obtained from the catalog.\n",
    "\n",
    "Let's try again, but this time let's use the shear estimate shape measurements we previously performed (recall that beta and q are the position angle and minor-to-major axis ratio, respectively, from the PSF-corrected shear estimate in the `shape2` variable above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [1., 0.44, flux, beta, 0., 0., q]\n",
    "\n",
    "# Create the model with our truth parameters\n",
    "fid_model_new = sersic_mod(cutout, *p1)\n",
    "\n",
    "pout_new, pcov_new = curve_fit(sersic_mod, cutout, cutout.array.flatten(), sigma=err_img,\n",
    "          p0=p1, bounds=([0.3, 0.01, 1e-11, 0, -size/2, -size/2, 0], [6.2, 4.0, 0.1, 360, size/2, size/2, 1]))\n",
    "\n",
    "labels = ('n', 'hlr', 'flux', 'pa', 'x0', 'y0', 'q')\n",
    "\n",
    "for i, _ in enumerate(p0):\n",
    "    print(f'{labels[i]}:')\n",
    "    print(f'\\tCatalog (p0): {p0[i]:.5f}')\n",
    "    print(f'\\tNew inputs (p1): {p1[i]:.5f}')\n",
    "    print('')\n",
    "    print(f'\\tOld fit: {pout[i]:.5f}')\n",
    "    print(f'\\tNew fit: {pout_new[i]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now the best-fit flux agrees well with the catalog value. We also see that we got similar values of the other best-fit parameters other than the position angle and minor-to-major axis ratio. It seems that these two parameters are not well constrained by the data using the Sérsic model fit.\n",
    "\n",
    "For completeness, let's also re-plot the cutout, models, and fit residuals with out new inputs and best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an image of the best-fit model and compute the fit residuals.\n",
    "model_out_new = sersic_mod(cutout, *pout_new)\n",
    "residual_new = cutout.array - model_out_new.reshape(cutout.array.shape)\n",
    "\n",
    "# Plot the original cutout, initial input model, best-fit model, and fit residuals.\n",
    "# Plot all images with the same normalization for comparison. As we may have negative\n",
    "# residual values, we use a linear normalization.\n",
    "norm = simple_norm(cutout.array, 'linear', vmin= -med_bkg, vmax=np.max(cutout.array))\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "# Original image\n",
    "orig = axs[0][0].imshow(cutout.array, norm=norm, origin='lower')\n",
    "axs[0][0].set_title('Image Cutout')\n",
    "\n",
    "# Fiducial (initial input) model\n",
    "axs[0][1].imshow(fid_model_new.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[0][1].set_title('Init Model')\n",
    "\n",
    "# Best-fitting model\n",
    "axs[1][0].imshow(model_out_new.reshape(cutout.array.shape), norm=norm, origin='lower')\n",
    "axs[1][0].set_title('Best-Fit')\n",
    "\n",
    "# Fit residuals\n",
    "axs[1][1].imshow(residual_new, norm=norm, origin='lower')\n",
    "axs[1][1].set_title('Fit Residuals')\n",
    "\n",
    "# Set the colorbar\n",
    "fig.colorbar(orig, ax=axs[0][0])\n",
    "fig.colorbar(orig, ax=axs[0][1])\n",
    "fig.colorbar(orig, ax=axs[1][0])\n",
    "fig.colorbar(orig, ax=axs[1][1])\n",
    "\n",
    "# Other plot settings to clean it up.\n",
    "plt.tight_layout(h_pad=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the initial model is quite different from our previous one based on inputs from the catalog, but our best-fit model and residuals still look quite good. Finally, the statistics of our residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Residual median: {np.median(residual_new):.5f} DN / s')\n",
    "print(f'Residual std dev: {np.std(residual_new):.5f} DN / s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these values with our earlier fit, we find that the median and standard deviation of the residuals of both fits are identical to within four decimal places despite differences in the input and best-fit model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional Resources\n",
    "\n",
    "- [Roman User Documentation (RDox)](https://roman-docs.stsci.edu/)\n",
    "- [Roman Notebooks](https://github.com/spacetelescope/roman_datamodels)\n",
    "- [Roman I-Sim documentation](https://romanisim.readthedocs.io/)\n",
    "- [WebbPSF documentation](https://webbpsf.readthedocs.io/)\n",
    "- [GalSim HSM shape fitting documentation](https://galsim-developers.github.io/GalSim/_build/html/hsm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About This Notebook\n",
    "\n",
    "**Author:** Javier Sánchez, Amethyst Barnes, Ami Choi, Tyler Desjardins  \n",
    "**Updated On:** 2024-09-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Roman Calibration latest (2024-03-25)",
   "language": "python",
   "name": "roman-cal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
